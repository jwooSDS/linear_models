[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"","code":""},{"path":"index.html","id":"who-is-this-book-for","chapter":"Preface","heading":"Who is this book for?","text":"many books linear models, various expectations different levels familiarity statistical, mathematical, coding concepts. books generally fall one two camps:Little familiarity statistical mathematical concepts, fairly familiar coding. books tend written programmers want get data science. books tend explain linear models trying avoid statistical mathematical concepts much, covering concepts absolutely necessary. books tend present linear models recipe format giving readers directions build models.drawback books readers get much understanding underlying concepts linear models. impossible give directions covering every possible scenario real world real data messy. Practitioners data science often think outside box order make linear models work particular data, difficult without understanding mathematical framework linear models.Familiarity mathematical notation introductory statistical concepts statistical inference, little familiarity coding. books tend written mathematicians (anyone strong background mathematics) want get data science. books cover mathematical framework linear models thoroughly.drawback books readers must comfortable mathematical notation. limits audience books people fairly thorough training mathematics. People without training get lost trying read books, understand need know mathematical foundations use linear models data science.book meant readable groups readers. foundational mathematical knowledge presented, written readable anyone. book also explain knowledge mean context data science. Practical advice, based foundational mathematical knowledge, also given.book accompanies course STAT 6021: Linear Models Data Science, Masters Data Science (MSDS) program University Virginia School Data Science.introductory statistics introductory programming pre-requisites entering MSDS program, book assumes basic knowledge statistical inference coding. Review materials covering concepts provided separately enrolled students.","code":""},{"path":"index.html","id":"data-sets-used","chapter":"Preface","heading":"Data sets used","text":"tried use many open source data sets much possible readers can work various examples provided . However, data sets may open source come experience teaching class since 2019 (variations class since 2013), used data sets shared statistics data science educators. goal eventually use open source data sets.","code":""},{"path":"index.html","id":"chapters","chapter":"Preface","heading":"Chapters","text":"chapters book follows:","code":""},{"path":"index.html","id":"other-resources","chapter":"Preface","heading":"Other resources","text":"resources readers may want check :OpenIntro Statistics, 4th ed. Diez, Cetinkaya-Rundel, Barr, OpenIntro. Get free PDF version https://leanpub.com/os, just set price want pay $0. good book introductory statistics.OpenIntro Statistics, 4th ed. Diez, Cetinkaya-Rundel, Barr, OpenIntro. Get free PDF version https://leanpub.com/os, just set price want pay $0. good book introductory statistics.Linear Models R, 2nd ed. Faraway. probably one books balances two camps wrote earlier. require familiarity matrices linear algebra though.Linear Models R, 2nd ed. Faraway. probably one books balances two camps wrote earlier. require familiarity matrices linear algebra though.Introduction Linear Regression Analysis, 5th 6th ed. Montgomery, Peck, Vining. may able access e-version book university library affiliated university. book mathematically rigorous useful interested mathematical proofs covered.Introduction Linear Regression Analysis, 5th 6th ed. Montgomery, Peck, Vining. may able access e-version book university library affiliated university. book mathematically rigorous useful interested mathematical proofs covered.Applied Linear Statistical Models (ALSM), Kutner, Nachtsheim, Neter, Li, 5th ed. book covers wide range topics linear models also mathematically rigorous.Applied Linear Statistical Models (ALSM), Kutner, Nachtsheim, Neter, Li, 5th ed. book covers wide range topics linear models also mathematically rigorous.Applied Linear Regression Models (ALRM), Kutner, Nachtsheim, Neter, 4th ed. ALRM first 14 chapters ALSM. second part ALSM covers topics Design Experiments, highly recommend interested topics.Applied Linear Regression Models (ALRM), Kutner, Nachtsheim, Neter, 4th ed. ALRM first 14 chapters ALSM. second part ALSM covers topics Design Experiments, highly recommend interested topics.","code":""},{"path":"wrangling.html","id":"wrangling","chapter":"1 Data Wrangling with R","heading":"1 Data Wrangling with R","text":"","code":""},{"path":"wrangling.html","id":"introduction","chapter":"1 Data Wrangling with R","heading":"1.1 Introduction","text":"data structure dealing often data frames. read data R, typically stored data frame. data frame can viewed like EXCEL spreadsheet, data stored rows columns. performing analysis, want data frame basic structure:row data frame corresponds observation.column data frame corresponds variable.Sometimes, data structured way, transform data take basic data structure. process called data wrangling. common basic operations transform data :Selecting subset columns data frame.Selecting subset rows data frame based criteria.Change column names.Find missing data.Create new variables based existing variables.Combine multiple data frames.explore two approaches data wrangling:Using functions already come pre-loaded R (sometimes called base R).Using functions dplyr package.two approaches quite different can achieve goals data wrangling. user R usually ends preferred way performing data wrangling operations, important know approaches able work broader audience.","code":""},{"path":"wrangling.html","id":"data-wrangling-using-base-r-functions","chapter":"1 Data Wrangling with R","heading":"1.2 Data Wrangling using Base R Functions","text":"use dataset ClassDataPrevious.csv example. data collected introductory statistics class UVa previous semester. Download dataset Canvas read R.check number rows columns dataframe.298 rows 8 columns: 298 students 8 variables. can also check names column.variables :Year: year student inSleep: much sleep student averages night (hours)Sport: student’s favorite sportCourses: many courses student taking semesterMajor: student’s majorAge: student’s age (years)Computer: operating system student uses (Mac PC)Lunch: much student usually spends lunch (dollars)","code":"\nData<-read.csv(\"ClassDataPrevious.csv\", header=TRUE)\ndim(Data)## [1] 298   8\ncolnames(Data)## [1] \"Year\"     \"Sleep\"    \"Sport\"    \"Courses\"  \"Major\"    \"Age\"      \"Computer\"\n## [8] \"Lunch\""},{"path":"wrangling.html","id":"view-specific-rows-andor-columns-of-a-data-frame","chapter":"1 Data Wrangling with R","heading":"1.2.1 View specific row(s) and/or column(s) of a data frame","text":"can view specific rows /columns data frame using square brackets [], example:row index listed first, column index, square brackets. means first student sleeps 8 hours night. can also view multiple rows columns, example:view 1st, 5th, 8th variables observations 1, 3, 4.several ways view specific column. example, view 1st column (variable called Year):Note comma separates indices row column. empty value comma means want rows, specific column. view multiple columns, example first four columns:view values certain rows, can useto view values observations 1 3. empty value comma means want columns specific rows.","code":"\nData[1,2] ##row index first, then column index## [1] 8\nData[c(1,3,4),c(1,5,8)]##     Year                            Major Lunch\n## 1 Second                         Commerce    11\n## 3 Second Cognitive science and psychology    10\n## 4  First                         Pre-Comm     4\nData$Year ##or\nData[,1] ##or\nData[,-c(2:8)]\nData[,1:4]\nData[,c(1,2,3,4)]\nData[c(1,3),]"},{"path":"wrangling.html","id":"select-observations-by-conditions","chapter":"1 Data Wrangling with R","heading":"1.2.2 Select observations by condition(s)","text":"may want analyze certain subsets data, based conditions. example, may want analyze students whose favorite sport soccer. () function R helps us find indices associated condition met. example:informs us rows belong observations whose favorite sport soccer, .e. 3rd, 20th, 25th () students. can create new data frame contains students whose favorite sport soccer:extracting rows satisfy condition, favorite sport soccer, storing rows new data frame called SoccerPeeps. can see new data frame 52 observations.Suppose want data frame satisfies two conditions: favorite sport soccer 2nd years UVa. can type:new data frame SoccerPeeps_2nd 25 observations.can also set conditions based numeric variables, example, want students sleep eight hours nightWe can also create data frame contains students satisfy least one two conditions, example, favorite sport soccer sleep 8 hours night:","code":"\nwhich(Data$Sport==\"Soccer\")##  [1]   3  20  25  26  31  32  33  38  44  46  48  50  51  64  67  71  87  92  98\n## [20]  99 118 122 124 126 128 133 136 137 143 146 153 159 165 174 197 198 207 211\n## [39] 214 226 234 241 255 259 260 266 274 278 281 283 294 295\nSoccerPeeps<-Data[which(Data$Sport==\"Soccer\"),]\ndim(SoccerPeeps)## [1] 52  8\nSoccerPeeps_2nd<-Data[which(Data$Sport==\"Soccer\" & Data$Year==\"Second\"),]\ndim(SoccerPeeps_2nd)## [1] 25  8\nSleepy<-Data[which(Data$Sleep>8),]\nSleepy_or_Soccer<-Data[which(Data$Sport==\"Soccer\" | Data$Sleep>8),]"},{"path":"wrangling.html","id":"change-column-names","chapter":"1 Data Wrangling with R","heading":"1.2.3 Change column name(s)","text":"datasets, names columns complicated make sense. always give descriptive names columns make sense. dataset, names self-explanatory really need change . example, suppose want change name 7th column Computer Comp:change names multiples columns (example, 1st 7th columns), type:","code":"\nnames(Data)[7]<-\"Comp\"\nnames(Data)[c(1,7)]<-c(\"Yr\",\"Computer\")"},{"path":"wrangling.html","id":"find-and-remove-missing-data","chapter":"1 Data Wrangling with R","heading":"1.2.4 Find and remove missing data","text":"ways locate missing data. Using .na() function directly data frame produces lot output can messy view:hand, using complete.cases() function pleasing view:code extract rows complete cases, words, rows missing entries. output informs us observation 103 missing value Sleep, observation 206 missing value Lunch.want remove observations missing value, can use one following two lines code create new data frames rows missing values removed:word caution: lines code remove entire row long least column missing entries. noted earlier, observation 103 missing value Sleep variable. observation still provides information variables, now removed.","code":"\nis.na(Data)\nData[!complete.cases(Data),]##         Yr Sleep      Sport Courses                                      Major\n## 103 Second    NA Basketball       7 psychology and youth and social innovation\n## 206 Second     8       None       4                          Cognitive Science\n##     Age Computer Lunch\n## 103  19      Mac    10\n## 206  19      Mac    NA\nData_nomiss<-na.omit(Data) ##or\nData_nomiss2<-Data[complete.cases(Data),]"},{"path":"wrangling.html","id":"summarizing-variables","chapter":"1 Data Wrangling with R","heading":"1.2.5 Summarizing variable(s)","text":"often, want obtain characteristics data. common way summarize numerical variable find mean. four numerical variables data frame, columns 2, 4, 6, 8. find mean four numerical variables, can use apply() function:Notice due missing values, first line NA variables. second line includes optional argument, na.rm=T, remove observations NA value variable calculation mean.least 3 arguments supplied apply() function:first argument data frame containing variables want find mean . case, want columns 2, 4, 6, 8 data frame Data.first argument data frame containing variables want find mean . case, want columns 2, 4, 6, 8 data frame Data.second argument takes value 1 2. Since want find mean columns, rather rows, type 2. want mean row, type 1.second argument takes value 1 2. Since want find mean columns, rather rows, type 2. want mean row, type 1.third argument specifies name function want apply columns supplied data frame. case, want mean. can change find median, standard deviation, etc, numeric variables want .third argument specifies name function want apply columns supplied data frame. case, want mean. can change find median, standard deviation, etc, numeric variables want .notice means variables suspiciously high, looking medians informative.","code":"\napply(Data[,c(2,4,6,8)],2,mean)##     Sleep   Courses       Age     Lunch \n##        NA  5.016779 19.573826        NA\napply(Data[,c(2,4,6,8)],2,mean,na.rm=T)##      Sleep    Courses        Age      Lunch \n## 155.559259   5.016779  19.573826 156.594175\napply(Data[,c(2,4,6,8)],2,median,na.rm=T)##   Sleep Courses     Age   Lunch \n##     7.5     5.0    19.0     9.0"},{"path":"wrangling.html","id":"summarizing-variable-by-groups","chapter":"1 Data Wrangling with R","heading":"1.2.6 Summarizing variable by groups","text":"Sometimes want summarize variable groups. Suppose want find median amount sleep separately 1st years, 2nd years, 3rd years, 4th years get. can use tapply() function:informs us median amount sleep first years get 8 hours night; fourth years median amount 7 hours night.least 3 arguments supplied tapply() function:first argument contains vector want summarize.first argument contains vector want summarize.second argument contains factor use subset data. example, want subset according Yr.second argument contains factor use subset data. example, want subset according Yr.third argument function want apply subset data.third argument function want apply subset data.fourth argument optional, case, want remove observations missing values calculation mean.fourth argument optional, case, want remove observations missing values calculation mean.Notice output orders factor levels alphabetical order. context, better rearrange levels First, Second, Third, Fourth using factor() function:output makes lot sense context.want summarize variable groups formed one variable, need adjust second argument tapply() function creating list. Suppose want find median sleep hour based Yr preferred operating system observations,Interestingly, looks like observations specify operating system use, hence extra column output.","code":"\ntapply(Data$Sleep,Data$Yr,median,na.rm=T)##  First Fourth Second  Third \n##    8.0    7.0    7.5    7.0\nData$Yr<-factor(Data$Yr, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\"))\n\nlevels(Data$Yr)## [1] \"First\"  \"Second\" \"Third\"  \"Fourth\"\ntapply(Data$Sleep,Data$Yr,median,na.rm=T) ##much nicer##  First Second  Third Fourth \n##    8.0    7.5    7.0    7.0\ntapply(Data$Sleep,list(Data$Yr,Data$Computer),median,na.rm=T)##           Mac   PC\n## First  NA 8.0 7.50\n## Second  7 7.5 7.50\n## Third  NA 7.5 7.00\n## Fourth NA 7.0 7.25"},{"path":"wrangling.html","id":"create-a-new-variable-based-on-existing-variables","chapter":"1 Data Wrangling with R","heading":"1.2.7 Create a new variable based on existing variable(s)","text":"Depending context analysis, may need create new variables based existing variables. variations task, based type variable want create, type variable based .","code":""},{"path":"wrangling.html","id":"create-a-numeric-variable-based-on-another-numeric-variable","chapter":"1 Data Wrangling with R","heading":"1.2.7.1 Create a numeric variable based on another numeric variable","text":"variable Sleep number hours. Suppose need convert values Sleep number minutes, can simply perform following mathematical operation:store transformed variable vector called Sleep_mins.","code":"\nSleep_mins<-Data$Sleep * 60"},{"path":"wrangling.html","id":"create-a-binary-variable-based-on-a-numeric-variable","chapter":"1 Data Wrangling with R","heading":"1.2.7.2 Create a binary variable based on a numeric variable","text":"Suppose want create binary variable (categorical variable two levels), called deprived. observation obtain value “yes” sleep less 7 hours night, “” otherwise. ifelse() function useful creating binary variables:3 arguments associated ifelse() function:first argument condition wish use.first argument condition wish use.second argument value observation condition true.second argument value observation condition true.third argument value observation condition false.third argument value observation condition false.","code":"\ndeprived<-ifelse(Data$Sleep<7, \"yes\", \"no\")"},{"path":"wrangling.html","id":"create-a-categorical-variable-based-on-a-numeric-variable","chapter":"1 Data Wrangling with R","heading":"1.2.7.3 Create a categorical variable based on a numeric variable","text":"Suppose want create categorical variable based number courses student takes. call new variable CourseLoad, takes following values:light 3 courses less,regular 4 5 courses,heavy 5 courses .cut() function used situationThere three arguments applied cut() function:first argument vector basing new variable .first argument vector basing new variable .argument breaks lists want set intervals associated Data$Courses. case, creating three intervals: one \\((-\\infty, 3]\\), another \\((3, 5]\\), last interval \\((5, \\infty]\\).argument breaks lists want set intervals associated Data$Courses. case, creating three intervals: one \\((-\\infty, 3]\\), another \\((3, 5]\\), last interval \\((5, \\infty]\\).argument labels gives label CourseLoad associated interval.argument labels gives label CourseLoad associated interval.","code":"\nCourseLoad<-cut(Data$Courses, breaks = c(-Inf, 3, 5, Inf), \n                labels = c(\"light\", \"regular\", \"heavy\"))"},{"path":"wrangling.html","id":"collapse-levels","chapter":"1 Data Wrangling with R","heading":"1.2.7.4 Collapse levels","text":"Sometimes, categorical variable levels need analysis, want collapse levels. example, variable Yr four levels: First, Second, Third, Fourth. Perhaps interested comparing upperclassmen underclassmen, want collapse First Second years underclassmen, Third Fourth years upperclassmen:levels associated variable Yr ordered First, Second, Third, Fourth. character vector new.levels first two characters, upper last two characters correspond original levels variable Yr. new variable called Year2.","code":"\nlevels(Data$Yr)## [1] \"First\"  \"Second\" \"Third\"  \"Fourth\"\nnew.levels<-c(\"und\", \"und\", \"up\",\"up\")\nYear2<-factor(new.levels[Data$Yr])\nlevels(Year2)## [1] \"und\" \"up\""},{"path":"wrangling.html","id":"combine-data-frames","chapter":"1 Data Wrangling with R","heading":"1.2.8 Combine data frames","text":"created four new variables, Sleep_mins, deprived, CourseLoad, Year2, based previously existing variables. Since variables based observations, can combine existing data frame using data.frame() function:Notice since listed four new variables Data data.frame() function, appear original columns data frame.Alternatively, can use cbind() function gives data frame:combining data frames different observations columns, can merge using rbind():","code":"\nData<-data.frame(Data,Sleep_mins,deprived,CourseLoad,Year2)\nhead(Data)##       Yr Sleep      Sport Courses                            Major Age Computer\n## 1 Second     8 Basketball       6                         Commerce  19      Mac\n## 2 Second     7     Tennis       5                       Psychology  19      Mac\n## 3 Second     8     Soccer       5 Cognitive science and psychology  21      Mac\n## 4  First     9 Basketball       5                         Pre-Comm  19      Mac\n## 5 Second     4 Basketball       6                      Statistics   19       PC\n## 6  Third     7       None       4                       Psychology  20       PC\n##   Lunch Sleep_mins deprived CourseLoad Year2\n## 1    11        480       no      heavy   und\n## 2    10        420       no    regular   und\n## 3    10        480       no    regular   und\n## 4     4        540       no    regular   und\n## 5     0        240      yes      heavy   und\n## 6    11        420       no    regular    up\nData2<-cbind(Data,Sleep_mins,deprived,CourseLoad,Year2)\ndat1<-Data[1:3,1:3]\ndat3<-Data[6:8,1:3]\nres.dat2<-rbind(dat1,dat3)\nhead(res.dat2)##       Yr Sleep      Sport\n## 1 Second     8 Basketball\n## 2 Second     7     Tennis\n## 3 Second     8     Soccer\n## 6  Third     7       None\n## 7 Second     7 Basketball\n## 8  First     7 Basketball"},{"path":"wrangling.html","id":"export-data-frame-in-r-to-a-.csv-file","chapter":"1 Data Wrangling with R","heading":"1.2.9 Export data frame in R to a .csv file","text":"export data frame .csv file, type:file newdata.csv created working directory. Note default, argument row.names set TRUE. add column dataframe index number. find step useful analyses almost always set row.names FALSE.","code":"\nwrite.csv(Data, file=\"newdata.csv\", row.names = FALSE)"},{"path":"wrangling.html","id":"sort-data-frame-by-column-values","chapter":"1 Data Wrangling with R","heading":"1.2.10 Sort data frame by column values","text":"sort data frame ascending order Age:sort descending order Age:sort ascending order Age first, Sleep:","code":"\nData_by_age<-Data[order(Data$Age),]\nData_by_age_des<-Data[order(-Data$Age),]\nData_by_age_sleep<-Data[order(Data$Age, Data$Sleep),]"},{"path":"wrangling.html","id":"data-wrangling-using-dplyr-functions","chapter":"1 Data Wrangling with R","heading":"1.3 Data Wrangling Using dplyr Functions","text":"previous section, performing data wrangling operations using functions built base R. module, using functions mostly package called dplyr, can perform operations well.performing data wrangling operations, let us clear environment, previously declared objects removed. allows us start clean slate, often desirable starting new analysis. done via:dplyr package subset tidyverse package, can access functions installing loading either package. installing tidyverse package, load typing:dplyr package developed make syntax intuitive broader range R users, primarily use pipes. However, code involved functions dlpyr tends longer code involved base R functions, functions learn dplyr.find lot articles internet various R users believes one approach superior . fussy approach use long can perform necessary operations. benefit familiar approaches can work broader range R users.continue use dataset ClassDataPrevious.csv example. Download dataset Canvas read R:examples , performing operations previous section, using dplyr functions instead base R functions.","code":"\nrm(list = ls())\n##library(dplyr) or\nlibrary(tidyverse) \nData<-read.csv(\"ClassDataPrevious.csv\", header=TRUE)"},{"path":"wrangling.html","id":"select-specific-columns-of-a-data-frame","chapter":"1 Data Wrangling with R","heading":"1.3.1 Select specific column(s) of a data frame","text":"select() function used select specific columns. couple ways use function. First:select column Year data frame called Data.","code":"\nselect(Data,Year)"},{"path":"wrangling.html","id":"pipes","chapter":"1 Data Wrangling with R","heading":"1.3.1.1 Pipes","text":"Alternatively, can use pipes:Pipes R typed using %>% pressing Ctrl + Shift + M keyboard. think operations , can read code astake data frame called Dataand select column named Year.can interpret pipe “”. Commands pipe placed new line (press enter). Pipes especially useful want execute several commands sequence, see later examples.","code":"\nData%>%\n  select(Year)"},{"path":"wrangling.html","id":"select-observations-by-conditions-1","chapter":"1 Data Wrangling with R","heading":"1.3.2 Select observations by condition(s)","text":"filter() function allows us subset data based conditions, example, select students whose favorite sport soccer:can create new data frame called SoccerPeeps contains students whose favorite sport soccer:Suppose want data frame, called SoccerPeeps_2nd, satisfies two conditions: favorite sport soccer 2nd years UVa:can also set conditions based numeric variables, example, want students sleep eight hours night:can also create data frame contains observations long satisfy least one two conditions: favorite sport soccer sleep 8 hours night:","code":"\nfilter(Data, Sport==\"Soccer\")\nSoccerPeeps<-Data%>%\n  filter(Sport==\"Soccer\")\nSoccerPeeps_2nd<-Data%>%\n  filter(Sport==\"Soccer\" & Year==\"Second\")\nSleepy<-Data%>%\n  filter(Sleep>8)\nSleepy_or_Soccer<-Data%>%\n  filter(Sport==\"Soccer\" | Sleep>8)"},{"path":"wrangling.html","id":"change-column-names-1","chapter":"1 Data Wrangling with R","heading":"1.3.3 Change column name(s)","text":"straightforward change names columns using rename() function. example:allows us change name two columns: Year Computer Yr Comp.","code":"\nData<-Data%>%\n  rename(Yr=Year, Comp=Computer)"},{"path":"wrangling.html","id":"summarizing-variables-1","chapter":"1 Data Wrangling with R","heading":"1.3.4 Summarizing variable(s)","text":"summarize() function allows us summarize column. Suppose want find mean value numeric columns: Sleep, Courses, Age, Lunch:output looks bit cumbersome. can give names summaryAs mentioned previously, means look suspiciously high couple variables, looking medians may informative:Note: lot functions dplyr package, using American spelling British spelling works. can use summarise() instead summarize().","code":"\nData%>%\n  summarize(mean(Sleep,na.rm = T),mean(Courses),mean(Age),mean(Lunch,na.rm = T))##   mean(Sleep, na.rm = T) mean(Courses) mean(Age) mean(Lunch, na.rm = T)\n## 1               155.5593      5.016779  19.57383               156.5942\nData%>%\n  summarize(avgSleep=mean(Sleep,na.rm = T),avgCourse=mean(Courses),avgAge=mean(Age),\n            avgLun=mean(Lunch,na.rm = T))##   avgSleep avgCourse   avgAge   avgLun\n## 1 155.5593  5.016779 19.57383 156.5942\nData%>%\n  summarize(medSleep=median(Sleep,na.rm = T),medCourse=median(Courses),\n            medAge=median(Age),medLun=median(Lunch,na.rm = T))##   medSleep medCourse medAge medLun\n## 1      7.5         5     19      9"},{"path":"wrangling.html","id":"summarizing-variable-by-groups-1","chapter":"1 Data Wrangling with R","heading":"1.3.5 Summarizing variable by groups","text":"Suppose want find median amount sleep 1st years, 2nd years, 3rd years, 4th years get. can use group_by() function:way read code isGet data frame called Data,group observations Yr,find median amount sleep Yr store median vector called medSleep.seen previously, ordering factor levels alphabetical order. context, better rearrange levels First, Second, Third, Fourth. can use mutate() function whenever want transform create new variable. case, transforming variable Yr reordering factor levels fct_relevel() function:Get data frame called Data,transform variable called Yr,reorder factor levels., use pipes, group_by(), summarize() functions like :output makes lot sense context.summarize variable groups formed one variable, just add variables group_by() function:","code":"\nData%>%\n  group_by(Yr)%>%\n  summarize(medSleep=median(Sleep,na.rm=T))## # A tibble: 4 × 2\n##   Yr     medSleep\n##   <chr>     <dbl>\n## 1 First       8  \n## 2 Fourth      7  \n## 3 Second      7.5\n## 4 Third       7\nData<- Data%>%\n  mutate(Yr = Yr%>%\n           fct_relevel(c(\"First\",\"Second\",\"Third\",\"Fourth\")))\nData%>%\n  group_by(Yr)%>%\n  summarize(medSleep=median(Sleep,na.rm=T))## # A tibble: 4 × 2\n##   Yr     medSleep\n##   <fct>     <dbl>\n## 1 First       8  \n## 2 Second      7.5\n## 3 Third       7  \n## 4 Fourth      7\nData%>%\n  group_by(Yr,Comp)%>%\n  summarize(medSleep=median(Sleep,na.rm=T))## `summarise()` has grouped output by 'Yr'. You can override using the `.groups`\n## argument.## # A tibble: 9 × 3\n## # Groups:   Yr [4]\n##   Yr     Comp  medSleep\n##   <fct>  <chr>    <dbl>\n## 1 First  \"Mac\"     8   \n## 2 First  \"PC\"      7.5 \n## 3 Second \"\"        7   \n## 4 Second \"Mac\"     7.5 \n## 5 Second \"PC\"      7.5 \n## 6 Third  \"Mac\"     7.5 \n## 7 Third  \"PC\"      7   \n## 8 Fourth \"Mac\"     7   \n## 9 Fourth \"PC\"      7.25"},{"path":"wrangling.html","id":"create-a-new-variable-based-on-existing-variables-1","chapter":"1 Data Wrangling with R","heading":"1.3.6 Create a new variable based on existing variable(s)","text":"mentioned previously, mutate() function used transform variable create new variable. variations task, based type variable want create, type variable based .","code":""},{"path":"wrangling.html","id":"create-a-numeric-variable-based-on-another-numeric-variable-1","chapter":"1 Data Wrangling with R","heading":"1.3.6.1 Create a numeric variable based on another numeric variable","text":"variable Sleep number hours. Suppose need convert values Sleep number minutes, can simply perform following mathematical operation:store transformed variable called Sleep_mins add Sleep_mins data frame called Data.","code":"\nData<-Data%>%\n  mutate(Sleep_mins = Sleep*60)"},{"path":"wrangling.html","id":"create-a-binary-variable-based-on-a-numeric-variable-1","chapter":"1 Data Wrangling with R","heading":"1.3.6.2 Create a binary variable based on a numeric variable","text":"Suppose want create binary variable , called deprived. observation obtain value yes sleep less 7 hours night, otherwise. add variable deprived data frame called Data:","code":"\nData<-Data%>%\n  mutate(deprived=ifelse(Sleep<7, \"yes\", \"no\"))"},{"path":"wrangling.html","id":"create-a-categorical-variable-based-on-a-numeric-variable-1","chapter":"1 Data Wrangling with R","heading":"1.3.6.3 Create a categorical variable based on a numeric variable","text":"Suppose want create categorical variable based number courses student takes. call new variable CourseLoad, takes following values:light 3 courses less,regular 4 5 courses,heavy 5 coursesand add CourseLoad data frame Data. can use case_when() function dplyr package, instead cut() function:Notice names categorical variable supplied specific condition specified.","code":"\nData<-Data%>%\n  mutate(CourseLoad=case_when(Courses <= 3 ~ \"light\", \n                              Courses >3 & Courses <=5 ~ \"regular\", \n                              Courses > 5 ~ \"heavy\"))"},{"path":"wrangling.html","id":"collapsing-levels","chapter":"1 Data Wrangling with R","heading":"1.3.6.4 Collapsing levels","text":"Sometimes, categorical variable levels need analysis, want collapse levels. example, variable Yr four levels: First, Second, Third, Fourth. Perhaps interested comparing upperclassmen underclassmen, want collapse First Second Yrs underclassmen, Third Fourth Yrs upperclassmen. use fct_collapse() function:creating new variable called UpUnder, done collapsing First Second new factor called , collapsing Third Fourth new factor called . UpUnder also added data frame Data.","code":"\nData<-Data%>%\n  mutate(UpUnder=fct_collapse(Yr,under=c(\"First\",\"Second\"),up=c(\"Third\",\"Fourth\")))"},{"path":"wrangling.html","id":"combine-data-frames-1","chapter":"1 Data Wrangling with R","heading":"1.3.7 Combine data frames","text":"combine data frames different observations columns, can combine using bind_rows():bind_rows() works way rbind(). Likewise, can use bind_cols() instead cbind().","code":"\ndat1<-Data[1:3,1:3]\ndat3<-Data[6:8,1:3]\nres.dat2<-bind_rows(dat1,dat3)\nhead(res.dat2)##       Yr Sleep      Sport\n## 1 Second     8 Basketball\n## 2 Second     7     Tennis\n## 3 Second     8     Soccer\n## 4  Third     7       None\n## 5 Second     7 Basketball\n## 6  First     7 Basketball"},{"path":"wrangling.html","id":"sort-data-frame-by-column-values-1","chapter":"1 Data Wrangling with R","heading":"1.3.8 Sort data frame by column values","text":"sort data frame ascending order Age:sort descending order Age:sort ascending order Age first, Sleep:","code":"\nData_by_age<-Data%>%\n  arrange(Age)\nData_by_age_des<-Data%>%\n  arrange(desc(Age))\nData_by_age_sleep<-Data%>%\n  arrange(Age,Sleep)"},{"path":"viz.html","id":"viz","chapter":"2 Data Visualization with R Using ggplot2","heading":"2 Data Visualization with R Using ggplot2","text":"","code":""},{"path":"viz.html","id":"introduction-1","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.1 Introduction","text":"Data visualizations tools summarize data. Consider visuals CDC covid tracker dashboard external site. Without actually access actual data, sense trends associated hospitalizations deaths. Good visualizations easy interpret wide variety audiences, easier explain statistical models.module, learn create common data visualizations. choice data visualization almost always determined whether variable(s) involved categorical quantitative. Discrete variables interesting depending circumstance, can viewed either categorical quantitative context data visualizations.using functions ggplot2 package create visualizations. ggplot2 package enables users create various kinds data visualizations, beyond visualizations can made base R. ggplot2 package automatically loaded load tidyverse package, although can load ggplot2 .use dataset ClassDataPrevious.csv example. data collected introductory statistics class UVa previous semester. Download dataset Canvas read R.variables :Year: year student inSleep: much sleep student averages night (hours)Sport: student’s favorite sportCourses: many courses student taking semesterMajor: student’s majorAge: student’s age (years)Computer: operating system student uses (Mac PC)Lunch: much student usually spends lunch (dollars)","code":"\nlibrary(tidyverse)\nData<-read.csv(\"ClassDataPrevious.csv\", header=TRUE)"},{"path":"viz.html","id":"visualizations-with-a-single-categorical-variable","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.2 Visualizations with a Single Categorical Variable","text":"","code":""},{"path":"viz.html","id":"frequency-tables","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.2.1 Frequency tables","text":"Frequency tables common tool summarize categorical variables. tables give us number observations (sometimes called counts) belong class categorical variable. tables created using table() function. Suppose want see number students year data:Notice order years rearranged make sense:83 first years, 139 second years, 46 third years, 30 fourth years dataset.can report numbers using proportions instead counts, using prop.table():percentages multiplying 100:round percentages two decimal places, use round() function:27.85% students first years, 46.64% second years, 15.44% third years, 10.07% fourth years.","code":"\ntable(Data$Year)## \n##  First Fourth Second  Third \n##     83     30    139     46\nData$Year<-factor(Data$Year, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\"))\nlevels(Data$Year)## [1] \"First\"  \"Second\" \"Third\"  \"Fourth\"\nmytab<-table(Data$Year)\nmytab## \n##  First Second  Third Fourth \n##     83    139     46     30\nprop.table(mytab)## \n##     First    Second     Third    Fourth \n## 0.2785235 0.4664430 0.1543624 0.1006711\nprop.table(mytab) * 100## \n##    First   Second    Third   Fourth \n## 27.85235 46.64430 15.43624 10.06711\nround(prop.table(mytab) * 100, 2)## \n##  First Second  Third Fourth \n##  27.85  46.64  15.44  10.07"},{"path":"viz.html","id":"bar-charts","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.2.2 Bar charts","text":"Bar charts simple way visualize categorical data, can viewed visual representation frequency tables. create bar chart years students, use:can read number students first, second, third, fourth years reading corresponding value vertical axis.two lines code, can see basic structure creating data visualizations ggplot() function:Use ggplot() function, supply name data frame, x- /y- variables via aes() function. End line + operator, press enter.Use ggplot() function, supply name data frame, x- /y- variables via aes() function. End line + operator, press enter.next line, specify type graph want create (called geoms). bar chart, type geom_bar().next line, specify type graph want create (called geoms). bar chart, type geom_bar().describe lines code two layers code. two layers must supplied data visualizations ggplot().Additional optional layers can added (usually deal details visuals). Suppose want change orientation bar chart, can add optional line, layer:recommended layer typed line previous layer. + sign used end layer add another layer .change color bars:different color outline bars:","code":"\nggplot(Data, aes(x=Year))+\n  geom_bar()\nggplot(Data, aes(x=Year))+\n  geom_bar()+\n  coord_flip()\nggplot(Data, aes(x=Year))+\n  geom_bar(fill=\"blue\")\nggplot(Data, aes(x=Year))+\n  geom_bar(fill=\"blue\",color=\"orange\")"},{"path":"viz.html","id":"customize-title-and-labels-of-axes-in-bar-charts","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.2.2.1 Customize title and labels of axes in bar charts","text":"change orientation labels horizontal axis, add extra layer called theme. useful many classes /labels long names.rotate labels horizontal 90 degrees:create visualizations, good practice give short meaningful descriptive names axis provide title. can change labels x- y- axes, well add title bar chart adding another layer called labs:can also adjust position title, example, center-justify via theme:","code":"\nggplot(Data, aes(x=Year))+\n  geom_bar()+\n  theme(axis.text.x = element_text(angle = 90))\nggplot(Data, aes(x=Year))+\n  geom_bar()+\n  theme(axis.text.x = element_text(angle = 90))+\n  labs(x=\"Year\", y=\"Number of Students\", title=\"Dist of Years\")\nggplot(Data, aes(x=Year))+\n  geom_bar()+\n  theme(axis.text.x = element_text(angle = 90), \n        plot.title = element_text(hjust = 0.5))+\n  labs(x=\"Year\", y=\"Number of Students\", title=\"Dist of Years\")"},{"path":"viz.html","id":"create-a-bar-chart-using-proportions","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.2.2.2 Create a bar chart using proportions","text":"Suppose want create bar chart y-axis displays proportions, rather counts level. steps produce bar chart. First, create new dataframe, row represents year, add proportion year new column:code following:Creates new data frame called newData taking data frame called Data,groups observations Year,counts number observations Year stores values vector called Counts,creates new vector called Percent using mathematical operations specified mutate(). Percent added newData.can take look contents newData:create bar chart using proportions:Note following:first layer, use newData instead old data frame. aes(), specified y-variable, want Percent.second layer, specified stat=\"identity\" inside geom_bar().","code":"\nnewData<-Data%>%\n  group_by(Year)%>%\n  summarize(Counts=n())%>%\n  mutate(Percent=Counts/nrow(Data))\nnewData## # A tibble: 4 × 3\n##   Year   Counts Percent\n##   <fct>   <int>   <dbl>\n## 1 First      83   0.279\n## 2 Second    139   0.466\n## 3 Third      46   0.154\n## 4 Fourth     30   0.101\nggplot(newData, aes(x=Year, y=Percent))+\n  geom_bar(stat=\"identity\")+\n  theme(axis.text.x = element_text(angle = 90), \n        plot.title = element_text(hjust = 0.5))+\n  labs(x=\"Year\", y=\"Percent of Students\", title=\"Dist of Years\")"},{"path":"viz.html","id":"visualizations-with-a-single-quantitative-variable","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.3 Visualizations with a Single Quantitative Variable","text":"","code":""},{"path":"viz.html","id":"number-summary","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.3.1 5 number summary","text":"summary() function, applied quantitative variable, produces 5 number summary: minimum, first quartile (25th percentile), median (50th percentile), third quartile (75th percentile), maximum, well mean. example, obtain 5 number summary ages students:average age observations dataset 19.57 years old. Notice first quartile median 19 years old, means least quarter observations 19 years old. Also note maximum 51 years old, student quite lot older rest.","code":"\nsummary(Data$Age)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   18.00   19.00   19.00   19.57   20.00   51.00"},{"path":"viz.html","id":"boxplots","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.3.2 Boxplots","text":"boxplot graphical representation 5 number summary. create generic boxplot, following two lines code using ggplot() function:Note still using structure creating data visualizations ggplot():Use ggplot() function, supply name data frame, x- /y- variables via aes() function. End line + operator, press enter.Use ggplot() function, supply name data frame, x- /y- variables via aes() function. End line + operator, press enter.next line, specify type graph want create (called geoms). boxplot, type geom_boxplot.next line, specify type graph want create (called geoms). boxplot, type geom_boxplot.Notice outliers (observations lot older younger) denoted dots. One 51 year old, 22 year olds deemed outliers. rule used \\(1.5 \\times IQR\\) rule.Similar bar charts, can change orientation boxplots adding additional layer :can change color box outliers similarly:","code":"\nggplot(Data, aes(y=Age))+\n  geom_boxplot()\nggplot(Data, aes(y=Age))+\n  geom_boxplot()+\n  coord_flip()\nggplot(Data, aes(y=Age))+\n  geom_boxplot(color=\"blue\", outlier.color = \"orange\" )"},{"path":"viz.html","id":"histograms","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.3.3 Histograms","text":"histogram displays number observations within bin x-axis:Notice warning message displayed creating basic histogram. fix , use binwidth argument within geom_histogram. try binwidth=1 now, means width bin 1 unit:ages students mostly young, 19 20 years olds commonly occuring.well-known drawback histograms width bins can drastically affect visual. example, suppose change binwidth 2:bar now contains two ages: first bar contains 18 19 year olds. Notice shape changed little bit previous histogram different binwidth?","code":"\nggplot(Data,aes(x=Age))+\n  geom_histogram()## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nggplot(Data,aes(x=Age))+\n  geom_histogram(binwidth = 1,fill=\"blue\",color=\"orange\")\nggplot(Data,aes(x=Age))+\n  geom_histogram(binwidth = 2,fill=\"blue\",color=\"orange\")"},{"path":"viz.html","id":"density-plots","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.3.4 Density plots","text":"Density plots variation histograms, plot attempts use smooth mathematical function approximate shape histogram, unaffected binwidth:can see 19 20 year olds common ages data. careful interpreting values veritical axis: represent proportions. characteristic density plots area plot always one.","code":"\nggplot(Data,aes(x=Age))+\n  geom_density()"},{"path":"viz.html","id":"bivariate-visualizations","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.4 Bivariate Visualizations","text":"now look visualizations can create explore relationship two variables. term bivariate means looking two variables.using new dataset example, clear environment:using dataset, gapminder, gapminder package. Install load gapminder package. Also load tidyverse package (automatically loads ggplot2 package):can take look gapminder dataset:Per documentation, variables arecountrycontinentyear: 1952 2007 increments 5 yearslifeExp: life expectancy birth, yearspop: population countrygdpPercap: GDP per capita US dollars, adjusted inflationWe notice data collected country across number different years: 1952 2007 increments five years. example, mainly focus data recent year, 2007:specific visuals use depend type variables using, whether categorical quantitative.","code":"\nrm(list = ls())\nlibrary(tidyverse)\nlibrary(gapminder)\ngapminder[1:15,]## # A tibble: 15 × 6\n##    country     continent  year lifeExp      pop gdpPercap\n##    <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## 11 Afghanistan Asia       2002    42.1 25268405      727.\n## 12 Afghanistan Asia       2007    43.8 31889923      975.\n## 13 Albania     Europe     1952    55.2  1282697     1601.\n## 14 Albania     Europe     1957    59.3  1476505     1942.\n## 15 Albania     Europe     1962    64.8  1728137     2313.\nData<-gapminder%>%\n  filter(year==2007)"},{"path":"viz.html","id":"compare-quantitative-variable-across-categories","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.4.1 Compare quantitative variable across categories","text":"","code":""},{"path":"viz.html","id":"side-by-side-boxplots","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.4.1.1 Side by side boxplots","text":"Side side boxplots useful compare quantitative variable across different classes categorical variable. example, want compare life expectancies across different continents year 2007:Countries Oceania region long life expectancies little variation. Comparing Americas Asia, median life expectancies similar, spread larger Asia.","code":"\nggplot(Data, aes(x=continent, y=lifeExp))+\n  geom_boxplot(fill=\"Blue\")+\n  labs(x=\"Continent\", y=\"Life Exp\", title=\"Dist of Life Expectancies by Continent\")"},{"path":"viz.html","id":"violin-plots","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.4.1.2 Violin plots","text":"Violin plots alternative boxplots. create plots compare life expectancies across different continents year 2007:width violin informs us values commonly occuring. example, look violin Europe. violin wider higher life expectancies, longer life expectancies common European countries.","code":"\nggplot(Data, aes(x=continent, y=lifeExp))+\n  geom_violin()+\n  labs(x=\"Continent\", y=\"Life Exp\", title=\"Dist of Life Expectancies by Continent\")"},{"path":"viz.html","id":"summarizing-two-categorical-variables","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.4.2 Summarizing two categorical variables","text":"example, create new binary variable called expectancy, denoted low life expectancy country less 70 years, high otherwise:","code":"\nData<-Data%>%\n  mutate(expectancy=ifelse(lifeExp<70,\"Low\",\"High\"))"},{"path":"viz.html","id":"two-way-tables","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.4.2.1 Two-way tables","text":"Suppose want see expectancy varies across continents. two-way table can created produce counts two categorical variables involved:first variable table() placed rows, second variable placed columns.table, can see 22 countries Americas high life expectancies, 3 countries Americas low life expectancies.may interested looking proportions, instead counts, countries continent high low life expectancies. convert table proportions, can use prop.table():want proportions continent, want proportions row add 1. Therefore, second argument prop.table() 1. Enter 2 argument want proportions column add 1., convert percentages round 2 decimal places:","code":"\nmytab2<-table(Data$continent, Data$expectancy)\n##continent in rows, expectancy in columns\nmytab2##           \n##            High Low\n##   Africa      7  45\n##   Americas   22   3\n##   Asia       22  11\n##   Europe     30   0\n##   Oceania     2   0\nprop.table(mytab2, 1) ##           \n##                 High       Low\n##   Africa   0.1346154 0.8653846\n##   Americas 0.8800000 0.1200000\n##   Asia     0.6666667 0.3333333\n##   Europe   1.0000000 0.0000000\n##   Oceania  1.0000000 0.0000000\nround(prop.table(mytab2, 1) * 100, 2)##           \n##              High    Low\n##   Africa    13.46  86.54\n##   Americas  88.00  12.00\n##   Asia      66.67  33.33\n##   Europe   100.00   0.00\n##   Oceania  100.00   0.00"},{"path":"viz.html","id":"bar-charts-1","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.4.2.2 Bar charts","text":"stacked bar chart can used display relationship binary variable expectancy across continents:can see many countries exist continent, many countries continent high low life expectancies. example, 25 countries Americas majority high life expectancies.can change way bar chart displayed changing position geom_bar() position = \"dodge\" position = \"fill\", latter useful proportions instead counts:","code":"\nggplot(Data, aes(x=continent, fill=expectancy))+\n  geom_bar(position = \"stack\")+\n  labs(x=\"Continent\", y=\"Count\", title=\"Life Expectancies by Continent\")\nggplot(Data, aes(x=continent, fill=expectancy))+\n  geom_bar(position = \"dodge\") \nggplot(Data, aes(x=continent, fill=expectancy))+\n  geom_bar(position = \"fill\")+\n  labs(x=\"Continent\", y=\"Proportion\", \n       title=\"Proportion of Life Expectancies by Continent\")"},{"path":"viz.html","id":"summarizing-two-quantitative-variables","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.4.3 Summarizing two quantitative variables","text":"","code":""},{"path":"viz.html","id":"scatterplots","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.4.3.1 Scatterplots","text":"Scatterplots standard visualization two quantitative variables involved. create scatterplot life expectancy GDP per capita:see curved relationship life expectancies GDP per capita. Countries higher GDP per capita tend longer life expectancies.many observations, plots scatterplot may actually overlap . sense many exist, can add transparency scale called alpha=0.2 inside geom_point():default value alpha 1, means points transparent. closer value 0, transparent points . darker point indicates observations specific values variables.","code":"\nggplot(Data, aes(x=gdpPercap,y=lifeExp))+\n  geom_point()\nggplot(Data, aes(x=gdpPercap,y=lifeExp))+\n  geom_point(alpha=0.2)+\n  labs(x=\"GDP\", y=\"Life Exp\", \n       title=\"Scatterplot of Life Exp against GDP\")"},{"path":"viz.html","id":"multivariate-visualizations","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.5 Multivariate Visualizations","text":"now look visualizations can create explore relationship multiple (two) variables. term multivariate means looking two variables.","code":""},{"path":"viz.html","id":"bar-charts-2","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.5.1 Bar charts","text":"Previously, created bar chart look expectancy varies across continents. Suppose want see bar graphs vary across years, use year variable third variable via layer facet_wrap:Notice three categorical variables summarized bar chart. something can done improve bar chart? make improvement?","code":"\n##another data frame across all years plus a binary variable \n##for expectancy\nData.all<-gapminder%>%\n  mutate(expectancy=ifelse(lifeExp<70,\"Low\",\"High\"))\n\nggplot(Data.all,aes(x=continent, fill=expectancy))+\n  geom_bar(position = \"fill\")+\n  facet_wrap(~year)"},{"path":"viz.html","id":"scatterplots-1","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.5.2 Scatterplots","text":"Previously, created scatterplot life expectancy GDP per capita. can include another quantitative variable scatterplot, using size plots. can use size plots denote population countries. supplied via size aes():can adjust size plots adding layer scale_size():scatterplot summarizes three quantitative variables.can use different-colored plots denote continent point belongs :scatterplot summarizes three quantitative variables one categorical variable.can adjust plots changing shape making translucent via shape alpha aes():","code":"\nggplot(Data, aes(x=gdpPercap, y=lifeExp, size=pop))+\n  geom_point()\nggplot(Data, aes(x=gdpPercap, y=lifeExp, size=pop))+\n  geom_point()+\n  scale_size(range = c(0.1,12))\nggplot(Data, aes(x=gdpPercap, y=lifeExp, size=pop, color=continent))+\n  geom_point()+\n  scale_size(range = c(0.1,12))\nggplot(Data, aes(x=gdpPercap, y=lifeExp, size=pop, fill=continent))+\n  geom_point(shape=21, alpha=0.5)+\n  scale_size(range = c(0.1,12))+\n  labs(x=\"GDP\", y=\"Life Exp\", title=\"Scatterplot of Life Exp against GDP\")"},{"path":"slr.html","id":"slr","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3 Basics with Simple Linear Regression (SLR)","text":"","code":""},{"path":"slr.html","id":"introduction-2","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.1 Introduction","text":"start module introducing simple linear regression model. Simple linear regression uses term “simple,” concerns study one predictor variable one quantitative response variable. contrast, multiple linear regression, study future modules, uses term “multiple,” concerns study two predictor variables one quantitative response variable. start simple linear regression much easier visualize concepts regression models one predictor variable.time , consider predictor variables quantitative. consider predictor variables categorical future modules.common way visualizing relationship one quantitative predictor variable one quantitative response variable scatter plot. simulated example , data 6000 UVa undergraduate students amount time spend studying week (minutes), many courses taking semester (3 4 credit courses).\nFigure 3.1: Scatterplot Study Time Number Courses Taken\nQuestions may include:study time number courses taken related one another?strong relationship?use data make prediction study time student scatterplot?confident prediction?questions can answered using simple linear regression.Note learning models just one response variable. cover multivariate regression, used one response variable. may confusion “multiple” linear regression “multivariate” regression due closeness terminology.","code":"\n##create dataframe\ndf<-data.frame(study,courses)\n\n##fit regression\nresult<-lm(study~courses, data=df)\n##create scatterplot with regression line overlaid\nplot(df$courses, df$study, xlab=\"# of Courses\", ylab=\"Study Time (Mins)\")\nabline(result)"},{"path":"slr.html","id":"basic-ideas-with-statistics","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.1.1 Basic Ideas with Statistics","text":"","code":""},{"path":"slr.html","id":"population-vs-sample","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.1.1.1 Population vs Sample","text":"Statistical methods usually used make inferences population based information sample.sample collection units actually measured surveyed study.population includes units interest.study time example , population UVa undergraduate students, sample 6000 students data displayed scatterplot.","code":""},{"path":"slr.html","id":"parameters-vs-statistics","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.1.1.2 Parameters vs Statistics","text":"Parameters numerical quantities describe population.Statistics numerical quantities describe sample.study time example, example parameter average study time among UVa undergraduate students (called population mean), example statistic average study time among 6000 UVa students data (called sample mean).Notice real life, rarely know actual numerical value parameter. use numerical value statistic estimate unknown numerical value corresponding parameter.also different notation parameters statistics. example,population mean denoted \\(\\mu\\).sample mean denoted \\(\\bar{x}\\).say \\(\\bar{x}\\) estimator \\(\\mu\\).important pay attention whether describing statistic (known value can calculated) parameter (unknown value).","code":""},{"path":"slr.html","id":"motivation","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.1.2 Motivation","text":"Linear regression models generally two primary uses:Prediction: Predict future value response variable, using information predictor variables.Association: Quantify relationship variables. change predictor variable change value response variable?always distinguish response variable, denoted \\(y\\), predictor variable, denoted \\(x\\). statistical models, say response variable can approximated mathematical function, denoted \\(f\\), predictor variable, .e.\\[\ny \\approx f(x).\n\\]\nOftentimes, write relationship \\[\ny = f(x) + \\epsilon,\n\\]\\(\\epsilon\\) denotes random error term, mean 0. error term predicted based data .various statistical methods estimate \\(f\\). estimate \\(f\\), can use method prediction / association.Using study time example :prediction example: student intends take 4 courses semester. student’s predicted study time, average?association example: want see taking courses increases study time.","code":""},{"path":"slr.html","id":"practice-questions","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.1.2.1 Practice questions","text":"examples , using regression model prediction association?early morning heading rest day. want know weather forecast rest day know wear.early morning heading rest day. want know weather forecast rest day know wear.executive sports league wants assess increasing length commercial breaks may impact enjoyment sports fans watch games TV.executive sports league wants assess increasing length commercial breaks may impact enjoyment sports fans watch games TV.Education Secretary like evaluate certain factors use technology classrooms investment teacher training teacher pay associated reading skills students.Education Secretary like evaluate certain factors use technology classrooms investment teacher training teacher pay associated reading skills students.buying home, prospective buyer like know home - - priced, given characteristics.buying home, prospective buyer like know home - - priced, given characteristics.","code":""},{"path":"slr.html","id":"simple-linear-regression-slr","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.2 Simple Linear Regression (SLR)","text":"simple linear regression (SLR), function \\(f\\) relates predictor variable response variable typically \\(\\beta_0 + \\beta_1 x\\). Mathematically, express \\[\ny \\approx \\beta_0 + \\beta_1 x,\n\\]words, response variable approximately linear relationship predictor variable.SLR, relationship explicitly formulated simple linear regression equation:\\[\\begin{equation}\nE(y|x)=\\beta_0+\\beta_{1}x.\n\\tag{3.1}\n\\end{equation}\\]\\(\\beta_0\\) \\(\\beta_1\\) parameters SLR equation, want estimate .\\(\\beta_0\\) \\(\\beta_1\\) parameters SLR equation, want estimate .parameters sometimes called regression coefficients.parameters sometimes called regression coefficients.\\(\\beta_1\\) also called slope. denotes change \\(y\\), average, \\(x\\) increases one unit.\\(\\beta_1\\) also called slope. denotes change \\(y\\), average, \\(x\\) increases one unit.\\(\\beta_0\\) also called intercept. denotes average \\(y\\) \\(x=0\\).\\(\\beta_0\\) also called intercept. denotes average \\(y\\) \\(x=0\\).notation left hand side (3.1) denotes expected value response variable, fixed value predictor variable. (3.1) implies , value predictor variable \\(x\\), expected value response variable \\(y\\) \\(\\beta_0+\\beta_{1}x\\). expected value also population mean. Applying (3.1) study time example, implies :\nstudents take 3 courses, expected study time equal \\(\\beta_0 + 3\\beta_1\\),\nstudents take 4 courses, expected study time equal \\(\\beta_0 + 4\\beta_1\\),\nstudents take 5 courses, expected study time equal \\(\\beta_0 + 5\\beta_1\\).\nnotation left hand side (3.1) denotes expected value response variable, fixed value predictor variable. (3.1) implies , value predictor variable \\(x\\), expected value response variable \\(y\\) \\(\\beta_0+\\beta_{1}x\\). expected value also population mean. Applying (3.1) study time example, implies :students take 3 courses, expected study time equal \\(\\beta_0 + 3\\beta_1\\),students take 4 courses, expected study time equal \\(\\beta_0 + 4\\beta_1\\),students take 5 courses, expected study time equal \\(\\beta_0 + 5\\beta_1\\).\\(f(x) = \\beta_0 + \\beta_1x\\) gives us value expected value response variable specific value predictor variable. , value predictor variable, value response variable constant. say value \\(x\\), response variable \\(y\\) variance. variance response variable value \\(x\\) variance error term, \\(\\epsilon\\). Thus simple linear regression model\\[\\begin{equation}\ny=\\beta_0+\\beta_{1} x + \\epsilon.\n\\tag{3.2}\n\\end{equation}\\]need make assumptions error term \\(\\epsilon\\). Generally, assumptions :errors mean 0.errors variance denoted \\(\\sigma^2\\). Notice variance constant.errors independent.errors normally distributed.(3.2), notice another parameter, \\(\\sigma^2\\).go detail assumptions mean, assess whether met, module 5.assumptions mean value predictor variable \\(x\\), response variable:follows normal distribution,mean equal \\(\\beta_0+\\beta_{1} x\\),variance equal \\(\\sigma^2\\).Using study time example, means :students take 3 courses, distribution study times \\(N(\\beta_0 + 3\\beta_1, \\sigma^2)\\).students take 4 courses, distribution study times \\(N(\\beta_0 + 4\\beta_1, \\sigma^2)\\).students take 5 courses, distribution study times \\(N(\\beta_0 + 5\\beta_1, \\sigma^2)\\).subset dataframe three subsets, one students take 3 courses, another subset students take 4 courses, another subset students take 5 courses, create density plot study times subset, density plot follow normal distribution, different means, spread.Let us take look density plots next.Notice plots normal, different means (centers), similar spreads.Please see associated video explanation distribution response variable, value predictor variable, SLR setting.","code":"\nlibrary(tidyverse)\n##subset dataframe\nx.3<-df[which(df$courses==3),]\n##density plot of study time for students taking 3 courses\nggplot(x.3,aes(x=study))+\n  geom_density()+\n  labs(x=\"Study Time (Mins)\", title=\"Dist of Study Times with 3 Courses\")\n##subset dataframe\nx.4<-df[which(df$courses==4),]\n##density plot of study time for students taking 4 courses\nggplot(x.4,aes(x=study))+\n  geom_density()+\n  labs(x=\"Study Time (Mins)\", title=\"Dist of Study Times with 4 Courses\")\n##subset dataframe\nx.5<-df[which(df$courses==5),]\n##density plot of study time for students taking 5 courses\nggplot(x.5,aes(x=study))+\n  geom_density()+\n  labs(x=\"Study Time (Mins)\", title=\"Dist of Study Times with 5 Courses\")"},{"path":"slr.html","id":"estimating-regression-coefficients-in-slr","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.3 Estimating Regression Coefficients in SLR","text":"(3.1) (3.2), noted estimate regression coefficients \\(\\beta_0, \\beta_1\\) well parameter \\(\\sigma^2\\) associated error term. mentioned earlier, unable obtain numerical values parameters data entire population. use data sample estimate parameters.estimate \\(\\beta_0,\\beta_1\\) using \\(\\hat{\\beta}_0,\\hat{\\beta}_1\\) based sample observations \\((x_i,y_i)\\) size \\(n\\).subscripts associated response predictor variables denote data point value belongs . Let us take look first rows data frame study time example:example, \\(x_1\\) denotes number courses taken student number 1 dataframe, 3. \\(y_4\\) denotes study time student number 4 dataframe, 378.0196456.Following (3.1) (3.2), sample versions \\[\\begin{equation}\n\\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1 x\n\\tag{3.3}\n\\end{equation}\\]\\[\\begin{equation}\ny=\\hat{\\beta}_0+\\hat{\\beta}_1 x + e\n\\tag{3.4}\n\\end{equation}\\]respectively. (3.3) called estimated SLR equation, fitted SLR equation. (3.4) called estimated SLR model.\\(\\hat{\\beta}_1,\\hat{\\beta}_0\\) estimators \\(\\beta_1,\\beta_0\\) respectively. estimators can interpreted following manner:\\(\\hat{\\beta}_1\\) denotes change predicted \\(y\\) \\(x\\) increases 1 unit. Alternatively, denotes change \\(y\\), average, \\(x\\) increases 1 unit.\\(\\hat{\\beta}_0\\) denotes predicted \\(y\\) \\(x=0\\). Alternatively, denotes average \\(y\\) \\(x=0\\).(3.4), notice use \\(e\\) denote residual, words, “error” sample.(3.3) (3.4), following quantities can compute:\\[\\begin{equation}\n\\text{Predicted/Fitted values: } \\hat{y}_i = \\hat{\\beta}_0+\\hat{\\beta}_1 x_i.\n\\tag{3.5}\n\\end{equation}\\]\\[\\begin{equation}\n\\text{Residuals: } e_i = y_i-\\hat{y}_i.\n\\tag{3.6}\n\\end{equation}\\]\\[\\begin{equation}\n\\text{Sum Squared Residuals: } SS_{res} =  \\sum\\limits_{=1}^n(y_i-\\hat{y}_i)^2.\n\\tag{3.7}\n\\end{equation}\\]compute estimated coefficients \\(\\hat{\\beta}_1,\\hat{\\beta}_0\\) using method least squares, .e. choose numerical values \\(\\hat{\\beta}_1,\\hat{\\beta}_0\\) minimize \\(SS_{res}\\) given (3.7).minimizing \\(SS_{res}\\) respect \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\), estimated coefficients simple linear regression equation \\[\\begin{equation}\n\\hat{\\beta}_1 = \\frac{\\sum\\limits_{=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum\\limits_{=1}^n(x_i-\\bar{x})^2}\n\\tag{3.8}\n\\end{equation}\\]\\[\\begin{equation}\n\\hat{\\beta}_0 = \\bar{y}- \\hat{\\beta}_1 \\bar{x}\n\\tag{3.9}\n\\end{equation}\\]\\(\\hat{\\beta}_1, \\hat{\\beta}_0\\) called least squares estimators.minimization \\(SS_{res}\\) respect \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\) done taking partial derivatives (3.7) respect \\(\\hat{\\beta}_1\\) \\(\\hat{\\beta}_0\\), setting two partial derivatives equal 0, solving two equations \\(\\hat{\\beta}_1\\) \\(\\hat{\\beta}_0\\).Let’s take look estimated coefficients study time example:sample 6000 students, \\(\\hat{\\beta}_1\\) = 120.3930985. predicted study time increases 120.3930985 minutes additional course taken.\\(\\hat{\\beta}_0\\) = 58.4482853. predicted study time 58.4482853 courses taken. Notice value make sense, student taking 0 courses. look data, number courses taken 3, 4, 5. use regression \\(3 \\leq x \\leq 5\\). use values \\(x\\) outside range data. Making predictions response variable predictors outside range data called extrapolation done.","code":"\nhead(df)##      study courses\n## 1 429.8311       3\n## 2 458.4588       3\n## 3 391.9406       3\n## 4 378.0196       3\n## 5 397.9856       3\n## 6 405.7145       3\n##fit regression\nresult<-lm(study~courses, data=df)\n##print out the estimated coefficients\nresult## \n## Call:\n## lm(formula = study ~ courses, data = df)\n## \n## Coefficients:\n## (Intercept)      courses  \n##       58.45       120.39"},{"path":"slr.html","id":"estimating-variance-of-errors-in-slr","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.4 Estimating Variance of Errors in SLR","text":"estimator \\(\\sigma^2\\), variance error terms (also variance probability distribution \\(y\\) given \\(x\\)) \n\\[\\begin{equation}\ns^2 = MS_{res} = \\frac{SS_{res}}{n-2} = \\frac{\\sum\\limits_{=1}^n e_i^2}{n-2},\n\\tag{3.10}\n\\end{equation}\\]\\(MS_{res}\\) called mean squared residuals.\\(\\sigma^2\\), variance error terms, measures spread response variable, value \\(x\\). smaller , closer data points regression equation.","code":""},{"path":"slr.html","id":"practice-questions-1","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.4.1 Practice questions","text":"Take look scatterplot study time number courses taken, Figure 3.1. plot, label following:estimated SLR equationthe fitted value \\(x=3\\), \\(x=4\\), \\(x=5\\).residual data point plot choosing.Try first, view associated video see labeled plot correctly!","code":""},{"path":"slr.html","id":"assessing-linear-association","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.5 Assessing Linear Association","text":"noted earlier, variance error terms inform us close data points estimated SLR equation. smaller variance error terms, closer data points estimated SLR equation. turn implies linear relationship variables stronger.learn common measures used quantify strength linear relationship response predictor variables. , need define terms.","code":""},{"path":"slr.html","id":"sum-of-squares","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.5.1 Sum of squares","text":"\\[\\begin{equation}\n\\text{Total Sum Squares: } SS_T = \\sum\\limits_{=1}^{n} (y_i - \\bar{y})^{2}.\n\\tag{3.11}\n\\end{equation}\\]Total sum squares defined total variance response variable. larger value , larger spread response variable.\\[\\begin{equation}\n\\text{Regression sum squares: } SS_R = \\sum\\limits_{=1}^{n} (\\hat{y_i} - \\bar{y})^{2}.\n\\tag{3.12}\n\\end{equation}\\]Regression sum squares defined variance response variable can explained regression.also residual sum squares, \\(SS_{res}\\). mathematical formulation given (3.7). defined variance response variable explained regression.can shown \\[\\begin{equation}\nSS_T = SS_R + SS_{res}.\n\\tag{3.13}\n\\end{equation}\\]sums squares associated degrees freedom (df):df \\(SS_R\\): \\(df_R = 1\\)df \\(SS_{res}\\): \\(df_{res} = n-2\\)df \\(SS_T\\): \\(df_T = n-1\\)Please see associated video explanation concept behind degrees freedom.","code":""},{"path":"slr.html","id":"anova-table","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.5.2 ANOVA Table","text":"Information regarding sums squares usually presented form ANOVA (analysis variance) table:Note:Dividing sum square corresponding degrees freedom gives corresponding mean square.last column, report \\(F\\) statistic, equal \\(\\frac{MS_R}{MS_{res}}\\). \\(F\\) statistic associated ANOVA F test, look detail next subsection.obtain ANOVA table study time example:Notice R print information line regarding \\(SS_T\\).","code":"\nanova(result)## Analysis of Variance Table\n## \n## Response: study\n##             Df   Sum Sq  Mean Sq F value    Pr(>F)    \n## courses      1 57977993 57977993   65404 < 2.2e-16 ***\n## Residuals 5998  5317017      886                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"slr.html","id":"anova-f-test","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.5.3 ANOVA \\(F\\) Test","text":"SLR, ANOVA \\(F\\) statistic ANOVA table can used test slope SLR equation 0 . words, means whether linear association variables . slope 0, means changes value predictor variable change value response variable, average; hence variables linearly associated.null alternative hypotheses :\\[\nH_0: \\beta_1 = 0, H_a: \\beta_1 \\neq 0.\n\\]\ntest statistic \\[\\begin{equation}\nF = \\frac{MS_R}{MS_{res}}\n\\tag{3.14}\n\\end{equation}\\]compared \\(F_{1,n-2}\\) distribution. Note \\(F_{1,n-2}\\) read F distribution 1 \\(n-2\\) degrees freedom.Going back study time example, \\(F\\) statistic 6.5403586^{4}. critical value can found usingSince test statistic larger critical value, reject null hypothesis. data support claim slope different 0, words, linear association study time number courses taken.","code":"\nqf(1-0.05, 1, 6000-2)## [1] 3.84301"},{"path":"slr.html","id":"coefficient-of-determination","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.5.4 Coefficient of determination","text":"coefficient determination, \\(R^2\\), \\[\\begin{equation}\nR^{2} = \\frac{SS_R}{SS_T} = 1 - \\frac{SS_{res}}{SS_T}.\n\\tag{3.15}\n\\end{equation}\\]\\(R^{2}\\) indication well data fits model. context simple linear regression, denotes proportion variance response variable explained predictor.notes \\(R^2\\):\\(0 \\leq R^2 \\leq 1\\).Values closer 1 indicate better fit; values closer 0 indicate poorer fit.Sometimes reported percentage.obtain \\(R^2\\) study time example:implies proportion variance study time can explained number courses taken 0.9159963.","code":"\nanova.tab<-anova(result)\n##SST not provided, so we add up SSR and SSres\nSST<-sum(anova.tab$\"Sum Sq\")\n##R2\nanova.tab$\"Sum Sq\"[1]/SST## [1] 0.9159963"},{"path":"slr.html","id":"correlation","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.5.5 Correlation","text":"measure used quantify strength linear association two quantitative variables sample correlation. sample correlation, \\(\\mbox{Corr}(x,y)\\) \\(r\\), given \\[\\begin{equation}\nr = \\frac{\\sum\\limits_{=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum\\limits_{=1}^{n}(x_i - \\bar{x})^{2}(y_i - \\bar{y})^{2}}}.\n\\tag{3.16}\n\\end{equation}\\]notes \\(r\\):\\(-1 \\leq r \\leq 1\\).Sign correlation indicates direction association. positive value indicates positive linear association: predictor variable increases, response variable, average. negative value indicates negative linear association: predictor variable increases, response variable decreases, average.Values closer 1 -1 indicate stronger linear association; values closer 0 indicate weaker linear association.SLR, turns \\(r^2 = R^2\\).Using study time example, correlation study time number courses taken isThis value indicates strong positive linear association study time number courses taken (remember simulated data real).","code":"\ncor(df$study, df$courses)## [1] 0.9570769"},{"path":"slr.html","id":"how-strong-is-strong","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.5.5.1 How strong is strong?","text":"question often raised large magnitude sample correlation considered strong? answer : depends context. conducting experiment governed scientific laws (e.g experiment verifying Newton’s 2nd law \\(F = ma\\)), expect extremely high correlation. correlation 0.9 instance may considered weak. value correlation compared correlations similar studies domain determine strong .","code":""},{"path":"slr.html","id":"a-word-of-caution","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.6 A Word of Caution","text":"able use measures learned (correlation, \\(R^2\\)) interpret estimated regression coefficients, must verify via scatterplot association two variables approximately linear. see non linear pattern scatterplot, use interpret values. learn remedy situation see non linear pattern scatterplot module 5.Please see associated video demonstration looking scatterplot can lead misleading interpretations.","code":""},{"path":"slr.html","id":"r-tutorial","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.7 R Tutorial","text":"tutorial, work dataset elmhurst openintro package R.Type ?openintro::elmhurst read documentation datasets R. Always seek understand background data! key pieces information :random sample 50 students (freshman 2011 class Elmhurst College).Family income student (units missing).Gift aid, $1000s.want explore family income may related gift aid, simple linear regression framework.","code":"\nlibrary(tidyverse)\nlibrary(openintro)\nData<-openintro::elmhurst"},{"path":"slr.html","id":"visualization","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"Visualization","text":"always verify scatterplot relationship (approximately) linear proceeding correlation simple linear regression!note observations fairly evenly scattered sides regression line, linear association exists. see negative linear association. family income increases, gift aid, average, decreases.also see observation weird values may warrant investigation.","code":"\n##scatterplot of gift aid against family income\nggplot2::ggplot(Data, aes(x=family_income,y=gift_aid))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se=FALSE)+\n  labs(x=\"Family Income\", y=\"Gift Aid\", title=\"Scatterplot of Gift Aid against Family\")"},{"path":"slr.html","id":"regression","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"Regression","text":"use lm() function fit regression model:Use summary() function display relevant information regression:see following values:\\(\\hat{\\beta}_1 =\\) -0.0430717. estimated slope informs us predicted gift aid decreases 0.0430717 thousands dollars ($43.07) per unit increase family income.\\(\\hat{\\beta}_0 =\\) 24.319329. students family income, predicted gift aid $24 319.33. Note: scatterplot, observation 0 family income. must careful extrapolating making predictions regression. make predictions family incomes minimum maximum values family incomes data.\\(s\\) = 4.7825989, estimate standard deviation error terms. reported residual standard error R. Squaring gives estimated variance.\\(F\\) = 15.8772043. value ANOVA \\(F\\) statistic. corresponding p-value reported. Since p-value small, reject null hypothesis. data support claim linear association gift aid family income.\\(R^2 =\\) 0.2485582. coefficient determination informs us 24.86% variation gift aid can explained family income.","code":"\n##regress gift aid against family income\nresult<-lm(gift_aid~family_income, data=Data)\n##look at information regarding regression\nsummary(result)## \n## Call:\n## lm(formula = gift_aid ~ family_income, data = Data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.1128  -3.6234  -0.2161   3.1587  11.5707 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   24.31933    1.29145  18.831  < 2e-16 ***\n## family_income -0.04307    0.01081  -3.985 0.000229 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.783 on 48 degrees of freedom\n## Multiple R-squared:  0.2486, Adjusted R-squared:  0.2329 \n## F-statistic: 15.88 on 1 and 48 DF,  p-value: 0.0002289"},{"path":"slr.html","id":"extract-values-from-r-objects","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"Extract values from R objects","text":"can actually extract values reported summary(result). see can extracted R object, use names() function:extract estimated coefficients:Notice information presented table. extract specific value, can specify row column indices:, extract values residual standard error, ANOVA F statistic, \\(R^2\\).","code":"\n##see what can be extracted from summary(result)\nnames(summary(result))##  [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n##  [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n##  [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\"\n##extract coefficients\nsummary(result)$coefficients##                  Estimate Std. Error   t value     Pr(>|t|)\n## (Intercept)   24.31932901 1.29145027 18.831022 8.281020e-24\n## family_income -0.04307165 0.01080947 -3.984621 2.288734e-04\n##extract slope\nsummary(result)$coefficients[2,1]## [1] -0.04307165\n##extract intercept\nsummary(result)$coefficients[1,1]## [1] 24.31933"},{"path":"slr.html","id":"prediction","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"Prediction","text":"use regression models prediction. Suppose want predict gift aid student family income 50 thousand dollars (assuming unit thousands dollars). use predict() function:student’s predicted gift aid $22 165.75. Alternatively, calculated plugging \\(x=50\\) estimated SLR equation:","code":"\n##create data point for prediction\nnewdata<-data.frame(family_income=50)\n##predicted gift aid when x=50\npredict(result,newdata)##        1 \n## 22.16575\nsummary(result)$coefficients[1,1] + summary(result)$coefficients[2,1]*50## [1] 22.16575"},{"path":"slr.html","id":"anova-table-1","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"ANOVA table","text":"use anova() function display ANOVA tableThe report \\(F\\) statistic value reported earlier summary(result).first line output gives \\(SS_{R}\\), second line gives \\(SS_{res}\\). function doesn’t provide \\(SS_T\\), know \\(SS_T = SS_{R} + SS_{res}\\)., see can extracted anova.tab:\\(SS_T\\) can easily calculated:\\(R^2\\) reported 0.2485582. verify using ANOVA table:","code":"\nanova.tab<-anova(result)\nanova.tab## Analysis of Variance Table\n## \n## Response: gift_aid\n##               Df  Sum Sq Mean Sq F value    Pr(>F)    \n## family_income  1  363.16  363.16  15.877 0.0002289 ***\n## Residuals     48 1097.92   22.87                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nnames(anova.tab)## [1] \"Df\"      \"Sum Sq\"  \"Mean Sq\" \"F value\" \"Pr(>F)\"\nSST<-sum(anova.tab$\"Sum Sq\")\nSST## [1] 1461.079\nanova.tab$\"Sum Sq\"[1]/SST## [1] 0.2485582"},{"path":"slr.html","id":"correlation-1","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"Correlation","text":"use cor() function find correlation two quantitative variables:correlation -0.4985561. moderate, negative linear association family income gift aid.","code":"\n##correlation\ncor(Data$family_income,Data$gift_aid)## [1] -0.4985561"},{"path":"inf.html","id":"inf","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4 Inference with Simple Linear Regression (SLR)","text":"","code":""},{"path":"inf.html","id":"introduction-3","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.1 Introduction","text":"Oftentimes, data collect come random sample representative population interest. common example election poll presidential election. Random sampling allows sample representative population. However, obtain another random sample, characteristics new sample unlikely exactly first sample. example, sample proportion vote certain party unlikely random samples. tells us even representative samples, sample proportions unlikely equal population proportion, sample proportions vary sample sample.Dr. W. Edwards Deming’s Red Bead experiment illustrates concept. video experiment can found .video, number red beads, represent bad products, varies time worker obtains random sample 50 beads. fact number red beads increases second sample indicate performed task worse, increase due random variation associated samples.Note: Deming’s Red Bead experiment developed illustrate concepts associated management. best known work developing Japanese economy World War II. able find many blogs/articles discussing experiment World Wide Web. Although many articles discuss experiment applies management, can used illustrate concepts variation.idea extends slope intercept regression line. estimated slope intercept vary sample sample unlikely equal population slope intercept. inferential statistics, use hypothesis tests confidence intervals aid us accounting random variation. module, learn account quantify random variation associated estimated regression model, interpret estimated regression model accounting random variation.","code":""},{"path":"inf.html","id":"review-from-previous-module","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.1.1 Review from previous module","text":"simple linear regression model written \\[\\begin{equation}\ny=\\beta_0+\\beta_{1} x + \\epsilon.\n\\tag{4.1}\n\\end{equation}\\]make assumptions error term \\(\\epsilon\\). :errors mean 0.errors variance denoted \\(\\sigma^2\\). Notice variance constant.errors independent.errors normally distributed.assumptions allow us derive distributional properties associated least squares estimators \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\), enables us compute reliable confidence intervals perform hypothesis tests SLR reliably.\\(\\hat{\\beta}_1,\\hat{\\beta}_0\\) estimators \\(\\beta_1,\\beta_0\\) respectively. estimators can interpreted following manner:\\(\\hat{\\beta}_1\\) denotes change predicted \\(y\\) \\(x\\) increases 1 unit. Alternatively, denotes change \\(y\\), average, \\(x\\) increases 1 unit.\\(\\hat{\\beta}_0\\) denotes predicted \\(y\\) \\(x=0\\). Alternatively, denotes average \\(y\\) \\(x=0\\).values estimators vary sample sample?","code":""},{"path":"inf.html","id":"hypothesis-testing-in-slr","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.2 Hypothesis Testing in SLR","text":"","code":""},{"path":"inf.html","id":"distribution-of-least-squares-estimators","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.2.1 Distribution of least squares estimators","text":"Gauss Markov Theorem: assumptions regression model, least squares estimators \\(\\hat{\\beta}_1\\) \\(\\hat{\\beta}_0\\) unbiased minimum variance among unbiased linear estimators.Thus, least squares estimators following properties:\\(\\mbox{E}(\\hat{\\beta}_1) = \\beta_1\\), \\(\\mbox{E}(\\hat{\\beta}_0) = \\beta_0\\)Note: estimator unbiased expected value exactly equal parameter estimating.variance \\(\\hat{\\beta}_1\\) \\[\\begin{equation}\n\\mbox{Var}(\\hat{\\beta}_1) = \\frac{\\sigma^{2}}{\\sum{(x_{}-\\bar{x})^{2}}}\n\\tag{4.2}\n\\end{equation}\\]variance \\(\\hat{\\beta}_0\\) \\[\\begin{equation}\n\\mbox{Var}(\\hat{\\beta}_0) = \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i -\\bar{x})^2}\\right]\n\\tag{4.3}\n\\end{equation}\\]\\(\\hat{\\beta}_1\\) \\(\\hat{\\beta}_0\\) follow normal distribution.Note (4.2) (4.3), use \\(s^2 = MS_{res}\\) estimate \\(\\sigma^2\\) since \\(\\sigma^2\\) unknown value.imply standardize \\(\\hat{\\beta}_1\\) \\(\\hat{\\beta}_0\\), standardized quantities follow \\(t_{n-2}\\) distribution, .e.\\[\\begin{equation}\n\\frac{\\hat{\\beta}_1 - \\beta_1}{se(\\hat{\\beta}_1)}\\sim t_{n-2}\n\\tag{4.4}\n\\end{equation}\\]\\[\\begin{equation}\n\\frac{\\hat{\\beta}_0 - \\beta_0}{se(\\hat{\\beta}_0)}\\sim t_{n-2},\n\\tag{4.5}\n\\end{equation}\\]\\[\\begin{equation}\nse(\\hat{\\beta}_1) = \\sqrt{\\frac{MS_{res}}{\\sum{(x_{}-\\bar{x})^{2}}}} = \\frac{s}{\\sqrt{\\sum{(x_{}-\\bar{x})^{2}}}}\n\\tag{4.6}\n\\end{equation}\\]\\[\\begin{equation}\nse(\\hat{\\beta}_0) = \\sqrt{MS_{res}\\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i -\\bar{x})^2}\\right]} = s \\sqrt{\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i -\\bar{x})^2}}\n\\tag{4.7}\n\\end{equation}\\]Note:\\(se(\\hat{\\beta}_1)\\) read standard error \\(\\hat{\\beta}_1\\). standard error estimator essentially sample standard deviation estimator, measures spread estimator.\\(se(\\hat{\\beta}_1)\\) read standard error \\(\\hat{\\beta}_1\\). standard error estimator essentially sample standard deviation estimator, measures spread estimator.\\(t_{n-2}\\) distribution read \\(t\\) distribution \\(n-2\\) degrees freedom.\\(t_{n-2}\\) distribution read \\(t\\) distribution \\(n-2\\) degrees freedom.","code":""},{"path":"inf.html","id":"testing-regression-coefficients","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.2.2 Testing regression coefficients","text":"Hypothesis testing used investigate population parameter different specific value. context SLR, usually want test \\(\\beta_1\\) 0 . \\(\\beta_1 = 0\\), linear relationship variables.general steps hypothesis testing :Step 1: State null alternative hypotheses.Step 2: test statistic calculated using sample, assuming null true. value test statistic measures sample deviates null.Step 3: Make conclusion, using either critical values p-values.previous module, introduced ANOVA \\(F\\) test. SLR, tests slope SLR equation 0 . turns can also perform \\(t\\) test slope. \\(t\\) test slope, null alternative hypotheses :\\[\nH_0: \\beta_1 = 0, H_a: \\beta_1 \\neq 0.\n\\]\ntest statistic \\[\\begin{equation}\nt = \\frac{\\hat{\\beta}_1 - \\text{ value null}}{se(\\hat{\\beta}_1)}\n\\tag{4.8}\n\\end{equation}\\]compared \\(t_{n-2}\\) distribution. Notice (4.8) comes (4.4).Let us go back simulated example saw last module. data 6000 UVa undergraduate students amount time spend studying week (minutes), many courses taking semester (3 4 credit courses).\\(t\\) statistic testing \\(H_0: \\beta_1 = 0, H_a: \\beta_1 \\neq 0\\) reported 255.7412482, can calculated using (4.8): \\(t= \\frac{120.39310 - 0}{0.4707614}\\). reported p-value virtually 0, reject null hypothesis. data support claim linear association study time number courses taken.","code":"\n##create dataframe\ndf<-data.frame(study,courses)\n\n##fit regression\nresult<-lm(study~courses, data=df)\n##look at regression coefficients\nsummary(result)$coefficients##              Estimate Std. Error   t value      Pr(>|t|)\n## (Intercept)  58.44829  1.9218752  30.41211 4.652442e-189\n## courses     120.39310  0.4707614 255.74125  0.000000e+00"},{"path":"inf.html","id":"confidence-intervals-for-regression-coefficients","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.3 Confidence Intervals for Regression Coefficients","text":"Confidence intervals (CIs) similar hypothesis testing sense also based distributional properties estimator. CIs may differ use following ways:assessing parameter different specific value.interested exploring plausible range values unknown parameter.CIs hypothesis tests based distributional properties estimator, conclusions consistent (long significance level ).Recall general form CIs:\\[\\begin{equation}\n\\mbox{estimator} \\pm (\\mbox{multiplier} \\times \\mbox{s.e estimator}).\n\\tag{4.9}\n\\end{equation}\\]following components CIestimator (statistic): numerical quantity describes samplemultiplier: determined confidence level relevant probability distributionstandard error estimator: measure variance estimator (basically square root variance estimator)Following (4.9) (4.4), \\(100(1-\\alpha)\\%\\) CI \\(\\beta_1\\) \\[\\begin{equation}\n\\hat{\\beta}_1 \\pm t_{1-\\alpha/2;n-2}  se(\\hat{\\beta}_1) = \\hat{\\beta}_1 \\pm t_{1-\\alpha/2;n-2} \\frac{s}{\\sqrt{\\sum(x_i - \\bar{x})^{2}}}.\n\\tag{4.10}\n\\end{equation}\\]Going back study time example, 95% CI \\(\\beta_1\\) (119.470237, 121.3159601).interpretation CI 95% confidence true slope \\(\\beta_1\\) lies (119.470237, 121.3159601). words, additional course taken, predicted study time increases 119.470237 121.3159601 minutes.","code":"\n##CI for coefficients\nconfint(result,level = 0.95)[2,]##    2.5 %   97.5 % \n## 119.4702 121.3160"},{"path":"inf.html","id":"thought-questions","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.3.1 Thought questions","text":"conclusion 95% CI consistent hypothesis test \\(H_0: \\beta_1 = 0\\) previous section 0.05 significance level?conclusion 95% CI consistent hypothesis test \\(H_0: \\beta_1 = 0\\) previous section 0.05 significance level?presented hypothesis tests CIs slope, \\(\\beta_1\\).\ncalculate \\(t\\) statistic wanted test \\(H_0: \\beta_0 = 0, H_0: \\beta_0 \\neq 0\\)?\ncalculate 95% CI intercept \\(\\beta_0\\)?\npresented hypothesis tests CIs slope, \\(\\beta_1\\).calculate \\(t\\) statistic wanted test \\(H_0: \\beta_0 = 0, H_0: \\beta_0 \\neq 0\\)?calculate \\(t\\) statistic wanted test \\(H_0: \\beta_0 = 0, H_0: \\beta_0 \\neq 0\\)?calculate 95% CI intercept \\(\\beta_0\\)?calculate 95% CI intercept \\(\\beta_0\\)?Generally, usually interested slope intercept.","code":""},{"path":"inf.html","id":"ci-of-the-mean-response","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.4 CI of the Mean Response","text":"established least squares estimators \\(\\hat{\\beta}_1,\\hat{\\beta}_0\\) associated variances. Since estimated SLR equation \\[\\begin{equation}\n\\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1 x,\n\\tag{4.11}\n\\end{equation}\\]stands reason \\(\\hat{y}\\) associated variance well, since function \\(\\hat{\\beta}_1,\\hat{\\beta}_0\\).two interpretations \\(\\hat{y}\\):estimates mean \\(y\\) \\(x=x_0\\);predicts value \\(y\\) new observation \\(x=x_0\\).Note: \\(x_0\\) denotes specific numerical value predictor variable.Depending interpretation want, two different intervals based \\(\\hat{y}\\). first interpretation associated confidence interval mean response, \\(\\hat{\\mu}_{y|x_0}\\), given predictor. used interested average value response variable, predictor equal specific value. CI \\[\\begin{equation}\n\\hat{\\mu}_{y|x_0}\\pm t_{1-\\alpha/2,n-2}s\\sqrt{\\frac{1}{n} +\n\\frac{(x_0-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}}.\n\\tag{4.12}\n\\end{equation}\\]Going back study time example, suppose want average study time students take 5 courses, 95% CI isWe 95% confidence average study time students take 5 courses 659.2223688 661.605187 minutes.","code":"\n##CI for mean y when x=5\nnewdata<-data.frame(courses=5)\npredict(result, newdata, level=0.95, interval=\"confidence\")##        fit      lwr      upr\n## 1 660.4138 659.2224 661.6052"},{"path":"inf.html","id":"pi-of-a-new-response","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.5 PI of a New Response","text":"Previously, found CI mean \\(y\\) given specific value \\(x\\), (4.12). CI gives us idea location regression line specific \\(x\\).Instead, may interest finding interval new value \\(\\hat{y}_0\\), new observation \\(x=x_0\\). called prediction interval (PI) future observation \\(y_0\\) predictor specific value. interval follows second interpretation \\(\\hat{y}\\).PI \\(\\hat{y}_0\\) takes account:Variation location distribution \\(y\\) (.e. center distribution \\(y\\)?).Variation within probability distribution \\(y\\).comparison, confidence interval mean response (4.12) takes account first element. PI \\[\\begin{equation}\n\\hat{y}_0\\pm t_{1-\\alpha/2,n-2}s \\sqrt{1+\\frac{1}{n} +\n\\frac{(x_0-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}}.\n\\tag{4.13}\n\\end{equation}\\]Going back study time example, suppose newly enrolled student wishes take 5 courses, student wants predict study timeWe 95% confidence study time student 602.0347305 718.7928253 minutes.","code":"\n##PI for y when x=5\npredict(result, newdata, level=0.95, interval=\"prediction\")##        fit      lwr      upr\n## 1 660.4138 602.0347 718.7928"},{"path":"inf.html","id":"thought-questions-1","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.5.1 Thought questions","text":"following two scenarios, decide interested CI mean response given predictor (4.12), PI future response given predictor (4.13).\nwish estimate waiting time, average, DMV customers 10 people line DMV.\nenter DMV notice 10 people line. want estimate waiting time.\nfollowing two scenarios, decide interested CI mean response given predictor (4.12), PI future response given predictor (4.13).wish estimate waiting time, average, DMV customers 10 people line DMV.wish estimate waiting time, average, DMV customers 10 people line DMV.enter DMV notice 10 people line. want estimate waiting time.enter DMV notice 10 people line. want estimate waiting time.Look standard errors associated intervals given (4.12) (4.13). related ?Look standard errors associated intervals given (4.12) (4.13). related ?","code":""},{"path":"inf.html","id":"supplemental-notes-on-statistical-inference","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6 Supplemental Notes on Statistical Inference","text":"","code":""},{"path":"inf.html","id":"hypothesis-statements","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6.1 Hypothesis statements","text":"Let’s consider \\(t\\) test regression parameter, \\(\\beta_1\\). Depending context, following null alternative hypotheses\\(H_0: \\beta_1 = 0, H_a: \\beta_1 \\neq 0\\).\\(H_0: \\beta_1 = 0, H_a: \\beta_1 > 0\\).\\(H_0: \\beta_1 = 0, H_a: \\beta_1 < 0\\).null hypothesis stated statement equality. concept holds true hypothesis tests general. books / resources might state \\(H_0: \\beta_1 = 0, H_a: \\beta_1 \\neq 0\\).\\(H_0: \\beta_1 \\leq 0, H_a: \\beta_1 > 0\\).\\(H_0: \\beta_1 \\geq 0, H_a: \\beta_1 < 0\\).prefer using equality statement null hypothesis following reasons (theoretical, pedagogical, practical):null hypothesis equality aligns definition p-value.p-value probability observing sample estimate (value extreme), null hypothesis true (.e. \\(\\beta_1\\) truly 0). assuming calculation test statistic.People tend get confused null alternative hypotheses involve inequalities (alternative hypothesis trying support).Conclusions made terms supporting (supporting) alternative hypothesis.","code":""},{"path":"inf.html","id":"sample-size-and-statistical-inference","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6.2 Sample size and statistical inference","text":"Generally speaking, relationship sample size statistical inference (assuming characteristics remain sample randomly obtained representative population interest):Larger sample sizes (typically) lead narrower confidence intervals (precise intervals).Sample estimates based larger samples likely closer true parameters.Larger sample (typically) lead evidence null hypothesis.\nmeans larger sample size leads powerful test. power test probability hypothesis test able correctly reject null hypothesis.\nmeans larger sample size leads powerful test. power test probability hypothesis test able correctly reject null hypothesis.","code":""},{"path":"inf.html","id":"small-sample-sizes","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6.2.1 Small sample sizes","text":"Small sample sizes tend result :Confidence intervals wide.Sample estimates likely away true parameters.Hypothesis tests likely incorrectly fail reject null hypothesis alternative hypothesis true.larger sample sizes advantages, also disadvantages sample sizes extremely large.","code":""},{"path":"inf.html","id":"large-sample-sizes","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6.2.2 Large sample sizes","text":"“statistically significant” result necessarily mean result practical consequences. Suppose 95% confidence interval \\(\\beta_1\\) \\((0.001, 0.002)\\). interval excludes 0, “statistically significantly” different 0 (!), result practical consequences? narrow CI barely excludes null value can happen large sample size.one conduct corresponding hypothesis test, reject null hypothesis \\(\\beta_1 = 0\\). large sample sizes, hypothesis tests sensitive small departures null hypothesis.instances, may worth considering hypothesis tests involving different value null hypothesis, one makes sense question. example, practically significant slope may need greater specific numerical value certain context.Statistical inference assess statistical significance.Subject area knowledge assess practical significance.","code":""},{"path":"inf.html","id":"questions","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6.2.3 Questions","text":"following results statistically significant? , results also practically significant? Assume two-sided test null value 0 (made examples):assessing studying associated better test scores, SLR carried test scores (100 points) study time (hours). 95% confidence interval slope \\(\\beta_1\\) (5.632, 7.829).assessing studying associated better test scores, SLR carried test scores (100 points) study time (hours). 95% confidence interval slope \\(\\beta_1\\) (5.632, 7.829).SLR carried explore linear relationship number years school income (thousands dollars). 95% confidence interval slope \\(\\beta_1\\) (0.051, 0.243).SLR carried explore linear relationship number years school income (thousands dollars). 95% confidence interval slope \\(\\beta_1\\) (0.051, 0.243).","code":""},{"path":"inf.html","id":"cautions-using-slr-and-correlation","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6.3 Cautions using SLR and Correlation","text":"Simple linear regression correlation meant assessing linear relationships. relationship linear, need transform variable(s) (transformed variables linear relationship. explore Module 5).Always verify via scatterplot relationship least approximately linear.high correlation significant estimated slope prove strong linear relationship variables. Conversely, correlation close 0 insignificant estimated slope also proof relationship variables.","code":""},{"path":"inf.html","id":"outliers","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6.3.1 Outliers","text":"SLR correlation sensitive outliers / influential observations. Generally speaking, data “far away” different rest observations. data points can visually inspected scatterplot. potential considerations dealing data points:Investigate observations. usually something making ``stand ” rest data.Data entry errors can corrected. sure mention report.Revisit data sampled. Perhaps data point part population interest. , data point can removed (legitimate), sure mention report.regards regression analysis:Exclusion data points must clearly documented.Fit regression without data points question, see similar different conclusions become.data points large value(s) predictor /response, log transformation variable can pull large values.Consider subsetting data create separate models subset; focus subset make clear analysis subset.Knowing data context can help lot decisions.","code":""},{"path":"inf.html","id":"association-and-causation","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6.3.2 Association and causation","text":"Two correlated variables mean one variable causes variable change. example, consider plot ice cream consumption deaths drowning various months. may positive correlation, clearly, eating ice cream cause drownings. correlation can explained third (lurking) variable: weather.lurking variable variable impact relationship variables studied, studied.carefully designed randomized experiment can control lurking variables, causal relationships can established. Typically, experiments include:control group treatment group.Random assignment large number observations treatment control groups. Due random assignment, general characteristics subjects group similar.Lurking variables always issue observational studies. Researchers observational studies intervene observations simply observe data observations generate. Causal relationships much difficult establish observational studies.","code":""},{"path":"inf.html","id":"questions-1","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6.3.3 Questions","text":"Consider palmerpenguins dataset working . data contain size measurements three different species penguins three islands Palmer Archipelago, Antarctica three years. observational study randomized experiment?Consider palmerpenguins dataset working . data contain size measurements three different species penguins three islands Palmer Archipelago, Antarctica three years. observational study randomized experiment?fertilizer company wishes evaluate effective new fertilizer terms improving yield crops. large field divided many smaller plots, smaller plot randomly assigned receive either new fertilizer standard fertilizer. observational study randomized experiment?fertilizer company wishes evaluate effective new fertilizer terms improving yield crops. large field divided many smaller plots, smaller plot randomly assigned receive either new fertilizer standard fertilizer. observational study randomized experiment?professor wishes evaluate effectiveness various teaching methods (traditional vs flipped classroom). professor uses traditional approach section meets Mondays, Wednesdays, Fridays 9 10am uses flipped classroom approach section meets Mondays, Wednesdays, Fridays 2 3pm. Students free choose whichever section wanted register , knowledge teaching method used. potential lurking variables study?professor wishes evaluate effectiveness various teaching methods (traditional vs flipped classroom). professor uses traditional approach section meets Mondays, Wednesdays, Fridays 9 10am uses flipped classroom approach section meets Mondays, Wednesdays, Fridays 2 3pm. Students free choose whichever section wanted register , knowledge teaching method used. potential lurking variables study?","code":""},{"path":"inf.html","id":"r-tutorial-1","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.7 R Tutorial","text":"tutorial, continue work dataset elmhurst openintro package R.key pieces information :random sample 50 students (freshman 2011 class Elmhurst College).Family income student (units missing).Gift aid, $1000s.want explore family income may related gift aid, simple linear regression framework.","code":"\nlibrary(tidyverse)\nlibrary(openintro)\nData<-openintro::elmhurst"},{"path":"inf.html","id":"hypothesis-test-for-beta_1-and-beta_0","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"Hypothesis test for \\(\\beta_1\\) (and \\(\\beta_0\\))","text":"Applying summary() function lm() gives results hypothesis tests \\(\\beta_1\\) \\(\\beta_0\\):coefficients, can see results hypothesis tests \\(\\beta_1\\) \\(\\beta_0\\). Specifically, \\(\\beta_1\\):\\(\\hat{\\beta}_1\\) = -0.0430717\\(se(\\hat{\\beta}_1)\\) = 0.0108095the test statistic \\(t\\) = -3.984621the corresponding p-value 2.2887345^{-4}can work p-value using R (slight difference due rounding):find critical value using R:Either way, end rejecting null hypothesis. data support claim linear association gift aid family income.Note:\\(t\\) tests regression coefficients based \\(H_0: \\beta_j = 0, H_a: \\beta_j \\neq 0\\). reported p-value based set null alternative hypotheses. null alternative hypotheses different, need compute test statistic p-value.\\(t\\) tests regression coefficients based \\(H_0: \\beta_j = 0, H_a: \\beta_j \\neq 0\\). reported p-value based set null alternative hypotheses. null alternative hypotheses different, need compute test statistic p-value.SLR, two-sided \\(t\\) test \\(\\beta_1\\) gives exact result ANOVA \\(F\\) test. Notice p-values . \\(F\\) statistic \\(15.88\\) squared \\(t\\) statistic, \\((-3.985)^2\\).SLR, two-sided \\(t\\) test \\(\\beta_1\\) gives exact result ANOVA \\(F\\) test. Notice p-values . \\(F\\) statistic \\(15.88\\) squared \\(t\\) statistic, \\((-3.985)^2\\).","code":"\n##Fit a regression model\nresult<-lm(gift_aid~family_income, data=Data)\n\n##look at t stats and F stat\nsummary(result)## \n## Call:\n## lm(formula = gift_aid ~ family_income, data = Data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.1128  -3.6234  -0.2161   3.1587  11.5707 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   24.31933    1.29145  18.831  < 2e-16 ***\n## family_income -0.04307    0.01081  -3.985 0.000229 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.783 on 48 degrees of freedom\n## Multiple R-squared:  0.2486, Adjusted R-squared:  0.2329 \n## F-statistic: 15.88 on 1 and 48 DF,  p-value: 0.0002289\n##pvalue\n2*pt(-abs(-3.985), df = 50-2)## [1] 0.0002285996\n##critical value\nqt(1-0.05/2, df = 50-2)## [1] 2.010635"},{"path":"inf.html","id":"confidence-interval-for-beta_1-and-beta_0","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"Confidence interval for \\(\\beta_1\\) (and \\(\\beta_0\\))","text":"find 95% confidence intervals coefficients, use confint() function:95% CI \\(\\beta_1\\) (-0.0648056, -0.0213378). 95% confidence additional thousand dollars family income, predicted gift aid decreases $21.3378 $64.8056.","code":"\n##to produce 95% CIs for all regression coefficients\nconfint(result,level = 0.95)##                     2.5 %      97.5 %\n## (Intercept)   21.72269421 26.91596380\n## family_income -0.06480555 -0.02133775"},{"path":"inf.html","id":"confidence-interval-for-mean-response-for-given-x","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"Confidence interval for mean response for given x","text":"Suppose want confidence interval average gift aid Elmhurst College students family income 80 thousand dollars. can use predict() function:95% CI mean gift aid students family income 80 thousand dollars (19.4336609, 22.3135327). 95% confidence mean gift aid students family income 80 thousand dollars $19 433.66 $22 313.53.","code":"\n##to produce 95% CI for the mean response when x=80, \nnewdata<-data.frame(family_income=80)\npredict(result,newdata,level=0.95, interval=\"confidence\")##       fit      lwr      upr\n## 1 20.8736 19.43366 22.31353"},{"path":"inf.html","id":"prediction-interval-for-a-response-for-a-given-x","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"Prediction interval for a response for a given x","text":"prediction interval gift aid Elmhurst College student family income 80 thousand dollars:95% confidence Elmhurst College student family income 80, student’s gift aid $11 150.32 $30 596.87.","code":"\n##and the 95% PI for the response of an observation when x=80\npredict(result,newdata,level=0.95, interval=\"prediction\")##       fit      lwr      upr\n## 1 20.8736 11.15032 30.59687"},{"path":"inf.html","id":"visualization-of-ci-for-mean-response-given-x-and-pi-of-response-given-x","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"Visualization of CI for mean response given x and PI of response given x","text":"using ggplot() function create scatterplot, can overlay SLR equation adding layer via geom_smooth(method = lm). default, CI mean response value predictor gets overlaid well. previous tutorial, removed adding se=FALSE inside geom_smooth():Overlaying prediction intervals require bit work. need compute lower upper bounds PI value predictor:Previously, used predict() function, provided numerical value \\(x\\) make prediction . supplied, function use current values \\(x\\) make predictions, actually print warning message. purpose, issue since exactly want.add preds data frame order overlay lower upper bounds scatterplot, adding extra layers via geom_line() ggplot() function:mentioned notes, CI captures location regression line, whereas PI captures data points.","code":"\n##regular scatterplot\n##with regression line overlaid, and bounds of CI for mean y\nggplot2::ggplot(Data, aes(x=family_income, y=gift_aid))+\n  geom_point() +\n  geom_smooth(method=lm)+\n  labs(x=\"Family Income\", \n       y=\"Gift Aid\", \n       title=\"Scatterplot of Gift Aid against Family Income\")## `geom_smooth()` using formula = 'y ~ x'\n##find PIs for each observation\npreds <- predict(result, interval=\"prediction\")## Warning in predict.lm(result, interval = \"prediction\"): predictions on current data refer to _future_ responses\n##add preds to data frame\nData<-data.frame(Data,preds)\n\n##overlay PIs via geom_line()\nggplot2::ggplot(Data, aes(x=family_income, y=gift_aid))+\n  geom_point() +\n  geom_line(aes(y=lwr), color = \"red\", linetype = \"dashed\")+\n  geom_line(aes(y=upr), color = \"red\", linetype = \"dashed\")+\n  geom_smooth(method=lm)+\n  labs(x=\"Family Income\", \n       y=\"Gift Aid\", \n       title=\"Scatterplot of Gift Aid against Family Income\")## `geom_smooth()` using formula = 'y ~ x'"},{"path":"diag.html","id":"diag","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5 Model Diagnostics and Remedial Measures in SLR","text":"","code":""},{"path":"diag.html","id":"introduction-4","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.1 Introduction","text":"regression model based number assumptions. assumptions made can apply commonly used probability distributions quantify variability associated estimated regression model. means assumptions met regression model, quantify variability associated model longer reliable. analysis statistical inference becomes questionable.module, learn assess whether regression assumptions met. explore ways can transform variables diagnosing assumptions met can still proceed build regression model.","code":""},{"path":"diag.html","id":"assumptions-in-linear-regression","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.2 Assumptions in Linear Regression","text":"module 3, stated SLR model \\[\\begin{equation}\ny=\\beta_0+\\beta_{1} x + \\epsilon.\n\\tag{5.1}\n\\end{equation}\\]\\(f(x) = \\beta_0 + \\beta_1 x\\). need make assumptions error term \\(\\epsilon\\). Mathematically, assumptions expressed \\[\\begin{equation}\n\\epsilon_1,\\ldots,\\epsilon_n \\ ..d. \\sim N(0,\\sigma^2)\n\\tag{5.2}\n\\end{equation}\\]Breaking (5.2) assumptions can expressed following:errors mean 0.errors constant variance denoted \\(\\sigma^2\\).errors independent.errors normally distributed.Let’s dig little deeper meaning implications 4 assumptions.","code":""},{"path":"diag.html","id":"assumption-1-errors-have-mean-0.","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.2.1 Assumption 1: Errors have mean 0.","text":"value predictor, errors mean 0. -product statement relationship \\(y\\) \\(x\\), expressed via \\(y \\approx f(x)\\), correct. , \\(f(x) = \\beta_0 + \\beta_1 x\\), relationship approximately linear.plots Figure 5.1 based simulated data. scatterplot shown Figure 5.1() example assumption met. move left right plot, data points generally evenly scattered sides regression line overlaid.\nFigure 5.1: Assumption 1\nscatterplot shown Figure 5.1(b) example assumption met. move left right plot Figure 5.1(b), data points generally evenly scattered sides regression line overlaid.\\(-2 \\leq x \\leq -1.2\\), data points generally regression line;\\(-1.2 < x < 1\\), data points generally regression line;\\(x \\geq 1\\), data points generally regression line.Please see associated video explanation use Figure 5.1 assess assumption 1.","code":""},{"path":"diag.html","id":"consequences-of-violating-this-assumption","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.2.1.1 Consequences of violating this assumption","text":"Predictions biased. means predicted values systematically - - estimate true values response variable. 4 assumptions listed, crucial assumption.Using Figure 5.1(b) example, implies thatwhen \\(-2 \\leq x \\leq -1.2\\), regression line systematically -predict response variable;\\(-1.2 < x < 1\\), regression line systematically -predict response variable;\\(x \\geq 1\\), regression line systematically -predict response variable.","code":""},{"path":"diag.html","id":"assumption-2-errors-have-constant-variance","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.2.2 Assumption 2: Errors have constant variance","text":"value predictor, error terms constant variance, denoted \\(\\sigma^2\\). implies looking scatterplot, vertical variation data points around regression equation magnitude everywhere.plots Figure 5.2 based simulated data. scatterplot shown Figure 5.2() example assumption met (figure actually Figure 5.1(), data produced plots satisfy assumptions). move left right plot, vertical variation data points regression line approximately constant.\nFigure 5.2: Assumption 2\nscatterplot shown Figure 5.2(b) example assumption met. move left right plot Figure 5.2(b), vertical variation data points regression line becomes larger value response variable gets larger, variance constant.Please see associated video explanation use Figure 5.2 assess assumption 2.","code":""},{"path":"diag.html","id":"consequences-of-violating-this-assumption-1","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.2.2.1 Consequences of violating this assumption","text":"Statistical inference longer reliable. means results hypothesis test, confidence interval, prediction interval longer reliable.Interestingly, scatterplot Figure 5.2(b), can say assumption 1 met, since data points generally evenly scattered sides regression line. Predictions still unbiased; predicted response, \\(\\hat{y}\\), systematically - -predict response variable. goal assess relationship approximately linear, scatterplot fine. lose utility hypothesis tests, CIs, PIs.","code":""},{"path":"diag.html","id":"assumption-3-errors-are-independent","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.2.3 Assumption 3: Errors are independent","text":"-product assumption values response variable, \\(y_i\\), independent . \\(y_i\\) depend values response variable.","code":""},{"path":"diag.html","id":"consequences-of-violating-this-assumption-2","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.2.3.1 Consequences of violating this assumption","text":"Statistical inference longer reliable. means results hypothesis test, confidence interval, prediction interval longer reliable.","code":""},{"path":"diag.html","id":"assumption-4-errors-are-normally-distributed","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.2.4 Assumption 4: Errors are normally distributed","text":"create density plot errors, errors follow normal distribution.","code":""},{"path":"diag.html","id":"consequences-of-violating-this-assumption-3","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.2.4.1 Consequences of violating this assumption","text":"regression model fairly robust assumption errors normally distributed. words, violation particular assumption consequential. 4 assumptions, least crucial satisfy.","code":""},{"path":"diag.html","id":"assessing-regression-assumptions","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.3 Assessing Regression Assumptions","text":"visualizations help detecting violations regression assumptions. visualizations :Scatterplot \\(y\\) \\(x\\) (assumptions 1 2).Residual plot (assumptions 1 2).Autocorrelation function (ACF) plot residuals (assumption 3).Normal probability plot residuals (often called QQ plot) (assumption 4).","code":""},{"path":"diag.html","id":"scatterplot","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.3.1 Scatterplot","text":"can examine scatterplot \\(y\\) \\(x\\) check assumptions 1 2. want see following scatterplot:nonlinear pattern (assumption 1).Data points evenly scattered (value x-axis) around fitted line (assumption 1).Vertical variation data points constant (assumption 2).used Figure 5.2() example scatterplot meets assumptions. Let us take look another example worked . scatterplot elmhurst dataset openintro package seeing tutorials. regressing amount gift aid student receives based student’s family income. corresponding scatterplot shown Figure 5.3.\nFigure 5.3: Scatterplot Gift Aid Family Income\nFigure 5.3, see data points evenly scattered around fitted line. also see vertical variation data points fairly constant. assumptions errors 0 mean constant variance appear met.","code":""},{"path":"diag.html","id":"practice-question","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.3.1.1 Practice question","text":"data prices used cars. regressing sale price car age car. corresponding scatterplot shown Figure 5.4. Based Figure 5.4, assumptions 1 2 (, neither), met? go tutorial.\nFigure 5.4: Scatterplot Sale Price Age\n","code":""},{"path":"diag.html","id":"residual-plot","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.3.2 Residual plot","text":"using scatterplot intuitive way assessing regression assumptions, limitation. used multiple predictors regression, encounter (happens often just one predictor). Another visualization can use assess assumptions 1 2 residual plot. scatterplot residuals, \\(e\\), fitted values, \\(\\hat{y}\\). want observe following residual plot.Residuals evenly scattered across horizontal axis (assumption 1).residuals similar vertical variation across plot (assumption 2).writers combine two points following statement: residuals fall horizontal band around 0 apparent pattern (assumption 1, 2).residual plots Figure 5.5 based simulated data Figures 5.1(), 5.1(b), 5.2(b).\nFigure 5.5: Residual Plots Fig 1(), 1(b), 2(b) Respectively\nmake following observations:Figure 5.5(), see residuals evenly scattered across horizontal axis, vertical variation fairly constant across plot. assumptions met.Figure 5.5(b), see residuals evenly scattered across horizontal axis, although vertical variation fairly constant across plot. assumption 1 met.Figure 5.5(c), see residuals evenly scattered across horizontal axis, vertical variation constant across plot. fact, vertical variation increasing move left right. assumption 2 met.compare conclusions residuals plots scatterplots, . SLR, takeaways consistent.Please see associated video explanation use Figure 5.5 assess assumptions 1 2.","code":""},{"path":"diag.html","id":"practice-questions-2","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.3.2.1 Practice questions","text":"residual plot Figure 5.6() comes regressing gift aid family income elmhurst dataset. Based residual plot, assumptions met?residual plot Figure 5.6() comes regressing gift aid family income elmhurst dataset. Based residual plot, assumptions met?residual plot Figure 5.6(b) comes regressing price cars age used cars dataset. Based residual plot, assumptions met?residual plot Figure 5.6(b) comes regressing price cars age used cars dataset. Based residual plot, assumptions met?Please see associated video go practice questions.\nFigure 5.6: Residual Plots Practice Questions\n","code":""},{"path":"diag.html","id":"acf-plot","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.3.3 ACF plot","text":"Assumption 3 states errors independent. assumption implies values response variable independent . assumption typically assessed via knowing nature data.observations obtained random sample, likely observations independent . nature random sample random samples preferred convenience samples.observations obtained random sample, likely observations independent . nature random sample random samples preferred convenience samples.data inherent sequence, likely observations independent, dependent. example, record value stock end day, value day 2 likely related value day 1. values stock prices end day independent.data inherent sequence, likely observations independent, dependent. example, record value stock end day, value day 2 likely related value day 1. values stock prices end day independent.autocorrelation function (ACF) plot residuals may used help assess assumption errors independent met. However, plot substitute using understanding nature data used confirmation.ACF plot measures correlation vector observations lagged versions observations. observations uncorrelated, correlations vector observations lagged versions observations theoretically 0. may create ACF plot residuals regression.ACF plot Figure 5.7() based simulated data independently generated.\nFigure 5.7: Assumption 3\nnotes ACF plot:ACF lag 0 always 1. correlation vector always 1.dashed horizontal lines represent critical values. ACF lag beyond critical value indicates ACF significant. evidence correlation (hence dependence) residuals.observed values response variable independent, expect ACFs lags greater 0 insignificant. note conducting multiple hypothesis tests, alarmed ACFs slightly beyond critical values isolated lag 2.Based Figure 5.7(), see ACFs lags greater 0 insignificant. evidence residuals correlated , evidence assumption 3 met.Sometimes, dataframe can sorted manner (e.g. increasing order response variable), , actually expect see significant correlations ACF plot. ACF plot Figure 5.7(b) example. residuals simulated dataset, data sorted response variable. just looked ACF plot Figure 5.7(b) without understanding data simulated independently sorted, erroneously concluded residuals independent regression assumption met.","code":""},{"path":"diag.html","id":"qq-plot","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.3.4 QQ plot","text":"normal probability plot (also called QQ plot) used assess distribution variable normal. typically plots residuals theoretical residual followed normal distribution. QQ line typically overlaid. plots fall closely QQ line, evidence observations follow normal distribution. Figure 5.8 shows QQ plot comes normally distributed variable.\nFigure 5.8: QQ Plot\n","code":""},{"path":"diag.html","id":"remedial-measures","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.3.5 Remedial measures","text":"now know assess specific regression assumptions met. remedial measures involve transforming either predictor variable / response variable. transformations chosen handle violations assumptions 1 / 2 respectively. general strategy selecting variable transform:Transforming response variable, \\(y\\), affects assumptions 1 2.\nVisually, can think transforming \\(y\\) terms stretching squeezing scatterplot \\(y\\) \\(x\\) vertically. Thus, transforming \\(y\\) affects shape relationship vertical spread data points.\nHowever, choice transform \\(y\\) based handling assumption 2.\nVisually, can think transforming \\(y\\) terms stretching squeezing scatterplot \\(y\\) \\(x\\) vertically. Thus, transforming \\(y\\) affects shape relationship vertical spread data points.However, choice transform \\(y\\) based handling assumption 2.Transforming predictor variable, \\(x\\) affects assumption 1 theoretically affect assumption 2.\nVisually, can think transforming \\(x\\) terms stretching squeezing scatterplot \\(y\\) \\(x\\) horizontally. Thus, transforming \\(x\\) affects shape relationship vertical spread data points.\nTherefore, transforming \\(x\\) based handling assumption 1.\nVisually, can think transforming \\(x\\) terms stretching squeezing scatterplot \\(y\\) \\(x\\) horizontally. Thus, transforming \\(x\\) affects shape relationship vertical spread data points.Therefore, transforming \\(x\\) based handling assumption 1.assumption 2 met, transform \\(y\\) stabilize variance make constant.assumption 1 met, transform \\(x\\) find appropriate shape relate variables.assumptions met, transform \\(y\\) first stabilize variance. assumption 2 solved, check assumption 1 met. met, transform \\(x\\).Assumption 1 deals whether way expressed \\(y\\) \\(x\\) related, \\(f(x)\\), appropriate. Assumption 2 deals vertical variation data points scatterplot.","code":""},{"path":"diag.html","id":"remedial-measures-variance-stabilizing-transformations","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.4 Remedial Measures: Variance Stabilizing Transformations","text":"transform response variable stabilize variance (assumption 2). couple ways decide appropriate transformation:Pattern seen residual plot can guide choice transform response variable.Box-Cox plot.","code":""},{"path":"diag.html","id":"use-pattern-in-residual-plot","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.4.1 Use Pattern in Residual Plot","text":"can stabilize variance errors based residual plot, see either following scenarios:vertical variation residuals increasing fitted response increases, move left right, Figure 5.9(), orvertical variation residuals decreasing fitted response increases, move left right, Figure 5.9(b).\nFigure 5.9: Non Constant Variance Residual Plot\nNote increasing variance fitted response increases much common real data. Generally, larger values variable associated larger spread.transform \\(y\\) using \\(y^{*} = y^{\\lambda}\\), \\(\\lambda\\) chosen based whether variance residuals increasing decreasing fitted response:Figure 5.9(), choose \\(\\lambda < 1\\).\n\\(\\lambda = 0\\), means use logarithmic transformation base e, .e. \\(y^* = \\log(y)\\).\nNote logarithm base means natural log, ln.\n\\(\\lambda = 0\\), means use logarithmic transformation base e, .e. \\(y^* = \\log(y)\\).Note logarithm base means natural log, ln.Figure 5.9(b), choose \\(\\lambda > 1\\).based residual plot, range values \\(\\lambda\\).","code":""},{"path":"diag.html","id":"box-cox-plot","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.4.2 Box-Cox Plot","text":"can use Box-Cox plot help us narrow range \\(\\lambda\\) use. plot log-likelihood function \\(\\lambda\\), choose \\(\\lambda\\) maximizes log-likelihood function. example, Figure 5.10 shows Box Cox plot generated regression associated residual plot Figure 5.9(b).\nFigure 5.10: Box-Cox Plot based Figure 9(b)\nNotice approximate 95% CI provided \\(\\lambda\\). comments use Box-Cox plot:Three vertical dashed lines displayed: middle line corresponds optimal value \\(\\lambda\\); two lines lower upper bounds 95% CI \\(\\lambda\\).choose \\(\\lambda\\) within CI (even close ) easy understand. choose optimal value, especially value difficult interpret. example, choose \\(\\lambda = 2\\), square transformation \\(y\\). Transform response \\(y^* = y^2\\). Regress \\(y^*\\) \\(x\\).1 lies CI, transformation \\(y\\) may needed.transformation needed, log transformation preferred, since can still interpret estimated coefficients. difficult interpret type transformation.View Box-Cox procedure guide selecting transformation, rather definitive.Need recheck residuals every transformation assess transformation worked.","code":""},{"path":"diag.html","id":"interpretation-with-log-transformed-response","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.4.3 Interpretation with Log Transformed Response","text":"log transformation response preferred transformation, can still interpret regression coefficients. couple ways interpret estimated slope \\(\\hat{\\beta}_1\\):predicted response variable multiplied factor \\(\\exp(\\hat{\\beta_1})\\) one-unit increase predictor.can also subtract 1 \\(\\exp(\\hat{\\beta_1})\\) express change percentage.\n\\(\\hat{\\beta}_1\\) positive, percent increase. predicted response variable increases \\((\\exp(\\hat{\\beta_1}) - 1) \\times 100\\) percent one-unit increase predictor.\n\\(\\hat{\\beta}_1\\) negative, percent decrease. predicted response variable decreases \\((1 - \\exp(\\hat{\\beta_1})) \\times 100\\) percent one-unit increase predictor.\n\\(\\hat{\\beta}_1\\) positive, percent increase. predicted response variable increases \\((\\exp(\\hat{\\beta_1}) - 1) \\times 100\\) percent one-unit increase predictor.\\(\\hat{\\beta}_1\\) negative, percent decrease. predicted response variable decreases \\((1 - \\exp(\\hat{\\beta_1})) \\times 100\\) percent one-unit increase predictor.Please see associated video go math explaining interpret regression coefficients response variable log transformed.","code":""},{"path":"diag.html","id":"remedial-measures-linearization-transformations","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.5 Remedial Measures: Linearization Transformations","text":"first ensure variance stabilized assumption 2 met. \\(f(x)\\) accurately capture relationship variables, transform predictor variable meet assumption 1. writers call linearization transformation, seek make transformed version predictor variable, \\(x^*\\), approximately linear response variable (transformed \\(y\\)), .e. \\(y = \\beta_0 + \\beta_1 x^* + \\epsilon\\). consider transforming response variable deal assumption 1, transforming response variable likely reintroduce violation assumption 2.general strategy transform predictor via scatterplot \\(y\\) (\\(y^*\\)) \\(x\\). use pattern seen plot decide transform predictor. examples shown Figure 5.11 .\nFigure 5.11: Transformations x\n","code":""},{"path":"diag.html","id":"hierarchical-principle","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.5.1 Hierarchical Principle","text":"One thing aware hierarchical principle: relationship response predictor higher order polynomial (e.g. quadratic, cubic), hierarchical principle states lower order terms remain model. example, relationship order \\(h\\), fit \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_h x^h + \\epsilon\\) via multiple linear regression framework. see next module.","code":""},{"path":"diag.html","id":"interpretation-with-log-transformed-predictor","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.5.2 Interpretation with Log Transformed Predictor","text":"log transformation predictor preferred transformation, can still interpret regression coefficient, \\(\\hat{\\beta}_1\\), couple ways:\\(\\%\\) increase predictor, predicted response increases \\(\\hat{\\beta}_1 \\log(1+ \\frac{}{100})\\).\\(\\log(1 + \\frac{1}{100}) \\approx \\frac{1}{100}\\) (Taylor series). alternative interpretation : 1% increase predictor, predicted response increases approximately \\(\\frac{\\hat{\\beta}_1}{100}\\).Please see associated video go math explaining interpret regression coefficients predictor variable log transformed.","code":""},{"path":"diag.html","id":"interpretation-with-log-transformed-response-and-predictor","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.5.3 Interpretation with Log Transformed Response and Predictor","text":"response predictor variables log transformed, regression coefficient, \\(\\hat{\\beta}_1\\), can interpreted couple ways:\\(\\%\\) increase predictor, predicted response multiplied \\((1 + \\frac{}{100})^{\\hat{\\beta}_1}\\).\\(\\%\\) increase predictor, predicted response multiplied \\((1 + \\frac{}{100})^{\\hat{\\beta}_1}\\).\\((1 + \\frac{1}{100})^{\\hat{\\beta}_1} \\approx 1 + \\frac{\\hat{\\beta}_1}{100}\\) (Taylor series). alternative interpretation : 1% increase predictor, predicted response increases approximately \\(\\hat{\\beta}_1\\) percent. Note approximation works better \\({\\hat{\\beta}_1}\\) small magnitude.\\((1 + \\frac{1}{100})^{\\hat{\\beta}_1} \\approx 1 + \\frac{\\hat{\\beta}_1}{100}\\) (Taylor series). alternative interpretation : 1% increase predictor, predicted response increases approximately \\(\\hat{\\beta}_1\\) percent. Note approximation works better \\({\\hat{\\beta}_1}\\) small magnitude.Please see associated video go math explaining interpret regression coefficients response predictor variables log transformed.","code":""},{"path":"diag.html","id":"some-general-comments-about-assessing-assumptions-and-transformations","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.5.4 Some General Comments about Assessing Assumptions and Transformations","text":"assessing assumptions residual plot, assessing assumptions reasonably / approximately met.assessing assumptions residual plot, assessing assumptions reasonably / approximately met.real data, assumptions rarely met 100%.real data, assumptions rarely met 100%.unsure, proceed model building, test model performs new data. poor performance, go back residual plot assess transformation appropriate.unsure, proceed model building, test model performs new data. poor performance, go back residual plot assess transformation appropriate.Assess plots decide variables need transformed, . choice transformation guided see plots, trial error.Assess plots decide variables need transformed, . choice transformation guided see plots, trial error.residual plot always produced transformation. Box Cox plot also produced. plots assessed transformation helped way intended.residual plot always produced transformation. Box Cox plot also produced. plots assessed transformation helped way intended.Solve assumption 2 first, assumption 1.Solve assumption 2 first, assumption 1.","code":""},{"path":"diag.html","id":"r-tutorial-2","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.6 R Tutorial","text":"","code":""},{"path":"diag.html","id":"example-1","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.6.1 Example 1","text":"linear regression model involves several assumptions. Among :errors, fixed value \\(x\\), mean 0. implies relationship specified regression equation appropriate.errors, fixed value \\(x\\), constant variance. , variation errors theoretically regardless value \\(x\\) (\\(\\hat{y}\\)).errors independent.errors, fixed value \\(x\\), follow normal distribution.assess assumptions 1 2, can examine scatterplots :\\(y\\) versus \\(x\\).residuals versus fitted values, \\(\\hat{y}\\).Assumption 3 assessed based knowledge data. autocorrelation (ACF) plot residuals may also used.Assumption 4 assessed normal probability plot, considered least crucial assumptions.see generate relevant graphical displays help us assess whether assumptions met, needed, carry transformations variable(s) assumptions met.tutorial, go dataset involving prices used cars (Mazdas). two variables sales price used car, age car years. Download data file, mazda.txt read data:","code":"\nData<-read.table(\"mazda.txt\", header=TRUE, sep=\"\")"},{"path":"diag.html","id":"model-diagnostics-with-scatterplots","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"Model Diagnostics with Scatterplots","text":"can use scatterplot response variable predictor assess assumptions 1 2:assess assumption 1, data points evenly scattered sides regression line, move left right. see scatterplot, assumption 1 met. age 0 2, data points mostly line. age 5 11, data points mostly line, age greater 13, data points line.assess assumption 2, vertical spread data points constant move left right. spread seems decreasing move left right (words, spread increasing response increases), assumption 2 met.","code":"\nlibrary(tidyverse)\n\n##scatterplot, and overlay regression line\nggplot2::ggplot(Data, aes(x=Age,y=Price))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se=FALSE)+\n  labs(x=\"Age\", y=\"Sales Price\", title=\"Scatterplot of Sales Price against Age\")"},{"path":"diag.html","id":"model-diagnostics-with-residual-plots","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"Model Diagnostics with Residual Plots","text":"Sometimes, residual plot easier visualize scatterplot. fit SLR model using lm() usual. Applying plot() function object created lm() actually produces four diagnostic plots. display four diagnostic plots 2 2 array, specify par(mfrow = c(2, 2)) plotting window split 2 2 array:first plot (top left) residual plot, residuals y-axis fitted values x-axis. residual plot can used address assumptions 1 2. red line overlayed represent average value residuals differing values along x-axis. line along x-axis without apparent curvature indicate form model reasonable. see, see clear curved pattern. assumption 1 met. assumption 2, want see vertical spread residuals fairly constant move left right. see residual plot; vertical spread increases move left right, assumption 2 met.first plot (top left) residual plot, residuals y-axis fitted values x-axis. residual plot can used address assumptions 1 2. red line overlayed represent average value residuals differing values along x-axis. line along x-axis without apparent curvature indicate form model reasonable. see, see clear curved pattern. assumption 1 met. assumption 2, want see vertical spread residuals fairly constant move left right. see residual plot; vertical spread increases move left right, assumption 2 met.second plot (top right) normal probability plot (also called QQ plot), addresses assumption 4. residuals normal, residuals fall along 45 degree line. regression model fairly robust assumption though; normality assumption least crucial four.second plot (top right) normal probability plot (also called QQ plot), addresses assumption 4. residuals normal, residuals fall along 45 degree line. regression model fairly robust assumption though; normality assumption least crucial four.third plot (bottom left) plot square root absolute value standardized residuals fitted values (scale-location). plot used assess assumption 2, constant variance assumption. red line overlayed represent average value vertical axis differing values along x-axis. variance constant, red line horizontal vertical spread plot constant. plot used assess assumption 2, small sample size. Otherwise, plot tell similar story first plot (top left) assessing assumption 2.third plot (bottom left) plot square root absolute value standardized residuals fitted values (scale-location). plot used assess assumption 2, constant variance assumption. red line overlayed represent average value vertical axis differing values along x-axis. variance constant, red line horizontal vertical spread plot constant. plot used assess assumption 2, small sample size. Otherwise, plot tell similar story first plot (top left) assessing assumption 2.last plot (bottom right) plot identify influential outliers. Data points lie contour lines large Cook’s distance influential. None data points Cook’s distance greater 0.5. general rule thumb, observations Cook’s distance greater 1 flagged influential. talk influential observations future module.last plot (bottom right) plot identify influential outliers. Data points lie contour lines large Cook’s distance influential. None data points Cook’s distance greater 0.5. general rule thumb, observations Cook’s distance greater 1 flagged influential. talk influential observations future module.Now know assumptions 1 2 met. need transform response variable first, stabilize variance.Based residual plot, see variance residuals increases move left right. know need transform response variable using \\(y^* = y^\\lambda\\) \\(\\lambda < 1\\). log transform considered since can still interpret regression coefficients.","code":"\nresult<-lm(Price~Age, data=Data)\npar(mfrow = c(2, 2))\nplot(result)"},{"path":"diag.html","id":"box-cox-transformation-on-y","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"Box Cox Transformation on y","text":"Box Cox plot can used decide transform response variable. transformation takes form \\(y^* = y^{\\lambda}\\), value \\(\\lambda\\) chosen. \\(\\lambda = 0\\), perform \\(\\log\\) transformation.use boxcox() function MASS package:can “zoom ” plot better idea value \\(\\lambda\\) can use, specifying range lambda inside function:can choose value \\(\\lambda\\) within CI. log transformation preferred possible, since can still interpret coefficients. Since 0 lies CI, choose \\(\\lambda = 0\\), log transform response variable get \\(y^* = \\log(y)\\). regress \\(y^*\\) \\(x\\), check resulting residual plot:need reassess assumptions 1 2 transformation.assumption 2, see vertical spread residuals residual plot (top left) fairly constant, move left right. assumption 2 met. log transformation worked.assumption 2, see vertical spread residuals residual plot (top left) fairly constant, move left right. assumption 2 met. log transformation worked.also notice residuals now evenly scattered across horizontal axis residual plot (top left). assumption 1 now met.also notice residuals now evenly scattered across horizontal axis residual plot (top left). assumption 1 now met.need perform transformations.","code":"\nlibrary(MASS) ##to use boxcox function\nMASS::boxcox(result)\n##adjust lambda for better visualization. Choose lambda between -0.5 and 0.5\nMASS::boxcox(result, lambda = seq(-0.5, 0.5, 1/10)) \n##transform y and then regress ystar on x\nystar<-log(Data$Price)\nData<-data.frame(Data,ystar)\nresult.ystar<-lm(ystar~Age, data=Data)\npar(mfrow = c(2, 2))\nplot(result.ystar)"},{"path":"diag.html","id":"interpreting-coefficients-with-log-transformed-response","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"Interpreting Coefficients with Log Transformed Response","text":"regression equation \\(\\hat{y^*} = 10.1878 - 0.1647x\\), \\(y^* = \\log(y)\\). interpret slope:price used Mazdas multiplied \\(\\exp(-0.1647) = 0.8481481\\) year older car .price used Mazdas decreases \\((1 - 0.8481481) \\times 100\\) percent, 15.18519 percent, year older car .","code":"\nresult.ystar## \n## Call:\n## lm(formula = ystar ~ Age, data = Data)\n## \n## Coefficients:\n## (Intercept)          Age  \n##     10.1878      -0.1647"},{"path":"diag.html","id":"acf-plot-of-residuals","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"ACF Plot of Residuals","text":"yet assess assumption observed prices independent . Assuming prices different cars car measured repeatedly time, reason think prices dependent .also produce ACF plot confirm thought:None ACFs beyond lag 0 significant, don’t evidence observations dependent .","code":"\nacf(result.ystar$residuals, main=\"ACF Plot of Residuals with ystar\")"},{"path":"diag.html","id":"example-2","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.6.2 Example 2","text":"second example, go example faraway package. dataframe called gala. data species diversity Galapagos Islands. 30 islands, island, data 7 variables. focus variable Species, denotes number plant species found island, Area, area island squared kilometers. wish see number plant species island related area island.","code":"\nlibrary(faraway)\nData<-faraway::gala"},{"path":"diag.html","id":"model-diagnostics-with-scatterplots-1","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"Model Diagnostics with Scatterplots","text":"can use scatterplot response variable predictor assess assumptions 1 2.assess assumption 1, data points evenly scattered sides regression line, move left right. plot looks nonlinear.assess assumption 2, vertical spread data points constant move left right. can bit difficult assess scatterplot, although observations small areas closer line, suggests assumption met.","code":"\nlibrary(tidyverse)\n\n##scatterplot, and overlay regression line\nggplot2::ggplot(Data, aes(x=Area,y=Species))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se=FALSE)+\n  labs(x=\"Area of Island (sq km)\", y=\"# of Plant Species\", \n       title=\"Scatterplot of Number of Plant Species against Area of Galapagos Island\")"},{"path":"diag.html","id":"model-diagnostics-with-residual-plots-1","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"Model Diagnostics with Residual Plots","text":"Fairly often, assessing regression assumptions, residual plot easier visualize scatterplot.residual plot (top left), see curved pattern, nonlinear relationship. Assumption 1 met.residual plot (top left), see curved pattern, nonlinear relationship. Assumption 1 met.scale-location plot (bottom left), vertical variance plots appear higher islands larger fitted valies, assumption 2 met.scale-location plot (bottom left), vertical variance plots appear higher islands larger fitted valies, assumption 2 met.Now know assumptions 1 2 met. need transform response variable first, stabilize variance.","code":"\nresult<-lm(Species~Area, data=Data)\npar(mfrow = c(2, 2))\nplot(result)"},{"path":"diag.html","id":"box-cox-transformation-on-y-1","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"Box Cox Transformation on y","text":"scale-location plot, see variance residuals increasing, expect transform response variable \\(y^* = y^{\\lambda}\\) \\(\\lambda < 1\\). see specific value \\(\\lambda\\) use, can use Box Cox plot:log transformation preferred possible, since can still interpret coefficients. Since 0 lies CI, choose \\(\\lambda = 0\\), log transform response variable get \\(y^* = \\log(y)\\). regress \\(y^*\\) \\(x\\), check resulting residual plot:need reassess assumptions 1 2 transformation.assumption 2, see vertical spread residuals residual plot (top left) fairly constant, move left right. assumption 2 met. log transformation worked stabilizing variance.assumption 2, see vertical spread residuals residual plot (top left) fairly constant, move left right. assumption 2 met. log transformation worked stabilizing variance.However, residual plot still appears nonlinear. assumption 1 still met.However, residual plot still appears nonlinear. assumption 1 still met.","code":"\nlibrary(MASS)\nMASS::boxcox(result)\n##log transform response and add to dataframe\nData$y.star<-log(Data$Species)\n##perform new regression\nresult.ystar<-lm(y.star~Area, data=Data)\npar(mfrow = c(2, 2))\nplot(result.ystar)"},{"path":"diag.html","id":"transformation-on-x","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"Transformation on x","text":"see specific transformation predictor use, create scatterplot transformed response predictor.plot resembles logarithmic curve, use log transformation predictor, let \\(x^{*} = \\log(x)\\). usual, log transformation preferred since can still interpret regression coefficients. perform regression using log transformed response variable predictor, assess diagnostic plots.Based residual plot (top left), assumptions met. residuals evenly scattered across horizontal axis pattern. vertical spread residuals also constant. transformations worked.","code":"\nggplot2::ggplot(Data, aes(x=Area,y=y.star))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se=FALSE)+\n  labs(x=\"Area of Island (sq km)\", y=\"Log # of Plant Species\", \n       title=\"Scatterplot of Log Number of Plant Species against Area of Galapagos Island\")\n##log transform predictor and add to dataframe\nData$x.star<-log(Data$Area)\n\n##perform new regression\nresult.xystar<-lm(y.star~x.star, data=Data)\npar(mfrow = c(2, 2))\nplot(result.xystar)"},{"path":"diag.html","id":"interpreting-coefficients-with-log-transformed-response-and-predictor","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"Interpreting Coefficients with Log Transformed Response and Predictor","text":"regression equation \\(\\hat{y^*} = 2.9037 + 0.3886x^{*}\\), \\(y^* = \\log(y)\\) \\(x^* = \\log(x)\\). couple ways interpret slope:1% increase area Galapagos island, number plant species found island multiplied \\((1.01)^{0.3886} = 1.003874\\), ORFor 1% increase area Galapagos island, number plant species found island multiplied \\((1.01)^{0.3886} = 1.003874\\), ORFor 1% increase area Galapagos island, number plant species increases 0.3886%.1% increase area Galapagos island, number plant species increases 0.3886%.","code":"\nresult.xystar## \n## Call:\n## lm(formula = y.star ~ x.star, data = Data)\n## \n## Coefficients:\n## (Intercept)       x.star  \n##      2.9037       0.3886"},{"path":"mlr.html","id":"mlr","chapter":"6 Multiple Linear Regression (MLR)","heading":"6 Multiple Linear Regression (MLR)","text":"","code":""},{"path":"mlr.html","id":"introduction-5","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.1 Introduction","text":"Linear regression models used explore relationship variables well make predictions. Simple linear regression (SLR) concerns study one predictor variable one response variable. However, given context, may clear multiple predictors relate response variable. context, want :Improve predictions response variable including useful predictors.Assess predictor relates response variable controlling predictors.Multiple linear regression (MLR) models allow us examine effect multiple predictors response variable simultaneously.couple ways think MLR:Extension SLR MLR.SLR special case MLR.motivating example, look data regarding black cherry trees. data, cherry come openintro package. Researchers want understand relationship volume trees diameter height. Data come 31 trees Allegheny National Forest, Pennsylvania.context, know volume tree influenced diameter height, one predictor study.read set notes, take note similarities differences SLR MLR.","code":"\nlibrary(openintro)\nData<-openintro::cherry"},{"path":"mlr.html","id":"notation-in-mlr","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.2 Notation in MLR","text":"write MLR model :\\[\\begin{equation}\ny_i = \\beta_0+\\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_{k} x_{k} + \\epsilon_i.\n\\tag{6.1}\n\\end{equation}\\]setup, \\(k\\) quantitative predictors. notation (6.1) follows:\\(y_i\\): value response variable observation \\(\\),\\(\\beta_0\\): intercept MLR model,\\(\\beta_j\\): coefficient (slope) predictor \\(j\\), \\(j = 1, 2, \\cdots, k\\). \\(k\\) predictors, corresponding coefficient.\\(x_{ij}\\): observation \\(\\)’s value predictor \\(j\\). Notice two numbers subscript. first number denotes observation, second denotes predictor.\\(\\epsilon_i:\\) error observation \\(\\).assumptions MLR identical SLR:\\[\\begin{equation}\n\\epsilon_1,\\ldots,\\epsilon_n \\ ..d. \\sim N(0,\\sigma^2).\n\\tag{6.2}\n\\end{equation}\\]Let us use cherry data openintro example:\\(y_2\\) = 10.3 cubic feet, volume observation 2\\(x_{41} = 10.5\\) inches, observation 4’s diameter (predictor 1)\\(x_{22} = 65\\) feet, observation 2’s height (predictor 2)MLR model (6.1) often expressed using matrices, lot neater:\\[\n\\left[\n\\begin{array}{c}\n   y_1  \\\\\n   y_2  \\\\\n   \\vdots   \\\\\n   y_n\n\\end{array}\n\\right] =\n\\left[\n\\begin{array}{cccc}\n   1 & x_{11} & \\cdots & x_{1k}  \\\\\n   1 & x_{21} & \\cdots & x_{2k}  \\\\\n   \\vdots   \\\\\n   1 & x_{n1} & \\cdots & x_{nk}  \\\\\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{c}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_{k}\n\\end{array}\n\\right] +\n\\left[\n\\begin{array}{c}\n   \\epsilon_1  \\\\\n   \\epsilon_2  \\\\\n   \\vdots   \\\\\n   \\epsilon_n\n\\end{array}\n\\right],\n\\]\\[\\begin{equation}\n\\boldsymbol{y} = \\boldsymbol{X \\beta} + \\boldsymbol{\\epsilon}.\n\\tag{6.3}\n\\end{equation}\\]notation (6.3) follows:\\(\\boldsymbol{y}\\): vector responses (length \\(n\\)),\\(\\boldsymbol{\\beta}\\): vector parameters (length \\(p = k+1\\), \\(p\\) denotes number regression parameters),\\(\\boldsymbol{X}\\): design matrix (dimension \\(n \\times p\\)),\\(\\boldsymbol{\\epsilon}\\): vector residuals (length \\(n\\)).formulation (6.3) basis calling model “linear” regression. model linear parameters, predictors. common misconception model linear predictors.Following (6.1), MLR equation can written :\\[\\begin{equation}\nE(y|x) = \\beta_0+\\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_{k} x_k.\n\\tag{6.4}\n\\end{equation}\\]turn, estimated MLR equation can written :\\[\\begin{equation}\n\\hat{y} = \\hat{\\beta_0}+\\hat{\\beta_1} x_1 + \\hat{\\beta_2} x_2 + \\cdots + \\hat{\\beta_{k}} x_k.\n\\tag{6.5}\n\\end{equation}\\]","code":"\nhead(Data)## # A tibble: 6 × 3\n##    diam height volume\n##   <dbl>  <int>  <dbl>\n## 1   8.3     70   10.3\n## 2   8.6     65   10.3\n## 3   8.8     63   10.2\n## 4  10.5     72   16.4\n## 5  10.7     81   18.8\n## 6  10.8     83   19.7"},{"path":"mlr.html","id":"interpreting-coefficients-in-mlr","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.2.1 Interpreting coefficients in MLR","text":"interpretation estimated coefficients similar SLR, small caveat: \\(\\hat{\\beta}_j\\) denotes change predicted response per unit change \\(x_j\\), predictors held constant. common ways state bold part:controlling predictors.predictors taken account.adjusting effect predictors.familiar partial derivative interpreted multivariate calculus, realize interpretation estimated coefficients MLR sound like partial derivative interpreted.Let us look estimated regression equation cherry data:estimated MLR equation \\(\\hat{y} = -57.9877 + 4.7082x_1 + 0.3393x_2\\). estimated coefficient diameter inform us additional inch diameter, predicted volume cherry tree increases 4.7082 cubic feet, holding height constant.","code":"\nresult<-lm(volume~., data=Data)\nresult## \n## Call:\n## lm(formula = volume ~ ., data = Data)\n## \n## Coefficients:\n## (Intercept)         diam       height  \n##    -57.9877       4.7082       0.3393"},{"path":"mlr.html","id":"visualizing-mlr","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.2.2 Visualizing MLR","text":"visualize MLR equation? Suppose two predictor variables, \\(x_1\\) \\(x_2\\), MLR equation \\(E(y|x_1,x_2) = 2 + 5x_1 + 5x_2\\). contour plot can used visualize response variable two predictors:contour plot creates axis predictor. value response variable denoted contour lines actual value displayed line. toy example, \\(\\beta_1 = 5\\). means hold \\(x_2\\) constant (e.g. set \\(x_2 = 1\\) per red horizontal line), increasing \\(x_1\\) 1 unit increases mean \\(y\\) 5 units.regression equation \\(E(y|x_1,x_2) = 2 + 5x_1 + 5x_2\\) sometimes called regression plane, instead regression line, since 1 predictor. can visualize regression plane belowDue limitations human visualization, going beyond 3-dimensional plot (1 response 2 predictors) difficult.Please see associated video little explanation regarding contour plot 3-dimensional plot.","code":""},{"path":"mlr.html","id":"estimating-coefficients-in-mlr","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.3 Estimating coefficients in MLR","text":"(6.5), predicted response (fitted values), can written matrix form :\\[\\begin{equation}\n\\boldsymbol{\\hat{y}} = \\boldsymbol{X\\hat{\\beta}},\n\\tag{6.6}\n\\end{equation}\\]\\(\\boldsymbol{\\hat{\\beta}} = (\\hat{\\beta_0}, \\hat{\\beta_1}, \\cdots, \\hat{\\beta_k})^\\prime\\).use method least squares find estimated coefficients MLR. idea applied SLR. method involves minimizing sum squared residuals, \\(SS_{res}\\). SLR, minimize\\[\n\\sum\\limits_{=1}^{n} \\left[ y_i - (\\hat{\\beta_0}+\\hat{\\beta_1} x_i) \\right]^{2}\n\\]respect \\(\\hat{\\beta_0}, \\hat{\\beta_1}\\). MLR, \\(SS_{res}\\) can expressed matrix form:\\[\\begin{equation}\nQ = \\left(\\boldsymbol{y - X\\hat{\\beta}}\\right)^{\\prime} \\left(\\boldsymbol{y - X\\hat{\\beta}}\\right)\n\\tag{6.7}\n\\end{equation}\\]minimize \\(Q = SS_{res}\\) respect \\(\\hat{\\beta_0}, \\hat{\\beta_1}, \\cdots, \\hat{\\beta_k}\\), take partial derivatives \\(Q\\) set 0, .e. \\(\\frac{\\nabla Q}{\\nabla \\hat{\\beta}}=0\\). Solving equations, get\\[\\begin{equation}\n\\boldsymbol{\\hat{\\beta}} = \\left[\n\\begin{array}{c}\n   \\hat{\\beta}_0  \\\\\n   \\hat{\\beta}_1 \\\\\n   \\vdots \\\\\n   \\hat{\\beta}_k\n\\end{array}\n\\right]  =\n\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{y} .\n\\tag{6.8}\n\\end{equation}\\]Residuals found way SLR:\\[\ne_i = y_i - \\hat{y_i},\n\\]matrix form:\\[\\begin{equation}\n\\boldsymbol{e} = \\boldsymbol{y} - \\boldsymbol{\\hat{y}} = \\boldsymbol{y} - \\boldsymbol{X\\hat{\\beta}}.\n\\tag{6.9}\n\\end{equation}\\]","code":""},{"path":"mlr.html","id":"estimating-variance-of-errors","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.3.1 Estimating variance of errors","text":"Similar SLR, \\(MS_{res}\\) used estimate \\(\\sigma^2\\), variance error terms. \\(MS_{res}\\) found using\\[\\begin{equation}\nMS_{res}=\\frac{SS_{res}}{n-p},\n\\tag{6.10}\n\\end{equation}\\]\\(p\\) denotes number regression parameters. SLR, \\(p=2\\), since intercept one slope. Note: seen many people think \\(p\\) denotes number predictors. incorrect! move forward, explore complicated regression models always think terms number regression parameters.","code":""},{"path":"mlr.html","id":"distribution-of-least-squares-estimators-1","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.3.2 Distribution of least squares estimators","text":"Following Gauss Markov theorem, least squares estimators \\(\\boldsymbol{\\hat{\\beta}}\\) unbiased, .e.\\[\\begin{equation}\n\\boldsymbol{E\\left(\\hat{\\beta}\\right)} = \\boldsymbol{\\beta},\n\\tag{6.11}\n\\end{equation}\\]variance-covariance matrix given \\[\\begin{equation}\n\\boldsymbol{Var}\\left(\\boldsymbol{\\hat{\\beta}}\\right) = \\sigma^{2}\\left(\\boldsymbol{X^{\\prime}X} \\right)^{-1}\n\\tag{6.12}\n\\end{equation}\\]\\(\\sigma^{2}\\) estimated \\(MS_{res}\\). notes variance-covariance matrix least squares estimators:dimension \\(p \\times p\\),diagonal elements denote variance estimated parameter. example, first diagonal element denotes variance \\(\\hat{\\beta}_0\\), first estimated parameter.-diagonal elements denote covariance respective parameters. example, (1,2) entry denotes covariance \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\).Please see associated video demonstration read variance-covariance matrix.","code":""},{"path":"mlr.html","id":"anova-f-test-in-mlr","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.4 ANOVA \\(F\\) Test in MLR","text":"","code":""},{"path":"mlr.html","id":"sum-of-squares-1","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.4.1 Sum of squares","text":"simple regression, analysis variance (ANOVA) table MLR model displays quantities measure much variability response variable explained (explained) regression model. underlying conceptual idea construction analysis variance table :\\[\\begin{equation}\nSS_T = SS_R + SS_{res}.\n\\tag{6.13}\n\\end{equation}\\]change associated degrees freedom:df \\(SS_R\\): \\(df_R = p-1\\)df \\(SS_{res}\\): \\(df_{res} = n-p\\)df \\(SS_T\\): \\(df_T = n-1\\)Notice degrees freedom SLR \\(p=2\\).","code":""},{"path":"mlr.html","id":"anova-table-2","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.4.2 ANOVA table","text":"ANOVA table thus","code":""},{"path":"mlr.html","id":"anova-f-test-1","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.4.3 ANOVA \\(F\\) test","text":"null alternative hypotheses associated ANOVA \\(F\\) test :\\[\nH_0: \\beta_1=\\beta_2=...=\\beta_{k}=0, H_a: \\text{ least one coefficients 0.}\n\\]\nnull hypothesis states regression coefficients predictors 0. Notice statement simplifies SLR.different ways view hypothesis statements:MLR model useful?MLR model preferred intercept-model?Can drop predictors MLR model?test statistic still\\[\\begin{equation}\nF = \\frac{MS_R}{MS_{res}}\n\\tag{6.14}\n\\end{equation}\\]compared \\(F_{p-1,n-p}\\) distribution.","code":""},{"path":"mlr.html","id":"coefficient-of-determination-1","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.4.4 Coefficient of determination","text":"coefficient determination, \\(R^2\\), still\\[\\begin{equation}\nR^{2} = \\frac{SS_R}{SS_T} = 1 - \\frac{SS_{res}}{SS_T},\n\\tag{6.15}\n\\end{equation}\\]\\(R^{2}\\) interpreted proportion variance response variable explained predictors.","code":""},{"path":"mlr.html","id":"caution-with-r2","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.4.4.1 Caution with \\(R^2\\)","text":"Adding predictors model can increase \\(R^2\\), \\(SS_{res}\\) never becomes larger predictors \\(SS_T\\) remains given set responses.even adding predictors don’t make sense increase \\(R^2\\).\\(R^2\\) used compare models number parameters.\\(R^2\\) popular measure nice geometric interpretation.response caution, adjusted \\(R^2\\), denoted \\(R_{}^{2}\\):\\[\\begin{equation}\nR_{}^{2} = 1 - \\frac{\\frac{SS_{res}}{n-p}}{\\frac{SS_T}{n-1}} = 1 - \\left(\\frac{n-1}{n-p} \\right) \\frac{SS_{res}}{SS_T}.\n\\tag{6.16}\n\\end{equation}\\]\\(R_{}^{2}\\) increases added predictors significantly improve fit model, decreases otherwise.Let us go back cherry dataset example:ANOVA \\(F\\) statistic 255` small p-value. reject null hypothesis state MLR model diameter height predictors useful.\\(R^2\\) 0.948. 94.8% variance volume cherry trees can explained diameter height.\\(R_{}^{2}\\) 0.9442. value used comparison another model decide preferred.","code":"\nsummary(result)## \n## Call:\n## lm(formula = volume ~ ., data = Data)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -6.4065 -2.6493 -0.2876  2.2003  8.4847 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\n## diam          4.7082     0.2643  17.816  < 2e-16 ***\n## height        0.3393     0.1302   2.607   0.0145 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.882 on 28 degrees of freedom\n## Multiple R-squared:  0.948,  Adjusted R-squared:  0.9442 \n## F-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16"},{"path":"mlr.html","id":"t-test-for-regression-coefficient-in-mlr","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.5 \\(t\\) Test for Regression Coefficient in MLR","text":"can assess whether regression coefficient significantly different 0 MLR. null alternative hypotheses much SLR:\\[\nH_0: \\beta_j = 0, H_a: \\beta_j \\neq 0.\n\\]\nhypotheses mean words:null hypothesis supports dropping predictor \\(x_j\\) MLR model, presence predictors.alternative hypothesis supports keeping predictor \\(x_j\\) MLR model, drop presence predictors.Notice meaning null alternative hypotheses little different SLR, predictors taken account.test statistic still\\[\\begin{equation}\nt = \\frac{\\hat{\\beta}_j}{se(\\hat{\\beta}_j)}\n\\tag{6.17}\n\\end{equation}\\]compared \\(t_{n-p}\\) distribution.Let us take look cherry dataset:Notice \\(t\\) statistics associated testing coefficient predictor highly significant. evidence drop predictors simplify model.","code":"\nsummary(result)$coefficients##                Estimate Std. Error   t value     Pr(>|t|)\n## (Intercept) -57.9876589  8.6382259 -6.712913 2.749507e-07\n## diam          4.7081605  0.2642646 17.816084 8.223304e-17\n## height        0.3392512  0.1301512  2.606594 1.449097e-02"},{"path":"mlr.html","id":"caution-in-interpretating-t-test-in-mlr","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.5.1 Caution in interpretating \\(t\\) test in MLR","text":"insignificant \\(t\\) test coefficient \\(\\beta_j\\) MLR indicates predictor \\(x_j\\) can removed model (leave predictors ). needed presence predictors.common misstatement many make insignificant \\(t\\) test coefficient \\(\\beta_j\\) MLR implies predictor \\(x_j\\) linear relation response variable. necessarily correct!\n\\(x_j\\) highly correlated least one predictors, linear combination number predictors, \\(x_j\\) probably insignificant addition \\(x_j\\) doesn’t help improving model. concept called multicollinearity explore depth next module. \\(x_j\\) provide independent information predictors, needed predictors model.\n\\(x_j\\) may still linearly related response variable, .\ngoal assess \\(x_j\\) linearly related response, need use SLR.\ncommon misstatement many make insignificant \\(t\\) test coefficient \\(\\beta_j\\) MLR implies predictor \\(x_j\\) linear relation response variable. necessarily correct!\\(x_j\\) highly correlated least one predictors, linear combination number predictors, \\(x_j\\) probably insignificant addition \\(x_j\\) doesn’t help improving model. concept called multicollinearity explore depth next module. \\(x_j\\) provide independent information predictors, needed predictors model.\\(x_j\\) highly correlated least one predictors, linear combination number predictors, \\(x_j\\) probably insignificant addition \\(x_j\\) doesn’t help improving model. concept called multicollinearity explore depth next module. \\(x_j\\) provide independent information predictors, needed predictors model.\\(x_j\\) may still linearly related response variable, .\\(x_j\\) may still linearly related response variable, .goal assess \\(x_j\\) linearly related response, need use SLR.goal assess \\(x_j\\) linearly related response, need use SLR.Another common misstatement people make observe one \\(t\\) statistic insignificant, means associated predictors can dropped model. necessarily correct.\ninsignificant \\(t\\) test informs us can drop particular predictor, leaving predictors model. can drop one predictor time based \\(t\\) tests.\nAnother common misstatement people make observe one \\(t\\) statistic insignificant, means associated predictors can dropped model. necessarily correct.insignificant \\(t\\) test informs us can drop particular predictor, leaving predictors model. can drop one predictor time based \\(t\\) tests.Notice limitation \\(t\\) test ANOVA \\(F\\) test MLR:can drop 1 predictor based \\(t\\) test.can drop predictors based ANOVA \\(F\\) test.wish drop 1 predictor simultaneously, , model? explore via another \\(F\\) test next module.","code":""},{"path":"mlr.html","id":"cis-in-mlr","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.6 CIs in MLR","text":"","code":""},{"path":"mlr.html","id":"ci-for-regression-coefficient","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.6.1 CI for regression coefficient","text":"general form CIs still :\\[\\begin{equation}\n\\mbox{estimator} \\pm (\\mbox{multiplier} \\times \\mbox{s.e estimator}).\n\\tag{6.18}\n\\end{equation}\\]\\(100(1-\\alpha)\\%\\) CI \\(\\beta_j\\) \\[\\begin{equation}\n\\hat{\\beta}_j \\pm t_{1-\\alpha/2;n-p}  se(\\hat{\\beta}_j) = \\hat{\\beta}_j \\pm t_{1-\\alpha/2;n-p} s \\sqrt{C_{jj}}\n\\tag{6.19}\n\\end{equation}\\]\\(C_{jj}\\) denotes \\(j\\)th diagonal entry variance-covariance matrix estimated coefficients, \\(\\boldsymbol{Var}\\left(\\boldsymbol{\\hat{\\beta}}\\right)\\).multiplier now based \\(t_{n-p}\\) distribution, instead \\(t_{n-2}\\) distribution SLR.","code":""},{"path":"mlr.html","id":"ci-of-the-mean-response-1","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.6.2 CI of the mean response","text":"Since multiple predictors, may interested CI mean response, predictors equal specific values. Let vector \\(\\boldsymbol{x_0}\\) denote values predictor, specifically\\[\n\\boldsymbol{x_0} = (1, x_{01}, x_{02}, \\cdots, x_{0k})^{\\prime},\n\\]\\(x_{0j}\\) denotes value predictor \\(x_j\\). CI mean response \\(\\boldsymbol{x} = \\boldsymbol{x_0}\\) \\[\\begin{equation}\n\\hat{\\mu}_{y|\\boldsymbol{x_0}}\\pm t_{1-\\alpha/2,n-p}s\\sqrt{\\boldsymbol{x_0}^{\\prime} \\boldsymbol{(X^\\prime X)^{-1}} \\boldsymbol{x_0}}.\n\\tag{6.20}\n\\end{equation}\\]","code":""},{"path":"mlr.html","id":"pi-of-a-new-response-1","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.6.3 PI of a new response","text":"new value response \\(\\boldsymbol{x} = \\boldsymbol{x_0}\\), PI \\[\\begin{equation}\n\\hat{y}_0\\pm t_{1-\\alpha/2,n-p}s \\sqrt{1+ \\boldsymbol{x_0}^{\\prime} \\boldsymbol{(X^\\prime X)^{-1}} \\boldsymbol{x_0}}.\n\\tag{6.21}\n\\end{equation}\\]","code":""},{"path":"mlr.html","id":"r-tutorial-3","chapter":"6 Multiple Linear Regression (MLR)","heading":"6.7 R Tutorial","text":"tutorial, learn fit multiple linear regression (MLR) R. realize fitting MLR similar fitting SLR.look data regarding black cherry trees. data, cherry, come openintro package. Researchers want understand relationship volume (cubic feet) trees diameter (inches, 54 inches ground) height (feet). Data come 31 trees Allegheny National Forest, Pennsylvania.","code":"\nlibrary(openintro)\nData<-openintro::cherry"},{"path":"mlr.html","id":"scatterplot-matrix","chapter":"6 Multiple Linear Regression (MLR)","heading":"Scatterplot matrix","text":"scatterplot matrix useful create scatterplots involving two quantitative variables. use ggpairs() function GGally package:pieces information presented output. Notice output displayed matrix format.-diagonal entries output give us scatterplot correlation corresponding pair quantitative variables.\nexample, look scatterplot row 3, column 1 output. corresponding label column diam label row volume. informs us scatterplot volume vertical axis diam horizontal axis. see strong positive linear association two variables.\ncorrelation volume diam displayed row 1, column 3. , notice label column row. correlation 0.967, high.\npractice, locate scatterplot volume height corresponding correlation. Also locate scatterplot diam height corresponding correlation.\nexample, look scatterplot row 3, column 1 output. corresponding label column diam label row volume. informs us scatterplot volume vertical axis diam horizontal axis. see strong positive linear association two variables.correlation volume diam displayed row 1, column 3. , notice label column row. correlation 0.967, high.practice, locate scatterplot volume height corresponding correlation. Also locate scatterplot diam height corresponding correlation.diagonal entries display density plot corresponding variable. example, third diagonal entry displays density plot volume. can see distribution somewhat right skewed trees volume 10 40 cubic feet.","code":"\nlibrary(GGally)\n##scatterplot matrix\nGGally::ggpairs(Data)"},{"path":"mlr.html","id":"fit-mlr-using-lm","chapter":"6 Multiple Linear Regression (MLR)","heading":"Fit MLR using lm()","text":"fit multiple linear regression (MLR)list predictors ~ + operator predictors. Another way beThe . ~ informs lm() function use every column volume data frame predictors.Just like simple linear regression (SLR) can get relevant information using summary():estimated regression equation \\(\\hat{y} = -57.988 + 4.708 diam + 0.339height\\).\nestimated coefficient diam interpreted : predicted volume cherry tree increases 4.708 cubic feet per inch increase diameter, holding height constant.\nestimated coefficient height interpreted : predicted volume cherry tree increases 0.339 cubic feet per foot increase height, holding diameter constant.\nestimated coefficient diam interpreted : predicted volume cherry tree increases 4.708 cubic feet per inch increase diameter, holding height constant.estimated coefficient height interpreted : predicted volume cherry tree increases 0.339 cubic feet per foot increase height, holding diameter constant.\\(R^2\\) 0.948. 94.8% variance volume cherry trees can explained diameter height.residual standard error 3.882. estimates \\(\\sigma\\), standard deviation error term.","code":"\n##Fit MLR model, using + in between predictors\nresult<-lm(volume~diam+height, data=Data)\nresult<-lm(volume~., data=Data)\nsummary(result)## \n## Call:\n## lm(formula = volume ~ diam + height, data = Data)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -6.4065 -2.6493 -0.2876  2.2003  8.4847 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\n## diam          4.7082     0.2643  17.816  < 2e-16 ***\n## height        0.3393     0.1302   2.607   0.0145 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.882 on 28 degrees of freedom\n## Multiple R-squared:  0.948,  Adjusted R-squared:  0.9442 \n## F-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16"},{"path":"mlr.html","id":"inference-with-mlr","chapter":"6 Multiple Linear Regression (MLR)","heading":"Inference with MLR","text":"Just like SLR, coefficient tested null hypothesis \\(\\beta_j = 0\\) two-sided alternative. test significant coefficients, drop either predictor model.ANOVA \\(F\\) statistic 255, small p-value. data supports claim model useful.confidence intervals coefficients can found using confint():confidence interval mean response prediction interval new observation given specific value predictors can also found using predict(). example, diameter 10 inches height 80 feet:might realize now using functions SLR.Note: Obviously, calculations performed interpreted assuming regression assumptions met. Regression assumptions checked way SLR. , practice, assess regression assumptions.","code":"\nconfint(result,level = 0.95)##                    2.5 %      97.5 %\n## (Intercept) -75.68226247 -40.2930554\n## diam          4.16683899   5.2494820\n## height        0.07264863   0.6058538\nnewdata<-data.frame(diam=10, height=80)\n\npredict(result, newdata, level=0.95,\n        interval=\"confidence\")##        fit      lwr      upr\n## 1 16.23404 13.36762 19.10047\npredict(result, newdata, level=0.95,\n        interval=\"prediction\")##        fit      lwr      upr\n## 1 16.23404 7.781596 24.68649"},{"path":"genF.html","id":"genF","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7 General Linear \\(F\\) Test and Multicollinearity","text":"","code":""},{"path":"genF.html","id":"introduction-6","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.1 Introduction","text":"purpose multiple linear regression use one predictor predict response variable. modules explores approach choosing variables include multiple regression model. example, many variables can used predict someone’s systolic blood pressure, age, weight, height, pulse rate. predictors likely influence systolic blood pressure, want know need , subset predictors perform just well. use general linear \\(F\\) test .Another issue multiple predictors likelihood least one predictors linearly dependent, correlated, predictors increases. called multicollinearity. negative consequences multicollinearity present. learn consequences, diagnose presence multicollinearity, solutions multicollinearity present.","code":""},{"path":"genF.html","id":"the-general-linear-f-test","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.2 The General Linear \\(F\\) Test","text":"","code":""},{"path":"genF.html","id":"motivation-1","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.2.1 Motivation","text":"previous module, noted limitation \\(t\\) test ANOVA \\(F\\) test MLR:can drop 1 predictor based \\(t\\) test.can drop predictors based ANOVA \\(F\\) test.wish drop 1 predictor simultaneously, , model? explore via general linear \\(F\\) test. fact, \\(t\\) test ANOVA \\(F\\) test actually special cases general linear \\(F\\) test.Let us look motivating example, using dataset Peruvian.txt. data contains variables relating blood pressures Peruvians migrated rural high altitude areas urban lower altitude areas. variables :\\(y\\): Systolic blood pressure\\(x_1\\): Age\\(x_2\\): Years urban area\\(x_3\\): fraction life urban area \\((x_1/x_2)\\)\\(x_4\\): Weight kg\\(x_5\\): Height mm\\(x_6\\): Chin skinfold\\(x_7\\): Forearm skinfold\\(x_8\\): Calf skinfold\\(x_9\\): Resting pulse rateWe want assess systolic blood pressure migrants may predicted related predictors.Let us fit MLR predictors take look \\(t\\) tests ANOVA \\(F\\) test:Notice \\(t\\) tests insignificant lot coefficients (last five). Individually, \\(t\\) test informing us can drop specific predictor, leaving predictors model.erroneous interpretation say collectively, \\(t\\) tests inform us can drop predictors, \\(x_5\\) \\(x_9\\), model. misconception.One idea drop insignificant predictor, refit model, reassess predictors insignificant, continue dropping insignificant predictor refitting model \\(t\\) tests significant. end conducting multiple hypothesis tests . possible, limit number hypothesis tests conduct: tests , likelihood us wrongly rejecting null hypothesis increases.general linear \\(F\\) test (sometimes called partial \\(F\\) test) used. can perform one test assess can simultaneously drop multiple predictors model.Based output, consider dropping \\(x_5, x_6, x_7, x_8, x_9\\) since \\(t\\) tests insignificant.","code":"\nData<-read.table(\"Peruvian.txt\", header=TRUE,sep=\"\")\nhead(Data)##   Systol_BP Age Years fraclife Weight Height Chin Forearm Calf Pulse\n## 1       170  21     1 0.047619   71.0   1629  8.0     7.0 12.7    88\n## 2       120  22     6 0.272727   56.5   1569  3.3     5.0  8.0    64\n## 3       125  24     5 0.208333   56.0   1561  3.3     1.3  4.3    68\n## 4       148  24     1 0.041667   61.0   1619  3.7     3.0  4.3    52\n## 5       140  25     1 0.040000   65.0   1566  9.0    12.7 20.7    72\n## 6       106  27    19 0.703704   62.0   1639  3.0     3.3  5.7    72\nresult<-lm(Systol_BP~., data=Data)\nsummary(result)## \n## Call:\n## lm(formula = Systol_BP ~ ., data = Data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -12.3443  -6.3972   0.0507   5.7293  14.5257 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  146.81883   48.97099   2.998 0.005526 ** \n## Age           -1.12143    0.32741  -3.425 0.001855 ** \n## Years          2.45538    0.81458   3.014 0.005306 ** \n## fraclife    -115.29384   30.16903  -3.822 0.000648 ***\n## Weight         1.41393    0.43097   3.281 0.002697 ** \n## Height        -0.03464    0.03686  -0.940 0.355196    \n## Chin          -0.94369    0.74097  -1.274 0.212923    \n## Forearm       -1.17085    1.19330  -0.981 0.334613    \n## Calf          -0.15867    0.53716  -0.295 0.769809    \n## Pulse          0.11455    0.17043   0.672 0.506822    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 8.655 on 29 degrees of freedom\n## Multiple R-squared:  0.6674, Adjusted R-squared:  0.5641 \n## F-statistic: 6.465 on 9 and 29 DF,  p-value: 5.241e-05"},{"path":"genF.html","id":"setting-up-the-general-linear-f-test","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.2.2 Setting up the general linear \\(F\\) test","text":"general linear \\(F\\) test allows us assess multiple predictors can dropped simultaneously model. associated F statistic measures change \\(SS_R\\) (\\(SS_{res}\\)) removal predictors model. test based following concepts:long response variable, \\(SS_T\\) constant, regardless model. \\(SS_T = \\sum(y_i - \\bar{y})\\). involves response variable.\\(SS_T = SS_R + SS_{Res}\\).time predictors added model, \\(SS_R\\) increases \\(SS_{Res}\\) decreases amount, since \\(SS_T\\) stays constant.general linear \\(F\\) test answers question: change \\(SS_R\\) (change \\(SS_{res}\\)) significant removal addition predictor(s)?question can answered framework compares two models:full model, denoted \\(F\\), uses predictors consideration,reduced model, denoted \\(R\\), results predictors full model dropped.","code":""},{"path":"genF.html","id":"hypothesis-statements-1","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.2.3 Hypothesis statements","text":"Based framework, null alternative hypotheses Peruvian.txt dataset \\[\nH_0: \\beta_5 = \\beta_6 = \\beta_7 = \\beta_8 = \\beta_9 = 0, H_a: \\text{ least one coeff } H_0 \\text{ zero}.\n\\]general, null hypothesis states parameters terms wish drop 0. Therefore, null hypothesis supports reduced model, R.alternative hypothesis states drop terms wish drop. Therefore, alternative hypothesis supports full model, F.","code":""},{"path":"genF.html","id":"test-statistic","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.2.4 Test statistic","text":"associated test statistic general linear \\(F\\) test \\[\\begin{equation}\nF_0=\\frac{[SS_R(F)-SS_R(R)]/r}{SS_{res}(F)/(n-p)},\n\\tag{7.1}\n\\end{equation}\\]equivalently\\[\\begin{equation}\nF_0=\\frac{[SS_{res}(R)-SS_{res}(F)]/r}{SS_{res}(F)/(n-p)}.\n\\tag{7.2}\n\\end{equation}\\]test statistic \\(F_0\\) compared \\(F_{r,n-p}\\) distribution. notation follows:\\(SS_R(F)\\) denotes \\(SS_R\\) full model,\\(SS_R(R)\\) denotes \\(SS_R\\) reduced model,\\(r\\) denotes number parameters dropped/tested,\\(p\\) denotes number parameters full model,\\(SS_{res}(F)\\) denotes \\(SS_{res}\\) full model,\\(SS_{res}(R)\\) denotes \\(SS_{res}\\) reduced model.Note change \\(SS_R\\), \\(SS_R(F)-SS_R(R)\\) always equal change \\(SS_{res}\\), \\(SS_{res}(R)-SS_{res}(F)\\). Therefore, (7.1) always equal (7.2).","code":""},{"path":"genF.html","id":"worked-example","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.2.5 Worked example","text":"Let us look output Peruvian.txt dataset:output, model 1 predictors \\(x_1, x_2, x_3, x_4\\), model 2 predictors \\(x_1, \\cdots, x_9\\). model 1 reduced model, model 2 full model.output, model 1 predictors \\(x_1, x_2, x_3, x_4\\), model 2 predictors \\(x_1, \\cdots, x_9\\). model 1 reduced model, model 2 full model.see information presented table. first line corresponds model 1, second line corresponds model 2.see information presented table. first line corresponds model 1, second line corresponds model 2.column RSS, values \\(SS_{res}\\). Admittedly, can bit confusing, \\(SS_R\\). can see \\(SS_{res}(R) = 2629.7\\) \\(SS_{res}(F) = 2172.6\\). Note \\(SS_{res}\\) always smaller full model, \\(SS_R\\) always larger full model.column RSS, values \\(SS_{res}\\). Admittedly, can bit confusing, \\(SS_R\\). can see \\(SS_{res}(R) = 2629.7\\) \\(SS_{res}(F) = 2172.6\\). Note \\(SS_{res}\\) always smaller full model, \\(SS_R\\) always larger full model.column Res.DF, degrees freedom \\(SS_{res}\\) model. calculation \\(F\\) statistic, want value associated full model, 29.column Res.DF, degrees freedom \\(SS_{res}\\) model. calculation \\(F\\) statistic, want value associated full model, 29.column Df, number parameters testing drop, 5.column Df, number parameters testing drop, 5.column Sum Sq, difference \\(SS_{res}\\) models, \\(SS_{res}(R) - SS_{res}(F) = 2629.7 - 2172.6 = 457.12\\).column Sum Sq, difference \\(SS_{res}\\) models, \\(SS_{res}(R) - SS_{res}(F) = 2629.7 - 2172.6 = 457.12\\).column F, F statistic, \\(F_0 = 1.2204\\). can verify calculation using (7.2), \\(F_0 = \\frac{(2629.7 - 2172.6)/5}{2172.6/29}\\) (rounding).column F, F statistic, \\(F_0 = 1.2204\\). can verify calculation using (7.2), \\(F_0 = \\frac{(2629.7 - 2172.6)/5}{2172.6/29}\\) (rounding).p-value general linear \\(F\\) test reported last column. can found using:p-value general linear \\(F\\) test reported last column. can found using:critical value can found using:fail reject null hypothesis. data support alternative hypothesis (.e. full model). go reduced model.","code":"\nreduced<-lm(Systol_BP~Age+Years+fraclife+Weight, data=Data)\nanova(reduced,result)## Analysis of Variance Table\n## \n## Model 1: Systol_BP ~ Age + Years + fraclife + Weight\n## Model 2: Systol_BP ~ Age + Years + fraclife + Weight + Height + Chin + \n##     Forearm + Calf + Pulse\n##   Res.Df    RSS Df Sum of Sq      F Pr(>F)\n## 1     34 2629.7                           \n## 2     29 2172.6  5    457.12 1.2204 0.3247\n1-pf(1.2204, 5, 29)## [1] 0.3247085\nqf(1-0.05, 5, 29)## [1] 2.545386"},{"path":"genF.html","id":"comparison-of-general-linear-f-test-with-other-hypothesis-tests-in-mlr","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.2.6 Comparison of general linear \\(F\\) test with other hypothesis tests in MLR","text":"\\(t\\) test ANOVA \\(F\\) test MLR special cases general linear \\(F\\) test, \\(r=1\\) \\(r=p-1\\) respectively.\\(t\\) test, reduced model 1 less term full model. \\(F_0\\) statistic compared \\(F_{1,n-p}\\) distribution. turns \\(F_{1,n-p}\\) distribution directly related \\(t_{n-p}\\) distribution, general linear \\(F\\) test exactly \\(t\\) test dropping 1 term.\\(t\\) test, reduced model 1 less term full model. \\(F_0\\) statistic compared \\(F_{1,n-p}\\) distribution. turns \\(F_{1,n-p}\\) distribution directly related \\(t_{n-p}\\) distribution, general linear \\(F\\) test exactly \\(t\\) test dropping 1 term.ANOVA \\(F\\) test. reduced model drops terms intercept. call intercept-model.ANOVA \\(F\\) test. reduced model drops terms intercept. call intercept-model.","code":""},{"path":"genF.html","id":"alternative-approach-to-general-linear-f-test","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.2.7 Alternative approach to general linear \\(F\\) test","text":"another way information needed perform general linear \\(F\\) test. approach called sequential sums squares (sometimes called extra sums squares). works principle every time predictor added model, \\(SS_R\\) model increases, \\(SS_{res}\\) decreases amount, since \\(SS_T\\) constant. information displayed add one predictor time. Let us define notation:\\(SS_R(x_1)\\) denotes \\(SS_R\\) \\(x_1\\) predictor model.\\(SS_R(x_1, x_2)\\) denotes \\(SS_R\\) \\(x_1, x_2\\) model.\\(SS_R(x_2|x_1)\\) denotes increase \\(SS_R\\) \\(x_2\\) added model \\(x_1\\) already . read \\(SS_R\\) \\(x_2\\) given \\(x_1\\).Based example, \\(SS_R(x_2|x_1) = SS_R(x_1, x_2) - SS_R(x_1)\\), \\(SS_R(x_1, x_2) = SS_R(x_1) + SS_R(x_2|x_1)\\). Note \\(SS_R(x_2|x_1)\\) equal \\(SS_R(x_2)\\), latter denotes \\(SS_R\\) \\(x_2\\) predictor model.Let us see can use sequential sums squares Peruvian.txt dataset:values column “Sum Sq” give sequential \\(SS_{R}\\)s. ,first line, \\(SS_{R}(x_1) = 0.22\\),second line, \\(SS_{R}(x_2|x_1) = 82.55\\),\\(SS_{R}(x_3|x_1, x_2) = 3112.40\\),term,finally, \\(SS_{R}(x_9|x_1, x_2, \\cdots, x_8) = 33.84\\).last line refers \\(SS_{Res}(x_1, x_2, \\cdots, x_9) = 2172.59\\).Essentially, output term informs us increase \\(SS_R\\) term added model, given previously listed terms already model.Notice order sequential sums squares displayed order used entering predictors lm().still testing\\[\nH_0: \\beta_5 = \\beta_6 = \\beta_7 = \\beta_8 = \\beta_9 = 0, H_a: \\text{ least one coeff } H_0 \\text{ zero}.\n\\]Using (7.1), F statistic test \\(\\begin{aligned} F_0 &= \\frac{[SS_R(F)-SS_R(R)]/r}{SS_{res}(F)/(n-p)} \\\\  &= \\frac{(1.68+297.68+113.91+10.01+33.84)/5}{2172.59/29}\\\\  &= 1.220339. \\end{aligned}\\)Compare \\(F_0\\) statistic using approach example shown Section 2.5. exact result (rounding).Please view associated video explanation extra sums squares approach.","code":"\nanova(result)## Analysis of Variance Table\n## \n## Response: Systol_BP\n##           Df  Sum Sq Mean Sq F value    Pr(>F)    \n## Age        1    0.22    0.22  0.0030  0.956852    \n## Years      1   82.55   82.55  1.1019  0.302514    \n## fraclife   1 3112.40 3112.40 41.5448 4.728e-07 ***\n## Weight     1  706.55  706.55  9.4311  0.004603 ** \n## Height     1    1.68    1.68  0.0224  0.882113    \n## Chin       1  297.68  297.68  3.9735  0.055703 .  \n## Forearm    1  113.91  113.91  1.5205  0.227440    \n## Calf       1   10.01   10.01  0.1336  0.717419    \n## Pulse      1   33.84   33.84  0.4518  0.506822    \n## Residuals 29 2172.59   74.92                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"genF.html","id":"practice-questions-3","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.2.7.1 Practice questions","text":"use sequential sums squares Peruvian.txt dataset:Carry general linear \\(F\\) test assess can drop \\(x_7, x_8, x_9\\) model predictors.value \\(SS_R(x_1, x_2, x_3)\\)?value \\(SS_{res}(x_1, x_2, x_3)\\)?value \\(SS_{res}(x_1, x_2, \\cdots, x_8)\\)?Please view associated video review practice questions.","code":"\nanova(result)## Analysis of Variance Table\n## \n## Response: Systol_BP\n##           Df  Sum Sq Mean Sq F value    Pr(>F)    \n## Age        1    0.22    0.22  0.0030  0.956852    \n## Years      1   82.55   82.55  1.1019  0.302514    \n## fraclife   1 3112.40 3112.40 41.5448 4.728e-07 ***\n## Weight     1  706.55  706.55  9.4311  0.004603 ** \n## Height     1    1.68    1.68  0.0224  0.882113    \n## Chin       1  297.68  297.68  3.9735  0.055703 .  \n## Forearm    1  113.91  113.91  1.5205  0.227440    \n## Calf       1   10.01   10.01  0.1336  0.717419    \n## Pulse      1   33.84   33.84  0.4518  0.506822    \n## Residuals 29 2172.59   74.92                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"genF.html","id":"multicollinearity","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.3 Multicollinearity","text":"happens least one predictor almost linear combination predictors? called multicollinearity, negative consequences MLR model. learn negative consequences , detect multicollinearity, solutions. consider predictors model, multicollinearity likely exist.","code":""},{"path":"genF.html","id":"linear-dependency-multicollinearity","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.3.1 Linear dependency & multicollinearity","text":"define multicollinearity, define linear dependency. Recall can write MLR model matrix form \\[\\begin{equation}\n\\boldsymbol{y} = \\boldsymbol{X \\beta} + \\boldsymbol{\\epsilon}.\n\\tag{7.3}\n\\end{equation}\\]\\(\\boldsymbol{X}\\) design matrix \\[\n\\left[\n\\begin{array}{cccc}\n   1 & x_{11} & \\cdots & x_{1k}  \\\\\n   1 & x_{21} & \\cdots & x_{2k}  \\\\\n   \\vdots   \\\\\n   1 & x_{n1} & \\cdots & x_{nk}  \\\\\n\\end{array}\n\\right]\n\\]Note column design matrix (column 1) represents predictor variable.columns matrix linearly dependent least one column can expressed linear combination columns (exist nonzero constants \\(c_i\\) \\(c_1x_1 + c_2x_2 + ... + c_{k}x_{k} = 0\\)).example, suppose three predictors: \\(x_1\\) denoting SAT verbal score, \\(x_2\\) denoting SAT math score, \\(x_3\\) denoting SAT score. Since SAT score sum SAT verbal math scores, \\(x_3 = x_1 + x_2\\). create design matrix three predictors, linear dependency. linear dependency, can predict \\(x_3\\) \\(x_1, x_2\\) error. Recall least squares estimators found using\\[\\begin{equation}\n\\boldsymbol{\\hat\\beta} = \\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{y}.\n\\tag{7.4}\n\\end{equation}\\]linear dependence among columns \\(\\boldsymbol{X}\\), \\(\\bf{(X^{\\prime}X)}^{-1}\\) exist. means unique estimates \\(\\beta_j\\)’s determined.Multicollinearity exists model least one predictor almost linearly dependent, can predicted high degree accuracy, predictors.example multicollinearity predictors \\(x_1\\) denoting right arm length, \\(x_2\\) denoting right thigh length, \\(x_3\\) denoting right calf length. know someone’s right arm right thigh lengths, can probably predict right calf length high degree accuracy. example, likely multicollinearity.multiple predictors, always find degree collinearity. question whether degree high enough warrant concern.predictors linearly dependent , provide independent information association response variable. becomes difficult separate effects response variable.","code":""},{"path":"genF.html","id":"sources","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.3.2 Sources of multicollinearity","text":"reasons presence multicollinearity.","code":""},{"path":"genF.html","id":"study-design","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.3.2.1 Study design","text":"design study might lead multicollinearity, solution change design. Let us consider example:Suppose Virginia Department Motor Vehicles (DMV) wants study waiting time customers spend waiting line, based number people ahead line number counters open.number people ahead line number counters open highly positively correlated; people line, counters staffed DMV. nature study leads multicollinearity.number people ahead line number counters open highly positively correlated; people line, counters staffed DMV. nature study leads multicollinearity.break multicollinearity number people ahead line number counters open, collect data instances number people line high, yet number counters opened low, vice versa. allow us isolate effect predictor waiting times.break multicollinearity number people ahead line number counters open, collect data instances number people line high, yet number counters opened low, vice versa. allow us isolate effect predictor waiting times.","code":""},{"path":"genF.html","id":"nature-of-the-data","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.3.2.2 Nature of the data","text":"Sometimes, nature variables lead multicollinearity much remedy .Suppose wish investigate electric consumption households based income size home city.Income size home likely highly correlated, due high income earners wanting buy bigger homes, low income earners unable buy bigger homes.Income size home likely highly correlated, due high income earners wanting buy bigger homes, low income earners unable buy bigger homes.force high income earners live small homes, low income earners buy bigger homes break multicollinearity. setting, choose one predictors.force high income earners live small homes, low income earners buy bigger homes break multicollinearity. setting, choose one predictors.","code":""},{"path":"genF.html","id":"too-many-predictors","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.3.2.3 Too many predictors","text":"collect data variables, likely encounter multicollinearity. ask predictors provide , similar information, predictors.","code":""},{"path":"genF.html","id":"consequences-of-multicollinearity","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.3.3 Consequences of multicollinearity","text":"main consequence multicollinearity high variance estimated coefficients. means value estimated coefficient may different true value. consequences :Estimated coefficients can difficult interpret, estimated value may different true parameter. Also, 2 predictors correlated, holding one constant increasing one may make much sense.Estimated coefficients can difficult interpret, estimated value may different true parameter. Also, 2 predictors correlated, holding one constant increasing one may make much sense.Algebraic sign coefficients can different known theoretically. true coefficient positive, estimated coefficient different, negative. may think direction association opposite.Algebraic sign coefficients can different known theoretically. true coefficient positive, estimated coefficient different, negative. may think direction association opposite.Predictors know impact response variable found insignificant, standard error estimated coefficient large, hence \\(t\\) statistic small. may erroneously think predictor related response variable.Predictors know impact response variable found insignificant, standard error estimated coefficient large, hence \\(t\\) statistic small. may erroneously think predictor related response variable.Interestingly, predictions may still unbiased regression assumptions met.depending using regression model , multicollinearity may may huge problem. Recall two main uses regression models:Prediction: Predict future value response variable, using information predictor variables.Association: Quantify relationship variables. change predictor variable change value response variable?goal regression analysis interpret coefficients understand effects predictors response variable, multicollinearity big issue.goal regression analysis predict future values response, multicollinearity may less issue long extrapolate.","code":""},{"path":"genF.html","id":"detecting-multicollinearity","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.4 Detecting Multicollinearity","text":"following indicators presence multicollinearity:Insignificant results individual tests regression\ncoefficients important predictor variables. significant ANOVA F test provides evidence multicollinearity.Insignificant results individual tests regression\ncoefficients important predictor variables. significant ANOVA F test provides evidence multicollinearity.presence estimated coefficients large standard errors.presence estimated coefficients large standard errors.Estimated regression coefficients algebraic sign\nopposite expected theoretical\nconsiderations prior experience.Estimated regression coefficients algebraic sign\nopposite expected theoretical\nconsiderations prior experience.High correlation pairs \npredictor variables.High correlation pairs \npredictor variables.High variance inflation factors (VIFs).High variance inflation factors (VIFs).touched upon first three ways earlier, using correlation makes intuitive sense. Next, look VIFs bit detail.","code":""},{"path":"genF.html","id":"variance-inflation-factors-vifs","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.4.1 Variance inflation factors (VIFs)","text":"Variance inflation factors (VIFs) associated coefficients predictor variables MLR. VIFs measure much variance corresponding coefficient multiplied due presence collinearity versus lack collinearity present. Mathematically, VIFs defined :\\[\\begin{equation}\n\\left(VIF\\right)_j = \\frac{1}{1-R_j^2},\n\\,\\,\\,\\,\\,\\,j=1,2,\\ldots,k,\n\\tag{7.5}\n\\end{equation}\\]\\(R_j^2\\) coefficient determination \n\\(x_j\\) regressed \\(k-1\\) predictors model.Larger VIFs indicate stronger evidence multicollinearity. Generally, VIFs greater 5 indicate degree multicollinearity, VIFs greater 10 indicate high level multicollinearity.Let us look VIFs Peruvian.txt dataset:VIFs coefficients \\(x_2, x_3\\) 5, indicating degree multicollinearity data.","code":"\nlibrary(faraway)\nround(faraway::vif(result),3)##      Age    Years fraclife   Weight   Height     Chin  Forearm     Calf \n##    3.213   34.289   24.387    4.748    1.914    2.064    3.802    2.415 \n##    Pulse \n##    1.329"},{"path":"genF.html","id":"handling-multicollinearity","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.4.2 Handling multicollinearity","text":"Depends source multicollinearity, discussed Section 7.3.2.due study design, can collect data observations break collinearity.due study design, can collect data observations break collinearity.due nature data predictors linearly dependent others, drop predictor(s). Choose subset predictors (maybe even just one) remove rest model.due nature data predictors linearly dependent others, drop predictor(s). Choose subset predictors (maybe even just one) remove rest model.Abandon least squares regression use methods. methods shrinkage methods principal components regression help improve predictions, may aid helping explore relationship predictors response variable. depends want use regression .Abandon least squares regression use methods. methods shrinkage methods principal components regression help improve predictions, may aid helping explore relationship predictors response variable. depends want use regression .","code":""},{"path":"genF.html","id":"r-tutorial-4","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"7.5 R Tutorial","text":"tutorial, learn conduct general linear \\(F\\) test well detect presence multicollinearity MLR. continue use Peruvian.txt dataset. data contains variables relating blood pressures Peruvians migrated rural high altitude areas urban lower altitude areas. variables :\\(y\\): Systolic blood pressure\\(x_1\\): Age\\(x_2\\): Years urban area\\(x_3\\): fraction life urban area \\((x_1/x_2)\\)\\(x_4\\): Weight kg\\(x_5\\): Height mm\\(x_6\\): Chin skinfold\\(x_7\\): Forearm skinfold\\(x_8\\): Calf skinfold\\(x_9\\): Resting pulse rateWe want assess systolic blood pressure migrants may predicted related predictors.Download data file read data .number strategies start building multiple linear regression (MLR) model. One possible strategy build initial model based appear predictors related response variable, systolic blood pressure. Let us create correlation matrix variables:use round() function can limit number decimal places output uses, case three.","code":"\nData<-read.table(\"Peruvian.txt\", header=TRUE,sep=\"\")\nround(cor(Data),3)##           Systol_BP    Age  Years fraclife Weight Height   Chin Forearm   Calf\n## Systol_BP     1.000  0.006 -0.087   -0.276  0.521  0.219  0.170   0.272  0.251\n## Age           0.006  1.000  0.588    0.365  0.432  0.056  0.158   0.055 -0.005\n## Years        -0.087  0.588  1.000    0.938  0.481  0.073  0.222   0.143  0.001\n## fraclife     -0.276  0.365  0.938    1.000  0.293  0.051  0.120   0.028 -0.113\n## Weight        0.521  0.432  0.481    0.293  1.000  0.450  0.562   0.544  0.392\n## Height        0.219  0.056  0.073    0.051  0.450  1.000 -0.008  -0.069 -0.003\n## Chin          0.170  0.158  0.222    0.120  0.562 -0.008  1.000   0.638  0.516\n## Forearm       0.272  0.055  0.143    0.028  0.544 -0.069  0.638   1.000  0.736\n## Calf          0.251 -0.005  0.001   -0.113  0.392 -0.003  0.516   0.736  1.000\n## Pulse         0.135  0.091  0.237    0.214  0.312  0.008  0.223   0.422  0.209\n##           Pulse\n## Systol_BP 0.135\n## Age       0.091\n## Years     0.237\n## fraclife  0.214\n## Weight    0.312\n## Height    0.008\n## Chin      0.223\n## Forearm   0.422\n## Calf      0.209\n## Pulse     1.000"},{"path":"genF.html","id":"general-linear-f-test","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"General Linear \\(F\\) Test","text":"appears correlation matrix \\(x_3, x_4, x_7, x_8\\) moderately strong linear associations systolic blood pressure (higest correlations). start four predictors MLR:Based \\(t\\) tests, consider dropping \\(x_7, x_8\\) model. perform general linear \\(F\\) test full model using predictors \\(x_3, x_4, x_7, x_8\\) reduced model using \\(x_3, x_4\\). null alternative hypotheses :\\(H_0: \\beta_7 = \\beta_8 =0\\),\\(H_a:\\) least one coefficients \\(H_0\\) 0.words, null hypothesis supports going reduced model dropping \\(x_7, x_8\\), whereas alternative hypothesis supports full model dropping \\(x_7, x_8\\).explore two approaches conducting general linear \\(F\\) test.","code":"\n##fit MLR\nresult<-lm(Systol_BP~fraclife+Weight+Forearm+Calf, data=Data)\nsummary(result)## \n## Call:\n## lm(formula = Systol_BP ~ fraclife + Weight + Forearm + Calf, \n##     data = Data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -17.7706  -7.0950   0.4133   5.8314  24.0159 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  57.11972   15.56936   3.669 0.000827 ***\n## fraclife    -27.79798    7.63270  -3.642 0.000892 ***\n## Weight        1.33346    0.28858   4.621  5.3e-05 ***\n## Forearm      -0.52910    1.14255  -0.463 0.646254    \n## Calf         -0.06171    0.60134  -0.103 0.918870    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.985 on 34 degrees of freedom\n## Multiple R-squared:  0.481,  Adjusted R-squared:  0.4199 \n## F-statistic: 7.878 on 4 and 34 DF,  p-value: 0.000132"},{"path":"genF.html","id":"directly-comparing-the-full-and-reduced-models","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"Directly comparing the full and reduced models","text":"approach, fit reduced model, use anova() function compare reduced model full model:\\(F\\) statistic test 0.2588, p-value 0.7735. fail reject null hypothesis, little evidence supporting full model. go reduced model full model.","code":"\nreduced<-lm(Systol_BP~fraclife+Weight, data=Data)\n\n##general linear F test to compare reduced model with full model\nanova(reduced, result)## Analysis of Variance Table\n## \n## Model 1: Systol_BP ~ fraclife + Weight\n## Model 2: Systol_BP ~ fraclife + Weight + Forearm + Calf\n##   Res.Df    RSS Df Sum of Sq      F Pr(>F)\n## 1     36 3441.4                           \n## 2     34 3389.8  2    51.598 0.2588 0.7735"},{"path":"genF.html","id":"sequential-sums-of-squares","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"Sequential sums of squares","text":"approach, use anova() function full model obtain sequential sums squares associated full model:values column “Sum Sq” give sequential \\(SS_{R}\\)s. Notice information provided: order predictors entered lm().general linear F statistic \\[\\begin{align}\nF &= \\frac{[SS_R(F) - SS_R(R)]/r}{SS_{res}(F)/(n-p)} \\nonumber \\\\\n  &= \\frac{[SS_R(x_3, x_4, x_7, x_8) - SS_R(x_3, x_4)]/2}{SS_{res}(x_3, x_4, x_7, x_8)/(39-5)} \\nonumber \\\\\n  &= \\frac{SS_R(x_7, x_8 | x_3, x_4)/2}{SS_{res}(x_3, x_4, x_7, x_8)/(39-5)} \\nonumber \\\\\n  &= \\frac{(50.5+1.0)/2}{3389.8/34} \\nonumber \\\\\n  &= 0.2582748\n\\end{align}\\]similar value found approach 1 (discrepancy due rounding intermediate steps).corresponding p-value isand critical value isSo fail reject null hypothesis go reduced model.","code":"\nanova(result)## Analysis of Variance Table\n## \n## Response: Systol_BP\n##           Df Sum Sq Mean Sq F value    Pr(>F)    \n## fraclife   1  498.1  498.06  4.9957   0.03209 *  \n## Weight     1 2592.0 2592.01 25.9984 1.279e-05 ***\n## Forearm    1   50.5   50.55  0.5070   0.48129    \n## Calf       1    1.0    1.05  0.0105   0.91887    \n## Residuals 34 3389.8   99.70                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n1-pf(0.2582748,2,34)## [1] 0.7738846\nqf(0.95,2,34)## [1] 3.275898"},{"path":"genF.html","id":"multicollinearity-1","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"Multicollinearity","text":"presence multiple predictors, often tempting start including predictors model.ways detect presence multicollinearity model.","code":"\n##fit MLR with all predictors\nall<-lm(Systol_BP~., data=Data)"},{"path":"genF.html","id":"t-tests-and-anova-f-test","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"\\(t\\) tests and ANOVA \\(F\\) test","text":"presence lot insignificant \\(t\\) tests regression coefficients, along highly significant ANOVA \\(F\\) test indication multicollinearity present:Notice five \\(t\\) tests insignificant, ANOVA \\(F\\) highly significant. evidence multicollinearity.","code":"\n##look at t tests, and F test\nsummary(all)## \n## Call:\n## lm(formula = Systol_BP ~ ., data = Data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -12.3443  -6.3972   0.0507   5.7293  14.5257 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  146.81883   48.97099   2.998 0.005526 ** \n## Age           -1.12143    0.32741  -3.425 0.001855 ** \n## Years          2.45538    0.81458   3.014 0.005306 ** \n## fraclife    -115.29384   30.16903  -3.822 0.000648 ***\n## Weight         1.41393    0.43097   3.281 0.002697 ** \n## Height        -0.03464    0.03686  -0.940 0.355196    \n## Chin          -0.94369    0.74097  -1.274 0.212923    \n## Forearm       -1.17085    1.19330  -0.981 0.334613    \n## Calf          -0.15867    0.53716  -0.295 0.769809    \n## Pulse          0.11455    0.17043   0.672 0.506822    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 8.655 on 29 degrees of freedom\n## Multiple R-squared:  0.6674, Adjusted R-squared:  0.5641 \n## F-statistic: 6.465 on 9 and 29 DF,  p-value: 5.241e-05"},{"path":"genF.html","id":"standard-errors-of-estimated-coefficients","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"Standard errors of estimated coefficients","text":"Looking output summary(), see standard errors large. strong multicollinearity, standard errors large.","code":""},{"path":"genF.html","id":"correlation-between-pairs-of-predictors","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"Correlation between pairs of predictors","text":"can also look pairwise correlations among predictors:Looking matrix, notice pairs predictors involving \\(x_1, x_2, x_3\\), \\(x_4, x_6, x_7\\) high correlations. pairs involving predictors, correlations lot weaker. degree multicollinearity.","code":"\n##correlation matrix, round to 3 decimal\nround(cor(Data[,-1]),3)##             Age Years fraclife Weight Height   Chin Forearm   Calf Pulse\n## Age       1.000 0.588    0.365  0.432  0.056  0.158   0.055 -0.005 0.091\n## Years     0.588 1.000    0.938  0.481  0.073  0.222   0.143  0.001 0.237\n## fraclife  0.365 0.938    1.000  0.293  0.051  0.120   0.028 -0.113 0.214\n## Weight    0.432 0.481    0.293  1.000  0.450  0.562   0.544  0.392 0.312\n## Height    0.056 0.073    0.051  0.450  1.000 -0.008  -0.069 -0.003 0.008\n## Chin      0.158 0.222    0.120  0.562 -0.008  1.000   0.638  0.516 0.223\n## Forearm   0.055 0.143    0.028  0.544 -0.069  0.638   1.000  0.736 0.422\n## Calf     -0.005 0.001   -0.113  0.392 -0.003  0.516   0.736  1.000 0.209\n## Pulse     0.091 0.237    0.214  0.312  0.008  0.223   0.422  0.209 1.000"},{"path":"genF.html","id":"vifs","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"VIFs","text":"High VIFs indication multicollinearity.largest VIFs belong \\(x_2\\) \\(x_3\\), 34.289220 24.387489 respectively. VIFs 10 indicate strong degree multicollinearity.summarize seen:ANOVA F test significant, lot t tests insignificant.don’t see huge standard errors estimated coefficients.predictors high pairwise correlations, e.g. Pairs involving \\(x_1, x_2, x_3\\), \\(x_4, x_6, x_7\\).largest VIF 34.289220.Collectively, degree multicollinearity model.","code":"\n##VIFs\nlibrary(faraway)\nfaraway::vif(all)##       Age     Years  fraclife    Weight    Height      Chin   Forearm      Calf \n##  3.213372 34.289220 24.387489  4.747710  1.913992  2.063865  3.802313  2.414603 \n##     Pulse \n##  1.329233"},{"path":"genF.html","id":"next-steps","chapter":"7 General Linear \\(F\\) Test and Multicollinearity","heading":"Next steps","text":"identified predictors \\(x_1, x_2, x_3\\), \\(x_4, x_6, x_7\\) ones likely causing multicollinearity. solution use subset predictors .Using subject matter knowledge can help decision.","code":""},{"path":"cat.html","id":"cat","chapter":"8 Categorical Predictors in MLR","heading":"8 Categorical Predictors in MLR","text":"","code":""},{"path":"cat.html","id":"introduction-7","chapter":"8 Categorical Predictors in MLR","heading":"8.1 Introduction","text":"Thus far, considered predictors quantitative MLR. need want consider predictors categorical instead? example, wish investigate earnings college graduates based type major? predictor variable, type major, clearly categorical quantitative. Categorical variables can incorporated MLR. module, learn use interpret MLR model categorical predictors present.","code":""},{"path":"cat.html","id":"quantitative-vs-categorical-variable","chapter":"8 Categorical Predictors in MLR","heading":"8.1.1 Quantitative vs categorical variable","text":"First, quick review variable types. Variables can divided two types: quantitative categorical. general way assess type variable answer following question: arithmetic operations variable make sense? yes, variable quantitative.","code":""},{"path":"cat.html","id":"quantitative-variable","chapter":"8 Categorical Predictors in MLR","heading":"8.1.1.1 Quantitative variable","text":"Quantitative variables measured terms numbers, number represents amount. Quantitative variables can subdivided either continuous discrete.Continuous quantitative variable: takes numerical value within range. E.g.: height can measured terms centimeters inches. continuous value can take value shortest tallest person.Discrete quantitative variable: takes distinct numerical values. E.g. number failures among 10 experiments. discrete can take integers 0 10.question determine variable continuous discrete: Can potentially list plausible values variable? yes, variable discrete. example , can list plausible values number failures among 10 experiments : 0, 1, 2, way 10. height variable, list numerical values height infinite list height can take infinite number decimal places.","code":""},{"path":"cat.html","id":"categorical-variables","chapter":"8 Categorical Predictors in MLR","heading":"8.1.1.2 Categorical variables","text":"Categorical variables express qualitative attributes (often called qualitative variable). levels (classes) various attributes variable can take . example, political affiliation categorical three levels: Democrat, Republican, Independent.binary variable categorical variable two levels. example, variable whether voted 2020 presidential election binary, answer either yes .choice methods used analyze data usually driven whether variables quantitative categorical. might noticed creating data visualizations: visualization use driven type variable. Discrete variables interesting since can use methods meant quantitative categorical variables. go make decision context building MLR models later module.see incorporate categorical predictors MLR. first start binary predictors moving predictors two levels.","code":""},{"path":"cat.html","id":"indicator-variables-and-dummy-coding","chapter":"8 Categorical Predictors in MLR","heading":"8.2 Indicator Variables and Dummy Coding","text":"Let us start considering simple example: study innovation insurance industry, economist wishes relate speed insurance innovation \nadopted (months), \\(y\\), size firm, \\(x_1\\), type firm (mutual stock firms).one categorical variable, type firm, binary, two levels, mutual stock. also quantitative predictor, size firm, \\(x_1\\), quantitative response variable, time adopt innovation.","code":"\nData<-read.table(\"insurance.txt\", header=FALSE,sep=\"\")\ncolnames(Data)<-c(\"time\",\"size\", \"firm\")\nhead(Data)##   time size   firm\n## 1   17  151 mutual\n## 2   26   92 mutual\n## 3   21  175 mutual\n## 4   30   31 mutual\n## 5   22  104 mutual\n## 6    0  277 mutual"},{"path":"cat.html","id":"indicator-variables","chapter":"8 Categorical Predictors in MLR","heading":"8.2.1 Indicator variables","text":"Indicator variables take values 0 1 commonly used represent levels categorical predictors. example, may following two indicator variables represent two types firms:\\[\\begin{eqnarray*}\nx_2 &=& \\left\\{ \\begin{array}{ll} 1 & \\mbox{ mutual firm} \\\\ 0 &\n\\mbox{ otherwise;} \\end{array}\\right. \\\\\nx_3 &=& \\left\\{ \\begin{array}{ll} 1 & \\mbox{ stock firm} \\\\ 0\n& \\mbox{ otherwise;} \\end{array} \\right.\n\\end{eqnarray*}\\]MLR model written \n\\[\ny_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i3} +\n\\epsilon_i.\n\\]\nHowever, work MLR. Recall least squares estimators \\[\\begin{equation}\n\\boldsymbol{\\hat{\\beta}}   =\n\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{y} .\n\\tag{8.1}\n\\end{equation}\\]matrix \\((\\boldsymbol{X}^{\\prime} \\boldsymbol{X})\\) invertible use indicator variables. Let us consider toy example \\(n=4\\), first two observations mutual fims, last two observations stock furms. design matrix, \\(\\boldsymbol{X}\\), becomes\\[\n\\left[\n\\begin{array}{cccc}\n   1 & x_{11} & 1 & 0   \\\\\n   1 & x_{21} & 1 & 0    \\\\\n   1 & x_{31} & 0 & 1   \\\\\n   1 & x_{41} & 0 & 1    \\\\\n\\end{array}\n\\right]\n\\]\nNotice design matrix, column 1 equals sum column 3 column 4. means linear dependence among columns design matrix. happens, \\((\\boldsymbol{X}^{\\prime} \\boldsymbol{X})^{-1}\\) found unique solutions least squares estimators (8.1) exist.get around issue, use known dummy coding.","code":""},{"path":"cat.html","id":"dummy-coding","chapter":"8 Categorical Predictors in MLR","heading":"8.2.2 Dummy coding","text":"drop one indicator variables. general, categorical variable \\(\\) levels represented \\(-1\\) indicator variables, taking values 0 1. method coding categorical variables called dummy coding. example, since firm type binary (two levels), need one indicator variable.can following model\\[\\begin{equation}\ny = \\beta_0 + \\beta_1x_{1} + \\beta_2I_{1} +\n\\epsilon,\n\\tag{8.2}\n\\end{equation}\\]\\(x_1=\\) size firm \\[\\begin{eqnarray*}\nI_1 &=& \\left\\{ \\begin{array}{ll} 1 & \\mbox{ stock firm} \\\\ 0 &\n\\mbox{ otherwise.} \\end{array}\\right.\n\\end{eqnarray*}\\]formulation, use one indicator variable, \\(I_1\\). Indicator variables typically denoted \\(\\). formulation, mutual firm coded 0, stock firm coded 1. level coded 0 called reference class (sometimes called baseline class). variable binary, choice reference class important. interpretation model change based choice reference class.","code":""},{"path":"cat.html","id":"regression-coefficient-interpretation","chapter":"8 Categorical Predictors in MLR","heading":"8.2.3 Regression coefficient interpretation","text":"see can interpret regression coefficients dummy coding, can substitute numerical value indicator variable (8.2) obtain regression equation firm type:Mutual firms: \\(E(y|x) = \\beta_0 + \\beta_1x_1 +\\beta_2(0) = \\beta_0 + \\beta_1x_1\\)Mutual firms: \\(E(y|x) = \\beta_0 + \\beta_1x_1 +\\beta_2(0) = \\beta_0 + \\beta_1x_1\\)Stock firms: \\(E(y|x) = \\beta_0 + \\beta_1x_1 + \\beta_2(1) = (\\beta_0+\\beta_2) + \\beta_1x_1\\)Stock firms: \\(E(y|x) = \\beta_0 + \\beta_1x_1 + \\beta_2(1) = (\\beta_0+\\beta_2) + \\beta_1x_1\\)make following observations:two straight lines, slope \\(\\beta_1\\), intercepts different, \\(\\beta_0\\) \\(\\beta_0 + \\beta_2\\). formulation assumes slope scatterplot \\(y\\) \\(x_1\\) firm types.\\(\\beta_2\\) indicates difference mean response stock firms versus mutual firms (stock firms minus mutual firms), controlling size firm.general, coefficient indicator variable shows much higher (lower) mean response class coded 1 reference class, controlling \\(x_1\\).Let us look example:create scatterplot response variable quantitative predictor, separate colors lines firm type. Notice lines almost parallel. noted earlier formulation assumes slopes parallel.\\(\\beta_1\\) denotes slopes lines, \\(\\beta_2\\) denotes much higher line stock firms mutual firms. Let us take look estimated regression coefficients:Note following output:know since categorical predictor binary, 1 indicator variable representing firm type.know since categorical predictor binary, 1 indicator variable representing firm type.output, note one line categorical predictor, called firmstock. tells us name predictor, followed class coded 1. tells us R coded stock firms 1, mutual firms 0.output, note one line categorical predictor, called firmstock. tells us name predictor, followed class coded 1. tells us R coded stock firms 1, mutual firms 0.estimated coefficient firmstock 8.055469. means time adopt innovation stock firms 8.055469 months longer mutual firms, controlling size firm. associated p-value testing coefficient small, data support claim significant difference mean time adopt inoovation based firm type, controlling size firm. drop firm type term model. Since estimated coefficient positive, mean time adopt innovation stock firms longer mutual firms, controlling size firm.estimated coefficient firmstock 8.055469. means time adopt innovation stock firms 8.055469 months longer mutual firms, controlling size firm. associated p-value testing coefficient small, data support claim significant difference mean time adopt inoovation based firm type, controlling size firm. drop firm type term model. Since estimated coefficient positive, mean time adopt innovation stock firms longer mutual firms, controlling size firm.estimated coefficient size -0.101742. means time adopt innovation decreases 0.101742 months, per unit increase firm size, given firm type. associated p-value testing coefficient small, drop size term model.estimated coefficient size -0.101742. means time adopt innovation decreases 0.101742 months, per unit increase firm size, given firm type. associated p-value testing coefficient small, drop size term model.estimated regression equation \\(\\hat{y} = 33.874069 - 0.101742 x_1 + 8.055469I_1\\).estimated regression equation \\(\\hat{y} = 33.874069 - 0.101742 x_1 + 8.055469I_1\\).Estimated regression equation mutual firms: \\(\\hat{y} = 33.874069 - 0.101742 x_1 + 8.055469(0) = 33.874 - 0.102 x_1\\).Estimated regression equation mutual firms: \\(\\hat{y} = 33.874069 - 0.101742 x_1 + 8.055469(0) = 33.874 - 0.102 x_1\\).Estimated regression equation stock firms: \\(\\hat{y} = 33.874069 - 0.101742 x_1 + 8.055469(1) = 41.930 - 0.102 x_1\\).Estimated regression equation stock firms: \\(\\hat{y} = 33.874069 - 0.101742 x_1 + 8.055469(1) = 41.930 - 0.102 x_1\\).","code":"\n##convert categorical predictor to factor\nData$firm<-factor(Data$firm)\n\nlibrary(ggplot2)\n\n##scatterplot with separate regression lines\nggplot2::ggplot(Data, aes(x=size, y=time, color=firm))+\n  geom_point()+\n  geom_smooth(method=lm, se=FALSE)+\n  labs(title=\"Scatterplot of Time to Adopt Innovation against Assets, by Firm Type\")\nresult<-lm(time~size+firm, data=Data)\nsummary(result)## \n## Call:\n## lm(formula = time ~ size + firm, data = Data)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.6915 -1.7036 -0.4385  1.9210  6.3406 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 33.874069   1.813858  18.675 9.15e-13 ***\n## size        -0.101742   0.008891 -11.443 2.07e-09 ***\n## firmstock    8.055469   1.459106   5.521 3.74e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.221 on 17 degrees of freedom\n## Multiple R-squared:  0.8951, Adjusted R-squared:  0.8827 \n## F-statistic:  72.5 on 2 and 17 DF,  p-value: 4.765e-09"},{"path":"cat.html","id":"thought-question","chapter":"8 Categorical Predictors in MLR","heading":"8.2.4 Thought question","text":"output regression change dummy coding changed, .e. \\(I_1 = 1\\) mutual firm 0 stock firm?Please view associated video review question, well commentary dummy coding.","code":""},{"path":"cat.html","id":"interaction-terms","chapter":"8 Categorical Predictors in MLR","heading":"8.3 Interaction Terms","text":"regression model stated (8.2) sometimes called model additive effects. Additive effects assume predictor’s effect response depend value predictor. long hold predictor constant, changing value predictor associated change mean response. Using example, implies looking scatterplot time adopt innovation size firms, separate lines firm type, regression lines parallel.lines parallel, means effect changing firm size time adopt innovation depends firm type. effect predictor response depends value predictor, interaction effect predictors response variable. add interaction effect predictors model, \\[\\begin{equation}\ny = \\beta_0 + \\beta_1x_{1} + \\beta_2I_1 + \\beta_3 x_1 I_1 + \\epsilon\n\\tag{8.3}\n\\end{equation}\\]\\(I_1 = 1\\) stock firm 0 mutual firm. Substituting values \\(I_1\\) (8.3), following regression equations firm type:Mutual firm: \\(E(y|x) = \\beta_0+\\beta_2(0)+\\beta_1x_1+\\beta_3 x_1(0)=\\beta_0+\\beta_1x_1.\\)Mutual firm: \\(E(y|x) = \\beta_0+\\beta_2(0)+\\beta_1x_1+\\beta_3 x_1(0)=\\beta_0+\\beta_1x_1.\\)Stock firm: \\(E(y|x) = \\beta_0+\\beta_2(1)+\\beta_1x_1+\\beta_3 x_1(1)=(\\beta_0+\\beta_2)+(\\beta_1+\\beta_3)x_1.\\)Stock firm: \\(E(y|x) = \\beta_0+\\beta_2(1)+\\beta_1x_1+\\beta_3 x_1(1)=(\\beta_0+\\beta_2)+(\\beta_1+\\beta_3)x_1.\\)different intercepts different slopes regressions, whereas model (8.2), slopes .interaction, can see effect changing \\(x_1\\) response depends predictor: mutual firms, increasing \\(x_1\\) one unit changes mean response \\(\\beta_1\\); tool stock firms, increasing \\(x_1\\) one unit changes mean response \\(\\beta_1 + \\beta_3\\).Models interactions bit difficult interpret models interactions. Therefore, typical assess interaction term significant . can tested general linear \\(F\\) test framework, since model additive effects (8.2) reduced model model interaction effects (8.3) full model. coefficient interaction term insignificant, can drop interaction term go model just additive effects.look result \\(t\\) test line size:firmstock, way R denotes interaction term \\(x_1 I_1\\). test insignificant, evidence interaction effect size firm firm type. can drop interaction go simpler model just additive effects. expected given seen almost parallel slopes scatterplot.Note hierarchical principle applies interaction term: interaction term significant, lower ordered terms must remain.","code":"\n##model with interaction\nresult.int<-lm(time~size*firm, data=Data)\nsummary(result.int) ##can drop interaction## \n## Call:\n## lm(formula = time ~ size * firm, data = Data)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.7144 -1.7064 -0.4557  1.9311  6.3259 \n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    33.8383695  2.4406498  13.864 2.47e-10 ***\n## size           -0.1015306  0.0130525  -7.779 7.97e-07 ***\n## firmstock       8.1312501  3.6540517   2.225   0.0408 *  \n## size:firmstock -0.0004171  0.0183312  -0.023   0.9821    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.32 on 16 degrees of freedom\n## Multiple R-squared:  0.8951, Adjusted R-squared:  0.8754 \n## F-statistic: 45.49 on 3 and 16 DF,  p-value: 4.675e-08\n##can also do general linear F test. same as t test since only dropping one term\nanova(result,result.int)## Analysis of Variance Table\n## \n## Model 1: time ~ size + firm\n## Model 2: time ~ size * firm\n##   Res.Df    RSS Df Sum of Sq     F Pr(>F)\n## 1     17 176.39                          \n## 2     16 176.38  1 0.0057084 5e-04 0.9821"},{"path":"cat.html","id":"consideration-of-interaction-terms","chapter":"8 Categorical Predictors in MLR","heading":"8.3.1 Consideration of interaction terms","text":"Typically, people start additive order model. Interactions considered start :Exploring interactions part research question.interaction makes sense contextually, well-established literature.evidence interaction based visualizations.","code":""},{"path":"cat.html","id":"interaction-vs-correlation","chapter":"8 Categorical Predictors in MLR","heading":"8.3.2 Interaction vs correlation","text":"question often get students “Aren’t variables interact also correlated?” short answer , interaction correlation two different concepts.short explanation say interaction two variables, \\(x_1, x_2\\), means predictor impacts response variable \\(y\\) depends value predictor. Notice three variables, \\(y, x_1, x_2\\) needed talk interaction \\(x_1, x_2\\).Correlation involves two variables.detailed explanation, including examples, please read page.","code":""},{"path":"cat.html","id":"dummy-coding-vs-separate-regressions","chapter":"8 Categorical Predictors in MLR","heading":"8.3.3 Dummy coding vs separate regressions","text":"reasonable question often raised : carry two separate regressions, one firm type? using dummy coding, using one regression. main reason turns using one model dummy coding leads precise estimates (smaller standard errors), creating separate regression level. true long regression assumptions met, specifically variance errors constant levels.","code":""},{"path":"cat.html","id":"beyond-binary-predictors","chapter":"8 Categorical Predictors in MLR","heading":"8.4 Beyond Binary Predictors","text":"categorical predictor two levels, dummy coding still used manner: categorical predictor \\(\\) levels represented \\(-1\\) indicator variables, taking values 0 1.Let us use another example. example, consider ratings wines California. response variable average quality rating, \\(y\\), predictors average flavor rating, \\(x_1\\), Region indicating three regions California wine produced . regions North, Central, Napa Valley.notes using dummy coding example:Since region three levels, 2 indicator variables, one class reference class.choice reference class can arbitrary, one class interested , make reference class.interpretations regression model consistent regardless choice reference class.fitting model interactions, coefficient indicator variable denotes difference mean response level coded 1 versus reference class, controlling predictor.Let us create scatterplot quality rating flavor rating, split region:regression equations almost parallel consider model interactions first. can use following indicator variables. Note ways define .Since Napa Valley California’s famous wine region, like make easy comparisons regressions Napa Valley. makes sense make Napa Valley reference class.corresponding model just additive effects \\[\\begin{equation*}\ny_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2I_{i1} + \\beta_3I_{i2} +  \\epsilon_i\n\\end{equation*}\\]regression functions :North: \\(\\mbox{E}\\{Y\\} = \\beta_0 + \\beta_1x_1 + \\beta_2(1) + \\beta_3(0) = (\\beta_0+\\beta_2) + \\beta_1x_1\\)North: \\(\\mbox{E}\\{Y\\} = \\beta_0 + \\beta_1x_1 + \\beta_2(1) + \\beta_3(0) = (\\beta_0+\\beta_2) + \\beta_1x_1\\)Central: \\(\\mbox{E}\\{Y\\} = \\beta_0 + \\beta_1x_1 + \\beta_2(0) + \\beta_3(1) = (\\beta_0+\\beta_3) + \\beta_1x_1\\)Central: \\(\\mbox{E}\\{Y\\} = \\beta_0 + \\beta_1x_1 + \\beta_2(0) + \\beta_3(1) = (\\beta_0+\\beta_3) + \\beta_1x_1\\)Napa Valley: \\(\\mbox{E}\\{Y\\} = \\beta_0 + \\beta_1x_1 + \\beta_2(0) + \\beta_3(0) = \\beta_0 + \\beta_1x_1\\)Napa Valley: \\(\\mbox{E}\\{Y\\} = \\beta_0 + \\beta_1x_1 + \\beta_2(0) + \\beta_3(0) = \\beta_0 + \\beta_1x_1\\)Recall model assumes slopes three regions. \\(\\beta_{2}\\), \\(\\beta_{3}\\) indicate different mean ratings North Central regions compared Napa Valley, respectively, controlling predictor, average flavor rating.Let us fit model look output:observations output:\\(\\hat{\\beta_2}\\) estimated coefficient indicator North region. value -1.2234. interpret average quality rating wines North region 1.2234 lower wines Napa Valley, controlling flavor rating.\\(\\hat{\\beta_2}\\) estimated coefficient indicator North region. value -1.2234. interpret average quality rating wines North region 1.2234 lower wines Napa Valley, controlling flavor rating.\\(\\hat{\\beta_3}\\) estimated coefficient indicator Central region.average quality rating wines South region 2.7569 lower wines Napa Valley, controlling flavor rating.\\(\\hat{\\beta_3}\\) estimated coefficient indicator Central region.average quality rating wines South region 2.7569 lower wines Napa Valley, controlling flavor rating.mentioned earlier, can easily make comparisons level reference class.mentioned earlier, can easily make comparisons level reference class.","code":"\n##clear environment to start new example\nrm(list=ls())\n\nData<-read.table(\"wine.txt\", header=TRUE, sep=\"\")\n##convert Region to factor\nData$Region<-factor(Data$Region) \n##assign descriptive labels for each region\nlevels(Data$Region) <- c(\"North\", \"Central\", \"Napa\") \n\nlibrary(ggplot2)\n##scatterplot of Quality against Flavor, \n##separated by Region\nggplot2::ggplot(Data, aes(x=Flavor, y=Quality, color=Region))+\n  geom_point()+\n  geom_smooth(method=lm, se=FALSE)+\n  labs(title=\"Scatterplot of Wine Quality against Flavor, by Region\")\n##fit regression with no interaction\nreduced<-lm(Quality~Flavor+Region, data=Data)\nsummary(reduced)## \n## Call:\n## lm(formula = Quality ~ Flavor + Region, data = Data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.97630 -0.58844  0.02184  0.51572  1.94232 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)     8.3177     1.0100   8.235 1.31e-09 ***\n## Flavor          1.1155     0.1738   6.417 2.49e-07 ***\n## RegionNorth    -1.2234     0.4003  -3.056  0.00435 ** \n## RegionCentral  -2.7569     0.4495  -6.134 5.78e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8946 on 34 degrees of freedom\n## Multiple R-squared:  0.8242, Adjusted R-squared:  0.8087 \n## F-statistic: 53.13 on 3 and 34 DF,  p-value: 6.358e-13"},{"path":"cat.html","id":"difference-in-mean-response-between-levels-excluding-the-reference-class","chapter":"8 Categorical Predictors in MLR","heading":"8.4.1 Difference in mean response between levels excluding the reference class","text":"Estimating difference mean response levels excluding reference class can done estimating difference regression coefficients respective indicator variables. example, \\(\\beta_2 - \\beta_3\\) measures difference mean response North Central regions, given average flavor rating. output model give us difference immediately. can either construct confidence interval (CI) \\(\\beta_2 - \\beta_3\\), perform hypothesis test \\(\\beta_2 - \\beta_3\\).","code":""},{"path":"cat.html","id":"ci","chapter":"8 Categorical Predictors in MLR","heading":"8.4.1.1 CI","text":"\\(100(1-\\alpha)\\%\\) confidence interval estimating \ndifferential effects regions \n\\[\\begin{equation}\n(\\hat{\\beta}_2-\\hat{\\beta}_3) \\pm\nt_{1-\\alpha/2,n-p}se\\left(\\hat{\\beta}_2-\\hat{\\beta}_3\\right),\n\\tag{8.4}\n\\end{equation}\\]\\[\nVar\\left(\\hat{\\beta}_2-\\hat{\\beta}_3\\right) = Var\\left(\\hat{\\beta}_2\\right) + Var\\left(\\hat{\\beta}_3\\right) - 2 Cov(\\hat{\\beta}_2, \\hat{\\beta}_3).\n\\]Note general, variance difference estimated coefficients \\[\\begin{equation}\nVar\\left(\\hat{\\beta}_j-\\hat{\\beta}_l\\right) = Var\\left(\\hat{\\beta}_j\\right) + Var\\left(\\hat{\\beta}_l\\right) - 2 Cov(\\hat{\\beta}_j, \\hat{\\beta}_l).\n\\tag{8.5}\n\\end{equation}\\]need obtain variance-covariance matrix estimated coefficients:Let us compute CI \\(\\beta_2 - \\beta_3\\) using (8.4)\\[\\begin{eqnarray*}\n(\\hat{\\beta}_2-\\hat{\\beta}_3) &\\pm&\nt_{1-\\alpha/2,n-p}\\sqrt{Var\\left(\\hat{\\beta}_2-\\hat{\\beta}_3\\right)} \\nonumber \\\\\n(-1.2234 + 2.7569) &\\pm& 2.032245 \\sqrt{0.160 + 0.202 - 2 \\times 0.113} \\nonumber \\\\\n(0.7840453&,& 2.2829547) \\nonumber\n\\end{eqnarray*}\\]Note \\(t_{1-\\alpha/2,n-p}\\) found using qt(0.975, 38-4).CI excludes 0, significant difference mean quality ratings wines North Central region, controlling flavor rating. CI consists entirely positive numbers, mean quality rating higher wines North region Central region, controlling flavor rating.View associated video depth explanation constructing CI.","code":"\n##variance covariance matrix of estimated coefficients\nround(vcov(reduced),3) ##display 3 dp##               (Intercept) Flavor RegionNorth RegionCentral\n## (Intercept)         1.020 -0.170      -0.277        -0.277\n## Flavor             -0.170  0.030       0.037         0.037\n## RegionNorth        -0.277  0.037       0.160         0.113\n## RegionCentral      -0.277  0.037       0.113         0.202"},{"path":"cat.html","id":"hypothesis-test","chapter":"8 Categorical Predictors in MLR","heading":"8.4.1.2 Hypothesis test","text":"can also compare North Central regions using hypothesis testing. hypothesis statements :\\[\nH_0: \\beta_2 - \\beta_3 = 0, H_a: \\beta_2 - \\beta_3 \\neq 0.\n\\]\\(t\\) statistic \\[\\begin{eqnarray*}\nt &=& \\frac{\\hat{\\beta}_2 - \\hat{\\beta}_3}{se\\left(\\hat{\\beta}_2-\\hat{\\beta}_3\\right)}\\nonumber \\\\\n  &=& \\frac{-1.2234 + 2.7569}{\\sqrt{0.160 + 0.202 - 2 \\times 0.113}} \\nonumber \\\\\n  &=& 4.158286,\n\\end{eqnarray*}\\]larger critical value qt(1-0.05/2, 38 - 4) = 2.032245. reject null hypothesis. Data support claim significant difference mean quality ratings wines North region Central region, controlling flavor rating.","code":""},{"path":"cat.html","id":"interactions","chapter":"8 Categorical Predictors in MLR","heading":"8.4.2 Interactions","text":"Suppose decide assess interaction flavor rating region. words, effect flavor rating quality rating differs region. scatterplot, slopes exactly parallel, significant interaction may exist. model interaction \\[\\begin{equation*}\ny = \\beta_0 + \\beta_1x_{1} + \\beta_2I_{1} + \\beta_3I_{2} + \\beta_4 x_1 I_{1} + \\beta_5 x_1 I_{2} + \\epsilon_i\n\\end{equation*}\\]regression functions :North: \\(\\mbox{E}\\{Y\\} = \\beta_0 + \\beta_1x_1 + \\beta_2(1) + \\beta_3(0) + \\beta_4x_1(1) + \\beta_5x_1(0) = (\\beta_0+\\beta_2) + (\\beta_1 + \\beta_4)x_1\\)North: \\(\\mbox{E}\\{Y\\} = \\beta_0 + \\beta_1x_1 + \\beta_2(1) + \\beta_3(0) + \\beta_4x_1(1) + \\beta_5x_1(0) = (\\beta_0+\\beta_2) + (\\beta_1 + \\beta_4)x_1\\)Central: \\(\\mbox{E}\\{Y\\} = \\beta_0 + \\beta_1x_1 + \\beta_2(0) + \\beta_3(1) + \\beta_4x_1(0) + \\beta_5x_1(1) = (\\beta_0+\\beta_3) + (\\beta_1 + \\beta_5)x_1\\)Central: \\(\\mbox{E}\\{Y\\} = \\beta_0 + \\beta_1x_1 + \\beta_2(0) + \\beta_3(1) + \\beta_4x_1(0) + \\beta_5x_1(1) = (\\beta_0+\\beta_3) + (\\beta_1 + \\beta_5)x_1\\)Napa Valley: \\(\\mbox{E}\\{Y\\} = \\beta_0 + \\beta_1x_1 + \\beta_2(0) + \\beta_3(0) + \\beta_4x_1(0) + \\beta_5x_1(0) =\\beta_0 + \\beta_1x_1\\)Napa Valley: \\(\\mbox{E}\\{Y\\} = \\beta_0 + \\beta_1x_1 + \\beta_2(0) + \\beta_3(0) + \\beta_4x_1(0) + \\beta_5x_1(0) =\\beta_0 + \\beta_1x_1\\)Let us perform general linear \\(F\\) test see use model interactions model interactions:general linear \\(F\\) test insignificant, use reduced model, .e. model interactions.","code":"\n##consider model with interactions \n##(when slopes are not parallel)\nresult<-lm(Quality~Flavor*Region, data=Data)\n\n##general linear F test for interaction terms\nanova(reduced,result)## Analysis of Variance Table\n## \n## Model 1: Quality ~ Flavor + Region\n## Model 2: Quality ~ Flavor * Region\n##   Res.Df    RSS Df Sum of Sq      F Pr(>F)\n## 1     34 27.213                           \n## 2     32 25.429  2    1.7845 1.1229 0.3378"},{"path":"cat.html","id":"pairwise-comparisons","chapter":"8 Categorical Predictors in MLR","heading":"8.5 Pairwise Comparisons","text":"categorical predictor, may interested making comparisons mean response multiple pairs levels within categorical predictor. Going back wine example, categorical predictor three levels. means can make \\(\\binom{3}{2} = 3\\) pairwise comparisons:North region vs Napa ValleyCentral region vs Napa ValleyNorth region vs Central regionBased indicator variables, regressions equationsNorth: \\(\\mbox{E}\\{Y\\} = (\\beta_0+\\beta_2) + \\beta_1x_1\\)North: \\(\\mbox{E}\\{Y\\} = (\\beta_0+\\beta_2) + \\beta_1x_1\\)Central: \\(\\mbox{E}\\{Y\\} = (\\beta_0+\\beta_3) + \\beta_1x_1\\)Central: \\(\\mbox{E}\\{Y\\} = (\\beta_0+\\beta_3) + \\beta_1x_1\\)Napa Valley: \\(\\mbox{E}\\{Y\\} = \\beta_0 + \\beta_1x_1\\)Napa Valley: \\(\\mbox{E}\\{Y\\} = \\beta_0 + \\beta_1x_1\\)parameters denoting pairwise comparisons \\(\\beta_2\\): North region vs Napa Valley\\(\\beta_3\\): Central region vs Napa Valley\\(\\beta_2 - \\beta_3\\): North region vs Central regionTo assess whether significant difference mean response pair, can either:Conduct 3 hypothesis tests:\n\\(H_0: \\beta_2 = 0, H_a: \\beta_2 \\neq 0\\)\n\\(H_0: \\beta_3 = 0, H_a: \\beta_2 \\neq 0\\)\n\\(H_0: \\beta_3 - \\beta_2 = 0, H_a: \\beta_3 - \\beta_2 \\neq 0\\)\nConduct 3 hypothesis tests:\\(H_0: \\beta_2 = 0, H_a: \\beta_2 \\neq 0\\)\\(H_0: \\beta_3 = 0, H_a: \\beta_2 \\neq 0\\)\\(H_0: \\beta_3 - \\beta_2 = 0, H_a: \\beta_3 - \\beta_2 \\neq 0\\)orConstruct 3 confidence intervals:\n\\(\\hat{\\beta}_2 \\pm t_{1- \\alpha/2, n-p} se\\left( \\hat{\\beta}_2 \\right)\\)\n\\(\\hat{\\beta}_3 \\pm t_{1- \\alpha/2, n-p} se\\left( \\hat{\\beta}_3 \\right)\\)\n\\((\\hat{\\beta}_2 - \\hat{\\beta}_3) \\pm t_{1- \\alpha/2, n-p} se\\left(\\hat{\\beta}_2 - \\hat{\\beta}_3 \\right)\\)\nConstruct 3 confidence intervals:\\(\\hat{\\beta}_2 \\pm t_{1- \\alpha/2, n-p} se\\left( \\hat{\\beta}_2 \\right)\\)\\(\\hat{\\beta}_3 \\pm t_{1- \\alpha/2, n-p} se\\left( \\hat{\\beta}_3 \\right)\\)\\((\\hat{\\beta}_2 - \\hat{\\beta}_3) \\pm t_{1- \\alpha/2, n-p} se\\left(\\hat{\\beta}_2 - \\hat{\\beta}_3 \\right)\\)careful conducting multiple tests constructing multiple CIs.","code":""},{"path":"cat.html","id":"significance-level","chapter":"8 Categorical Predictors in MLR","heading":"8.5.1 Significance level","text":"significance level, \\(\\alpha\\), hypothesis test probability wrongly rejecting \\(H_0\\) true. Type error defined wrongly rejecting null hypothesis true. alternate definition significance level probability making Type error, null hypothesis true.Suppose conduct three hypothesis tests compare three pairs regions, significance level \\(\\alpha\\). null hypothesis true 3 tests (.e. significant different mean response regions, controlling flavor), probability making right conclusions tests, assuming tests independent, \\((1-\\alpha)^3\\). \\(\\alpha=0.05\\), probability \\(0.95^3 = 0.857375\\), 0.95. couple things consider:perform hypothesis tests, probability making right conclusions (assuming null true ), decreases.Can something probability making least one Type error still least \\(1-\\alpha\\)?confidence intervals, want ensure confidence entire set intervals capture true values parameters still least \\(1-\\alpha\\).","code":""},{"path":"cat.html","id":"multiple-pairwise-comparisons","chapter":"8 Categorical Predictors in MLR","heading":"8.5.2 Multiple pairwise comparisons","text":"account fact making multiple pairwise comparisons, need make confidence intervals wider, critical value larger ensure chance making Type error \\(\\alpha\\). procedures . look two common procedures:Bonferroni procedureTukey procedureWith various procedures multiple comparison:confidence intervals take form\\[\\begin{equation}\n\\text{estimate } \\pm \\text{ multiplier } \\times \\text{se(estimate) }.\n\\tag{8.6}\n\\end{equation}\\]hypothesis tests, reject null hypothesis \\[\\begin{equation}\n\\text{test statistic } > \\text{ critical value}.\n\\tag{8.7}\n\\end{equation}\\]multiplier critical values change.","code":""},{"path":"cat.html","id":"bonferroni-procedure","chapter":"8 Categorical Predictors in MLR","heading":"8.5.3 Bonferroni procedure","text":"Let \\(g\\) denote number CIs wish construct, number hypothesis tests need perform pairwise comparisons.","code":""},{"path":"cat.html","id":"cis","chapter":"8 Categorical Predictors in MLR","heading":"8.5.3.1 CIs","text":"Bonferroni procedure ensure least \\((1-\\alpha)100\\%\\) confidence \\(g\\) CIs capture true value \\[\\begin{equation}\n\\hat{\\beta}_j \\pm t_{1-\\alpha/(2g); n-p} se(\\hat{\\beta}_j).\n\\tag{8.8}\n\\end{equation}\\]multiplier (8.8) found using \\(t_{1-\\alpha/(2g); n-p}\\) instead \\(t_{1-\\alpha/2; n-p}\\).Going back wine example, suppose wish make three pairwise comparisons. CI compare North region Napa Valley \\[\\begin{eqnarray*}\n\\hat{\\beta}_2 &\\pm& t_{1-\\alpha/(2\\times3); 38-4} se(\\hat{\\beta}_2) \\nonumber \\\\\n-1.2234 &\\pm& 2.518259 \\times 0.4003 \\nonumber \\\\\n(-2.2314592 &,& -0.2153408). \\nonumber\n\\end{eqnarray*}\\]CI compare Central region Napa Valley \\[\\begin{eqnarray*}\n\\hat{\\beta}_3 &\\pm& t_{1-\\alpha/(2\\times3); 38-4} se(\\hat{\\beta}_3) \\nonumber \\\\\n-2.7569 &\\pm& 2.518259 \\times 0.4495 \\nonumber \\\\\n(-3.888858 &,& -1.624942). \\nonumber\n\\end{eqnarray*}\\]CI compare North region Central region \\[\\begin{eqnarray*}\n(\\hat{\\beta}_2-\\hat{\\beta}_3) &\\pm&\nt_{1-\\alpha/(2\\times3); 38-4}\\sqrt{Var\\left(\\hat{\\beta}_2-\\hat{\\beta}_3\\right)} \\nonumber \\\\\n(-1.2234 + 2.7569) &\\pm& 2.518259 \\sqrt{0.160 + 0.202 - 2 \\times 0.113} \\nonumber \\\\\n(0.6048119&,& 2.4621881) \\nonumber\n\\end{eqnarray*}\\]three CIs exclude 0, significant difference mean quality rating wines pairs regions, controlling flavor rating.","code":""},{"path":"cat.html","id":"hypothesis-tests","chapter":"8 Categorical Predictors in MLR","heading":"8.5.3.2 Hypothesis tests","text":"critical value, based Bonferroni procedure, \\(t_{1-\\alpha/(2g); n-p}\\) (instead \\(t_{1-\\alpha/2; n-p}\\)). test statistic still takes form\\[\nt = \\frac{\\text{estimate}}{s.e. \\text{ estimate}}.\n\\]compare North region Napa Valley, \\(H_0: \\beta_2 = 0, H_a: \\beta_2 \\neq 0\\). test statistic \\[\\begin{eqnarray*}\nt &=& \\frac{\\hat{\\beta}_2}{se(\\hat{\\beta}_2)} \\nonumber \\\\\n  &=& \\frac{-1.2234}{0.4003} \\nonumber \\\\\n  &=& -3.056208.\n\\end{eqnarray*}\\]compare Central region Napa Valley, \\(H_0: \\beta_3 = 0, H_a: \\beta_3 \\neq 0\\). test statistic \\[\\begin{eqnarray*}\nt &=& \\frac{\\hat{\\beta}_3}{se(\\hat{\\beta}_3)} \\nonumber \\\\\n  &=& \\frac{-2.7569}{0.4495} \\nonumber \\\\\n  &=& -6.133259.\n\\end{eqnarray*}\\]compare North region Central region, \\(H_0: \\beta_2 - \\beta_3 = 0, H_a: \\beta_2 - \\beta_3 \\neq 0\\). test statistic \\[\\begin{eqnarray*}\nt &=& \\frac{\\hat{\\beta}_2 - \\hat{\\beta}_3}{se(\\hat{\\beta}_2 - \\hat{\\beta}_3)} \\nonumber \\\\\n  &=& \\frac{-1.2234 + 2.7569}{\\sqrt{0.160 + 0.202 - 2 \\times 0.113}} \\nonumber \\\\\n  &=& 4.158286.\n\\end{eqnarray*}\\]critical value 2.518259, found using qt(1-0.05/(2*3), 38-4). magnitudes test statistics larger critical value, significant difference mean quality rating wines pair regions, controlling flavor rating.View associated video additional set notes explanation rationale behind Bonferroni procedure.","code":""},{"path":"cat.html","id":"tukey-procedure","chapter":"8 Categorical Predictors in MLR","heading":"8.5.4 Tukey procedure","text":"calculations involved Tukey procedure little bit involved cover details. can take look output based Tukey procedure:line showing result testing mean quality rating differs listed pair regions (controlling flavor rating). note results three hypothesis tests significant. data support claim significant difference mean quality wines pairs regions, controlling flavor rating.Given negative values difference estimated coefficients, wines \nNapa valley highest ratings, followed wines North region, \nwines Central region, flavor rating controlled.","code":"\nlibrary(multcomp)\npairwise<-multcomp::glht(reduced, linfct = mcp(Region= \"Tukey\"))\nsummary(pairwise)## \n##   Simultaneous Tests for General Linear Hypotheses\n## \n## Multiple Comparisons of Means: Tukey Contrasts\n## \n## \n## Fit: lm(formula = Quality ~ Flavor + Region, data = Data)\n## \n## Linear Hypotheses:\n##                      Estimate Std. Error t value Pr(>|t|)    \n## North - Napa == 0     -1.2234     0.4003  -3.056 0.011669 *  \n## Central - Napa == 0   -2.7569     0.4495  -6.134  < 1e-04 ***\n## Central - North == 0  -1.5335     0.3688  -4.158 0.000606 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## (Adjusted p values reported -- single-step method)"},{"path":"cat.html","id":"comments-about-multiple-comparisons","chapter":"8 Categorical Predictors in MLR","heading":"8.5.5 Comments about Multiple Comparisons","text":"procedures handle multiple pairwise comparisons require interactions involving categorical predictor, assume difference mean response long control predictors.comments Bonferroni procedure:probability making least one Type error \\(\\leq \\alpha\\). \\(g\\) increases, probability becomes lot less \\(\\alpha\\).product power (ability correctly reject null hypothesis) sacrificed, especially \\(g\\) increases.Confidence intervals higher level confidence wider.Bonferroni procedure considered conservative (less powerful, wider intervals).Fine use \\(g\\) known prior looking data small.Easy implement simple adjustment multiplier critical value.hand, Tukey procedure less conservative powerful. Typically used \\(g\\) larger.","code":""},{"path":"cat.html","id":"practical-considerations","chapter":"8 Categorical Predictors in MLR","heading":"8.6 Practical Considerations","text":"","code":""},{"path":"cat.html","id":"categorical-predictor-with-many-levels","chapter":"8 Categorical Predictors in MLR","heading":"8.6.1 Categorical predictor with many levels","text":"categorical predictor many levels, output can daunting look , number indicator variables (hence number regression coefficients) increases levels. things consider:really interested exploring differences mean response across levels?logical way collapse levels together still answers research question reduces number regression parameters?parameters needed may lead poorer predictive performance.","code":""},{"path":"cat.html","id":"discrete-predictors","chapter":"8 Categorical Predictors in MLR","heading":"8.6.2 Discrete predictors","text":"treat discrete predictors quantitative categorical MLR? things consider:fine assuming ``linear” relationship response variable? yes, likely treat variable quantitative.many distinct values discrete variable? distinct values, likely treat variable quantitative.concerned needlessly adding parameters model especially small sample size? yes, likely treat variable quantitative.","code":""},{"path":"cat.html","id":"r-tutorial-5","chapter":"8 Categorical Predictors in MLR","heading":"8.7 R Tutorial","text":"tutorial, use data set wine.txt. data set contains ratings various wines produced California. focus response variable \\(y=\\)Quality (average quality rating), \\(x_1=\\)Flavor (average flavor rating), Region indicating three regions California wine produced . regions coded \\(1=\\) North, \\(2=\\) Central, \\(3=\\) Napa.Read data also load tidyverse package","code":"\nlibrary(tidyverse)\nData<-read.table(\"wine.txt\", header=TRUE, sep=\"\")\nhead(Data)##   Clarity Aroma Body Flavor Oakiness Quality Region\n## 1       1   3.3  2.8    3.1      4.1     9.8      1\n## 2       1   4.4  4.9    3.5      3.9    12.6      1\n## 3       1   3.9  5.3    4.8      4.7    11.9      1\n## 4       1   3.9  2.6    3.1      3.6    11.1      1\n## 5       1   5.6  5.1    5.5      5.1    13.3      1\n## 6       1   4.6  4.7    5.0      4.1    12.8      1"},{"path":"cat.html","id":"data-wrangling","chapter":"8 Categorical Predictors in MLR","heading":"Data Wrangling","text":"Notice description variable Region recorded numerically, even though categorical variable. need make sure R viewing type correctly applying function class() variable:need convert Region viewed categorical using factor(), otherwise R treat quantitative predictor use dummy coding:can check levels described:Notice names levels regions descriptive. also give descriptive names levels Region:done needed data wrangling: making sure categorical variables viewed factors, giving descriptive names levels categorical variable.Note: categorical variables already dummy coded, need convert factor fitting MLR using lm(). lm() function converts factors dummy codes.","code":"\n##is Region a factor?\nclass(Data$Region)## [1] \"integer\"\n##convert Region to factor\nData$Region<-factor(Data$Region) \n##check Region is now the correct type\nclass(Data$Region) ## [1] \"factor\"\n##how are levels described\nlevels(Data$Region)## [1] \"1\" \"2\" \"3\"\n##Give names to the levels\nlevels(Data$Region) <- c(\"North\", \"Central\", \"Napa\") \nlevels(Data$Region)## [1] \"North\"   \"Central\" \"Napa\""},{"path":"cat.html","id":"scatterplot-with-categorical-predictor","chapter":"8 Categorical Predictors in MLR","heading":"Scatterplot with categorical predictor","text":"Since quantitative response variable, Quality, quantitative predictor Flavor categorical predictor Region, can create scatterplot, Quality y-axis, Flavor x-axis, use different colored plots denote different regions:notice positive linear association Quality Flavor across three regions, better flavor wine, higher quality rating wine.slopes exactly parallel, indicating may exist interaction region wine flavor; impact flavor quality rating differs among regions. regression model interaction region flavor may appropriate.","code":"\nggplot2::ggplot(Data, aes(x=Flavor, y=Quality, color=Region))+\n  geom_point()+\n  geom_smooth(method=lm, se=FALSE)+\n  labs(title=\"Scatterplot of Wine Quality against Flavor, by Region\")"},{"path":"cat.html","id":"fitting-mlr","chapter":"8 Categorical Predictors in MLR","heading":"Fitting MLR","text":"Since categorical variable Region three levels, know two indicator variables created represent various regions. check dummy coding using contrasts() function:output informs us North region reference class, coded 0 indicator variables. can change reference class Napa region via relevel() function:Based possibility non-parallel slopes scatterplot, consider fitting regression model interaction term predictors:Given \\(t\\) tests interaction terms insignificant, conduct partial \\(F\\) test see interaction terms can dropped:insignificant result partial \\(F\\) test means can drop interaction terms. little evidence slopes truly different.regression assumptions categorical predictor involved pretty much , assessed similarly .","code":"\n##check dummy coding\ncontrasts(Data$Region)##         Central Napa\n## North         0    0\n## Central       1    0\n## Napa          0    1\n##Set a different reference class\nData$Region<-relevel(Data$Region, ref = \"Napa\") \ncontrasts(Data$Region)##         North Central\n## Napa        0       0\n## North       1       0\n## Central     0       1\nresult<-lm(Quality~Flavor*Region, data=Data)\nsummary(result)## \n## Call:\n## lm(formula = Quality ~ Flavor * Region, data = Data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.94964 -0.58463  0.04393  0.49607  1.97295 \n## \n## Coefficients:\n##                      Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)           10.1144     1.6692   6.060 9.14e-07 ***\n## Flavor                 0.7957     0.2936   2.710   0.0107 *  \n## RegionNorth           -3.3833     2.0153  -1.679   0.1029    \n## RegionCentral         -6.2775     2.4491  -2.563   0.0153 *  \n## Flavor:RegionNorth     0.4029     0.3878   1.039   0.3066    \n## Flavor:RegionCentral   0.7137     0.4992   1.430   0.1625    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8914 on 32 degrees of freedom\n## Multiple R-squared:  0.8357, Adjusted R-squared:  0.8101 \n## F-statistic: 32.56 on 5 and 32 DF,  p-value: 1.179e-11\n##notice hierarchical principle is used\n##fit regression with no interaction\nreduced<-lm(Quality~Flavor+Region, data=Data)\n##general linear F test for interaction terms\nanova(reduced,result)## Analysis of Variance Table\n## \n## Model 1: Quality ~ Flavor + Region\n## Model 2: Quality ~ Flavor * Region\n##   Res.Df    RSS Df Sum of Sq      F Pr(>F)\n## 1     34 27.213                           \n## 2     32 25.429  2    1.7845 1.1229 0.3378\npar(mfrow=c(2,2))\nplot(reduced)"},{"path":"cat.html","id":"multiple-comparisons","chapter":"8 Categorical Predictors in MLR","heading":"Multiple comparisons","text":"Since model interactions, can interpret coefficients indicator variables difference mean quality rating, given flavor rating, class question reference class. regression equation \\[\nE(y|x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 I_1 + \\beta_3 I_2\n\\]\n\\(I_1 = 1\\) North 0 otherwise, \\(I_2 = 1\\) Central 0 otherwise.Plugging values indicator variables, can write regression equations regionNorth: \\(E(y|x) = \\beta_0 + \\beta_ 2 + \\beta_1 x_1\\).Central: \\(E(y|x) = \\beta_0 + \\beta_ 3 + \\beta_1 x_1\\).Napa: \\(E(y|x) = \\beta_0 + \\beta_1 x_1\\).Since 3 levels, 3 possible pairs regions compare (controlling flavor rating):North Napa: denoted \\(\\beta_ 2\\)Central Napa: denoted \\(\\beta_ 3\\)North Central: \\(\\beta_2 - \\beta_3\\)Let’s take look estimated coefficientsThe estimated difference mean quality rating sampled wines North Napa regions -1.22, given flavor ratings.first two comparisons easy can just refer coefficients indicator variables. little bit work needs done compare North Central regions. Also, note performing three hypothesis tests. need use multiple comparison methods ensure probability making least one Type error significance level 0.05.","code":"\nsummary(reduced)## \n## Call:\n## lm(formula = Quality ~ Flavor + Region, data = Data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.97630 -0.58844  0.02184  0.51572  1.94232 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)     8.3177     1.0100   8.235 1.31e-09 ***\n## Flavor          1.1155     0.1738   6.417 2.49e-07 ***\n## RegionNorth    -1.2234     0.4003  -3.056  0.00435 ** \n## RegionCentral  -2.7569     0.4495  -6.134 5.78e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8946 on 34 degrees of freedom\n## Multiple R-squared:  0.8242, Adjusted R-squared:  0.8087 \n## F-statistic: 53.13 on 3 and 34 DF,  p-value: 6.358e-13"},{"path":"cat.html","id":"bonferroni-procedure-1","chapter":"8 Categorical Predictors in MLR","heading":"Bonferroni procedure","text":"noted earlier, 3 pairs regions compare (controlling flavor rating):North Napa: denoted \\(\\beta_ 2\\)Central Napa: denoted \\(\\beta_ 3\\)North `Central: \\(\\beta_2 - \\beta_3\\)t multiplier critical value now \\(t_{1-\\frac{\\alpha}{2g}, n-p}\\), can find :","code":"\nn<-dim(Data)[1]\np<-4\ng<-3\nt.bon<-qt(1-0.05/(2*g), n-p)\nt.bon## [1] 2.518259"},{"path":"cat.html","id":"pairwise-comparison-with-reference-class","chapter":"8 Categorical Predictors in MLR","heading":"Pairwise comparison with reference class","text":"difference mean quality rating wines North Napa regions, controlling flavor, use confidence interval \\(\\beta_2\\), .e.,\\[\n\\hat{\\beta}_2 \\pm t_{1-\\frac{\\alpha}{2g}, n-p} se(\\hat{\\beta}_2) = {-1.2234} \\pm 2.518259 \\times 0.4003 = (-2.2315, -0.2153)\n\\]excludes 0, significant difference mean quality rating wines North Napa regions, controlling flavor. Since interval negative, can say mean quality rating wines Napa region higher North region, controlling flavor.conduct hypothesis test \\(H_0: \\beta_2 = 0, H_a: \\beta_2 \\neq 0\\), can use critical value (t multiplier) 2.518259. test statistic \\[\n\\frac{\\hat{\\beta}_2}{se(\\hat{\\beta}_2)} = \\frac{-1.2234}{0.4003} = -3.056\n\\]\nlarger magnitude critical value, reject null hypothesis.Notice result confidence interval hypothesis test consistent level significance.","code":""},{"path":"cat.html","id":"pairwise-comparison-excluding-reference-class","chapter":"8 Categorical Predictors in MLR","heading":"Pairwise comparison excluding reference class","text":"Comparisons excluding reference require bit work. difference mean quality rating wines North Central regions, controlling flavor, use CI \\(\\beta_2 - \\beta_3\\):\\[\n(\\hat{\\beta}_2 - \\hat{\\beta}_3) \\pm t_{1-\\frac{\\alpha}{2g}, n-p} se(\\hat{\\beta}_2 - \\hat{\\beta}_3)\n\\]output summary() provide \\(se(\\hat{\\beta}_2 - \\hat{\\beta}_3)\\). obtain values calculate , need produce variance-covariance matrix estimated coefficientsSo \\[\nVar(\\hat{\\beta}_2-\\hat{\\beta}_3) = Var(\\hat{\\beta}_2) + Var(\\hat{\\beta}_3) - 2Cov(\\hat{\\beta}_2,\\hat{\\beta}_3) = 0.1603 + 0.2020 - 2 \\times 0.1131 = 0.1361\n\\]\nTherefore, CI \\(\\beta_2 - \\beta_3\\) \\[\n(-1.2234 - -2.7569 ) \\pm 2.518259 \\times \\sqrt{0.1361} = (0.6045, 2.4625)\n\\]\nexcludes 0, significant difference mean quality rating wines North Central regions, controlling flavor. Since interval positive, can say mean quality rating wines North region higher Central region, controlling flavor.perform hypothesis test \\(H_0: \\beta_2 - \\beta_3 = 0, H_a: \\beta_2 - \\beta_3 \\neq 0\\), need calculate t-statistic\\[\nt-stat = \\frac{\\hat{\\beta}_2 - \\hat{\\beta}_3}{se(\\hat{\\beta}_2 - \\hat{\\beta}_3)} = \\frac{-1.2234 - -2.7569}{\\sqrt{0.1361}} = 4.1568\n\\]\nlarger magnitude critical value 2.518259, reject null hypothesis. , conclusions hypothesis test CI consistent.","code":"\nvcov(reduced)##               (Intercept)      Flavor RegionNorth RegionCentral\n## (Intercept)     1.0201363 -0.16975148 -0.27722389   -0.27700199\n## Flavor         -0.1697515  0.03022282  0.03748222    0.03744271\n## RegionNorth    -0.2772239  0.03748222  0.16026554    0.11313506\n## RegionCentral  -0.2770020  0.03744271  0.11313506    0.20201780"},{"path":"cat.html","id":"tukey-procedure-1","chapter":"8 Categorical Predictors in MLR","heading":"Tukey procedure","text":"Another procedure multiple pairwise comparisons Tukey procedure. use glht() function multcomp packageWe can see significant difference mean Quality rating pairs regions, given flavor rating, since tests significant.Given negative values difference estimated coefficients, wines Napa valley highest ratings, followed wines North region, wines Central region, flavor rating controlled.","code":"\nlibrary(multcomp)\npairwise<-multcomp::glht(reduced, linfct = mcp(Region= \"Tukey\"))\nsummary(pairwise)## \n##   Simultaneous Tests for General Linear Hypotheses\n## \n## Multiple Comparisons of Means: Tukey Contrasts\n## \n## \n## Fit: lm(formula = Quality ~ Flavor + Region, data = Data)\n## \n## Linear Hypotheses:\n##                      Estimate Std. Error t value Pr(>|t|)    \n## North - Napa == 0     -1.2234     0.4003  -3.056 0.011651 *  \n## Central - Napa == 0   -2.7569     0.4495  -6.134  < 1e-04 ***\n## Central - North == 0  -1.5335     0.3688  -4.158 0.000573 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## (Adjusted p values reported -- single-step method)"},{"path":"crit.html","id":"crit","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"9 Model Selection Criteria and Automated Search Procedures","text":"","code":""},{"path":"crit.html","id":"introduction-8","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"9.1 Introduction","text":"building regression model, faced two conflicting objectives:include predictors model improve predictive ability model andto include predictors unnecessary, lead uncertainty predictions make model needlessly complicated, making predictions interpretations challenging.example, recall NFL dataset seen worked previous modules. data set, presented 9 potential predictors predict number wins team. faced question many predictors need include model. start using just offensive statistics, start also including strength schedule? just use predictors right away?module, learn various ways assess different competing models.","code":""},{"path":"crit.html","id":"model-assessment-concepts","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"9.2 Model Assessment Concepts","text":"Previously, looked model diagnostics, tools assess assumptions regression model met. assessing assumption error terms regression model independent identically distributed normal distribution mean 0 constant variance denoted \\(\\sigma^2\\).Meeting assumptions guarantee model fits data well, variance error terms large. now shift attention model assessment, measures well model fits data well model predicts future observations.Recall two main uses regression models:Prediction: Predict future value response variable, using information predictor variables.Association: Quantify relationship variables. change predictor variable change value response variable, average?multiple predictors choose , need way chose possible models can fit. example, \\(k\\) predictors, \\(2^{k}\\) possible models just additive effects consider. predictors include?Generally speaking, can improve model fit (terms improving \\(R^2\\) reducing \\(SS_{res}\\)) adding predictors (even added needlessly).Generally speaking, can improve model fit (terms improving \\(R^2\\) reducing \\(SS_{res}\\)) adding predictors (even added needlessly).However, needlessly add predictors, predictive ability model can suffer, make model difficult interpret.However, needlessly add predictors, predictive ability model can suffer, make model difficult interpret.can see needlessly adding predictors negatively affects uses regression models. Generally, apply principle “Ockham’s razor”:\nGiven two theories describe phenomenon equally well, prefer theory simpler. selecting regression models, choose one fewer parameters multiple models give nearly fit data. always asking: improvement fit models parameters justify extra complexity?Model assessment can viewed trade-model complexity model fit, model can interpreted predicts well future data.","code":""},{"path":"crit.html","id":"training-and-test-data","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"9.2.1 Training and test data","text":"now, realize model fits data well necessarily predict well future observations. introduce definitions training test data:Training data: observations used build regression model.Test data: observations used build regression model solely used assess well model predicts future observations.Model fit typically measures well training data fits model (e.g. \\(R^2\\), \\(SS_{res}\\)). Model assessment measures well model predicting test data.","code":""},{"path":"crit.html","id":"overfitting","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"9.2.2 Overfitting","text":"model fits training data well guaranteed predict test data. fact, model performs lot worse test data training data indication model needlessly complicated. model overfitted.may seem counter intuitive model parameters perform worse test data. reason following: mentioned models take form \\(y = f(\\boldsymbol{x}) + \\epsilon\\), function \\(f\\) denotes true relationship response predictor variables, \\(\\epsilon\\) denotes random error. model overfitted fails account errors properly incorporating errors estimation \\(f\\), estimated \\(f\\) good approximation true relationship \\(f\\). end using poor approximation \\(f\\) predicting test data.Model selection criteria used select several models assessing models terms model fit model complexity. criteria prevent overfitting happening.","code":""},{"path":"crit.html","id":"model-selection-criteria","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"9.3 Model Selection Criteria","text":"variety model selection criteria proposed assess regression models. criteria typically measure model fit penalty additional parameter. penalty penalizes model gets complicated.coefficient determination, \\(R^2\\), typically used model selection criteria, since always increases add parameters model, favor models add parameters. \\(R^2\\) penalty additional parameter. \\(R^2\\) can used comparing models number parameters.next look various model selection criteria penalty additional parameter.","code":""},{"path":"crit.html","id":"adjusted-r-squared-r_adjp2","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"9.3.1 Adjusted R-squared: \\(R_{Adj,p}^{2}\\)","text":"adjusted R-squared, denoted \\(R_{Adj,p}^{2}\\), \\[\\begin{equation}\nR_{Adj,p}^{2} = 1 - \\left( \\frac{n-1}{n-p} \\right) (1 - R^2_{p})\n\\tag{9.1}\n\\end{equation}\\]\\(R_{Adj,p}^{2}\\) measure fit penalty additional parameters.including one additional parameter, \\(R^2_{p}\\) increase divisor \\(n-p\\) decreases.Choosing model largest \\(R_{Adj,p}^{2}\\) equivalent selecting one smallest \\(MS_{res}(p)\\).Larger value better.","code":""},{"path":"crit.html","id":"mallows-c_p","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"9.3.2 Mallow’s \\(C_p\\)","text":"Mallow’s \\(C_p\\) measures variance bias associated predicting test data. context MLR, found using\\[\\begin{equation}\nC_p = \\frac{SS_{res}(p)}{MS_{res}(P)} - (n-2p).\n\\tag{9.2}\n\\end{equation}\\]\\(C_p\\) measure fit penalty additional parameters.including one additional parameter, \\(2p\\) increases \\(SS_{res}\\) decreases.\\(MS_{res}(P)\\) denotes \\(MS_{res}\\) model \\(P\\) parameters.authors suggest choosing simplest model \\(C_p\\) closest \\(p\\). argument model unbiased, \\(C_p = p\\).However, unbiased models guaranteed perform best test data. , select model smallest \\(C_p\\).","code":""},{"path":"crit.html","id":"aic_p-and-bic_p","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"9.3.3 \\(AIC_p\\) and \\(BIC_p\\)","text":"couple related measures also used, Akaike information criterion, \\(AIC_p\\), Bayesian information criterion, \\(BIC_p\\). MLR, \\[\\begin{equation}\n\\mbox{AIC}_p = n\\log \\frac{SS_{res}(p)}{n} + 2p\n\\tag{9.3}\n\\end{equation}\\]\\[\\begin{equation}\n\\mbox{BIC}_p = n\\log \\frac{SS_{res}(p)}{n} +p\\log n.\n\\tag{9.4}\n\\end{equation}\\]measures fit penalty additional parameters.including one additional parameter, \\(2p\\) \\(p\\log n\\) increase \\(SS_{res}\\) decreases.Models low \\(AIC_p\\), \\(BIC_p\\) desired.\\(\\log n > 2\\) (.e. \\(n \\geq 8\\)), \\(BIC_p\\) increases \\(p\\) quickly \\(AIC_p\\). Thus, \\(BIC_p\\) favors smaller models compared AIC.","code":""},{"path":"crit.html","id":"press-statistic","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"9.3.4 PRESS statistic","text":"PRESS statistic measures difference predicted response observation excluded estimating model. defined \\[\\begin{equation}\nPRESS = \\sum_{=1}^n\\left(y_i-\\hat{y}_{()}\\right)^2\n\\tag{9.5}\n\\end{equation}\\]\\(\\hat{y}_{()}\\) predicted value \\(\\)th observation\nregression fitted without \\(\\)th observation.Small PRESS statistic desired.Models small PRESS statistics small prediction errors test data.criteria listed, PRESS statistic (9.5) motivated measuring prediction error test data, whereas criteria (9.1), (9.2), (9.3), (9.4) motivated balancing model fit model complexity, consequence perform well predicting test data.related measure PRESS statistic (9.5) \\(R^2_{prediction}\\),\\[\\begin{equation}\nR^2_{prediction} = 1 - \\frac{PRESS}{SS_T},\n\\tag{9.6}\n\\end{equation}\\]can interpreted proportion variability response new observations can explained model.average, \\(R^2_{prediction}\\) less \\(R^2_p\\).\\(R^2_{prediction}\\) lot smaller \\(R^2_p\\) indicates overfitting.","code":""},{"path":"crit.html","id":"summary-and-final-comments","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"9.3.5 Summary and final comments","text":"introduced number criteria. criteria attempt measure model fit complexity, mathematically equivalent. None regarded definitive , taken together\nprovide range possible options , combined \njudgment data , can usually used decide\nupon model reasonable criteria. couple points note:still need check model regression assumptions.criteria can used compare models response variable, affected unit associated response variable.","code":""},{"path":"crit.html","id":"automated-search-procedures","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"9.4 Automated Search Procedures","text":"multiple predictors choose , need way choose possible models can fit. example, \\(k\\) quantitative predictors, \\(2^{k}\\) possible models just additive effects consider. \\(k\\) large, many models compare. Can automate process model selection make things computationally efficient?","code":""},{"path":"crit.html","id":"forward-selection-backward-elimination","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"9.4.1 Forward selection, backward elimination","text":"couple methods automate process:Forward Selection:\nbegin model predictors, add predictor variables one time optimal way, desirable stopping point reached.\nbegin model predictors, add predictor variables one time optimal way, desirable stopping point reached.Backward Elimination:\nbegin model potential predictors, remove “weak” predictor variables one time optimal way, desirable stopping point reached.\nbegin model potential predictors, remove “weak” predictor variables one time optimal way, desirable stopping point reached.critical concept step conditional previous step. instance, forward selection adding variable already selected.criterion add (eliminate) predictor step forward selection (backward elimination) might based \\(p\\)-value, \\(SS_{res}\\), \\(AIC_{p}\\), etc.Suppose use \\(AIC_p\\) criterion. know smaller values \\(\\mbox{AIC}_{p}\\) desired.forward step, given model size \\(p\\) previous step, compare candidate models size \\(p+1\\) adding one remaining predictor variables. Among size-(\\(p+1\\)) models, select one results smallest \\(AIC_{p+1}\\), also smaller \\(AIC_p\\) model previous step.backward step, given model size \\(p\\) previous step, compare candidate models size \\(p-1\\) eliminating one existing predictors. Among size-(\\(p-1\\)) models, remove one results smallest \\(AIC_{p-1}\\) value, also smaller \\(AIC_p\\) model previous step.algorithm stops \\(AIC\\) longer decreases.Let us consider toy example. Suppose four potential predictors, \\(x_1,x_2,x_3,x_4\\). table gives \\(AIC\\) possible models. Assume AIC intercept-model 25.Suppose start intercept-model. model gets selected forward election?step 1, add \\(x_3\\) intercept-model. model just 1 predictor smallest \\(AIC\\) also decreases AIC previous step.step 2, consider adding one three remaining predictors, \\(x_1, x_2, x_4\\) model \\(x_3\\) already . \\(x_3\\) removed. \\(x_4\\) added results smallest \\(AIC\\) also decreases AIC previous step.step 3, consider adding one two remaining predictors, \\(x_1, x_2\\) model \\(x_3, x_4\\) already . \\(x_2\\) added results smallest \\(AIC\\) also decreases AIC previous step.Algorithm stops , adding additional predictor step decrease AIC. model selected \\(x_2, x_3, x_4\\).","code":""},{"path":"crit.html","id":"practice-question-1","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"9.4.1.1 Practice question","text":"model gets selected backward elimination, start model 4 predictors?View associated video review practice question.","code":""},{"path":"crit.html","id":"stepwise-regression","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"9.4.1.2 Stepwise regression","text":"combination procedure called stepwise regression can also used. combination procedure step, algorithm considers adding remaining predictors removing current predictors.","code":""},{"path":"crit.html","id":"final-comments","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"9.4.2 Final comments","text":"still need check model regression assumptions. Also, model chosen procedures guaranteed . starting point, criteria used, can also affect model selected. Typically, automated search procedures consider models just additive effects.Use procedures starting point model building, definitive answer choose final model.","code":""},{"path":"crit.html","id":"r-tutorial-6","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"9.5 R Tutorial","text":"tutorial, learn use model selection criteria automated search procedures setting MLR model. using regsubsets() function leaps package automate process assessing models using model selection criteria, install load leaps package:use mtcars data set comes built R. data come 32 classic automobiles.Type ?mtcars read description data. Notice two variables, vs actually categorical coded using 0-1 indicators. Since correctly coded 0-1 indicators, need use factor() function convert variables viewed categorical.using lm(), R perform 0-1 coding associated categorical variables.examples , consider mpg response variable variables potential predictors.","code":"\nlibrary(leaps)\nData<-mtcars"},{"path":"crit.html","id":"model-selection-criteria-1","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"Model Selection Criteria","text":"regsubsets() function leaps package fit possible regression models based supplied dataframe specified response variable, calculate values \\(R^2\\), adjusted \\(R^2\\), \\(SS_{res}\\), Mallows \\(C_p\\), BIC model. calculate PRESS statistic AIC. :default value nbest 1. means algorithm return one best set predictors (based \\(R^2\\)) number possible predictors.based \\(R^2\\), among possible 1-predictor models, model best wt one predictor. Among possible 2-predictor models, model best cyl wt two predictors.Changing nbest 2 gives:based \\(R^2\\), among possible 1-predictor models, model best wt one predictor. second best 1-predictor model cyl one predictor.Let’s see can extracted summary(allreg2):can extract information regarding adjusted \\(R^2\\), Mallow’s \\(C_p\\), \\(BIC\\), can find best models based criteria:allreg2, model 9 best adjusted \\(R^2\\), model 5 best Mallow’s \\(C_p\\) \\(BIC\\). get corresponding coefficients predictors models:turns 2 candidate models. wt, qsec, . model best adjusted \\(R^2\\) two additional predictors: disp hp.","code":"\nallreg <- leaps::regsubsets(mpg ~., data=Data, nbest=1)\nsummary(allreg)## Subset selection object\n## Call: regsubsets.formula(mpg ~ ., data = Data, nbest = 1)\n## 10 Variables  (and intercept)\n##      Forced in Forced out\n## cyl      FALSE      FALSE\n## disp     FALSE      FALSE\n## hp       FALSE      FALSE\n## drat     FALSE      FALSE\n## wt       FALSE      FALSE\n## qsec     FALSE      FALSE\n## vs       FALSE      FALSE\n## am       FALSE      FALSE\n## gear     FALSE      FALSE\n## carb     FALSE      FALSE\n## 1 subsets of each size up to 8\n## Selection Algorithm: exhaustive\n##          cyl disp hp  drat wt  qsec vs  am  gear carb\n## 1  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n## 2  ( 1 ) \"*\" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n## 3  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n## 4  ( 1 ) \" \" \" \"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n## 5  ( 1 ) \" \" \"*\"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n## 6  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n## 7  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \" \" \n## 8  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \"*\"\nallreg2 <- leaps::regsubsets(mpg ~., data=Data, nbest=2)\nsummary(allreg2)## Subset selection object\n## Call: regsubsets.formula(mpg ~ ., data = Data, nbest = 2)\n## 10 Variables  (and intercept)\n##      Forced in Forced out\n## cyl      FALSE      FALSE\n## disp     FALSE      FALSE\n## hp       FALSE      FALSE\n## drat     FALSE      FALSE\n## wt       FALSE      FALSE\n## qsec     FALSE      FALSE\n## vs       FALSE      FALSE\n## am       FALSE      FALSE\n## gear     FALSE      FALSE\n## carb     FALSE      FALSE\n## 2 subsets of each size up to 8\n## Selection Algorithm: exhaustive\n##          cyl disp hp  drat wt  qsec vs  am  gear carb\n## 1  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n## 1  ( 2 ) \"*\" \" \"  \" \" \" \"  \" \" \" \"  \" \" \" \" \" \"  \" \" \n## 2  ( 1 ) \"*\" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n## 2  ( 2 ) \" \" \" \"  \"*\" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n## 3  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n## 3  ( 2 ) \"*\" \" \"  \"*\" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n## 4  ( 1 ) \" \" \" \"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n## 4  ( 2 ) \" \" \" \"  \" \" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \"*\" \n## 5  ( 1 ) \" \" \"*\"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n## 5  ( 2 ) \" \" \" \"  \" \" \"*\"  \"*\" \"*\"  \" \" \"*\" \" \"  \"*\" \n## 6  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n## 6  ( 2 ) \" \" \"*\"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \"*\"  \" \" \n## 7  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \" \" \n## 7  ( 2 ) \"*\" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n## 8  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \"*\" \n## 8  ( 2 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \"*\" \"*\" \"*\"  \" \"\nnames(summary(allreg2))## [1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"\nwhich.max(summary(allreg2)$adjr2)## [1] 9\nwhich.min(summary(allreg2)$cp)## [1] 5\nwhich.min(summary(allreg2)$bic)## [1] 5\ncoef(allreg2, which.max(summary(allreg2)$adjr2))## (Intercept)        disp          hp          wt        qsec          am \n## 14.36190396  0.01123765 -0.02117055 -4.08433206  1.00689683  3.47045340\ncoef(allreg2, which.min(summary(allreg2)$cp))## (Intercept)          wt        qsec          am \n##    9.617781   -3.916504    1.225886    2.935837\ncoef(allreg2, which.min(summary(allreg2)$bic))## (Intercept)          wt        qsec          am \n##    9.617781   -3.916504    1.225886    2.935837"},{"path":"crit.html","id":"forward-selection-backward-elimination-and-stepwise-regression","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"Forward selection, backward elimination, and stepwise regression","text":"Fitting possible regression models can slow run many predictors many observations. cut number potential models considered, can use forward selection, backward elimination, stepwise regression. procedures require declaring smallest possible model, largest possible model first. algorithm consider models within range models., start declaring intercept-model full model (predictors). two models contain scope possible models consider:carry forward selection:start algorithm, AIC intercept-model calculated 115.94. algorithm considers adding predictor intercept-model. 1-predictor model, AIC calculated, 1-predictor models arranged smallest largest terms AIC. output, 1-predictor models superior intercept-model. predictor wt chosen used since results model smallest AIC.next step, wt model removed. AIC 73.22. algorithm considers adding predictor addition wt. two-predictor model, AIC calculated. two-predictor models ordered smallest largest. Adding cyl leads smallest AIC chosen added wt. Note adding drat, gear, wt actually increases AIC.algorithm continues last step. stage, wt, cyl, hp added model AIC 62.66. algorithm considers adding one remaining predictors, adding results higher AIC. Thus algorithm stops.backward elimination, code similar. direction changed forward backward:stepwise regression, direction set :Notice stepwise regression, consider removing predictor well adding predictor step.turns dataset, forward selection, backward elimination, stepwise regression propose model wt, cyl, hp predictors.","code":"\n##intercept only model\nregnull <- lm(mpg~1, data=Data)\n##model with all predictors\nregfull <- lm(mpg~., data=Data)\nstep(regnull, scope=list(lower=regnull, upper=regfull), direction=\"forward\")## Start:  AIC=115.94\n## mpg ~ 1\n## \n##        Df Sum of Sq     RSS     AIC\n## + wt    1    847.73  278.32  73.217\n## + cyl   1    817.71  308.33  76.494\n## + disp  1    808.89  317.16  77.397\n## + hp    1    678.37  447.67  88.427\n## + drat  1    522.48  603.57  97.988\n## + vs    1    496.53  629.52  99.335\n## + am    1    405.15  720.90 103.672\n## + carb  1    341.78  784.27 106.369\n## + gear  1    259.75  866.30 109.552\n## + qsec  1    197.39  928.66 111.776\n## <none>              1126.05 115.943\n## \n## Step:  AIC=73.22\n## mpg ~ wt\n## \n##        Df Sum of Sq    RSS    AIC\n## + cyl   1    87.150 191.17 63.198\n## + hp    1    83.274 195.05 63.840\n## + qsec  1    82.858 195.46 63.908\n## + vs    1    54.228 224.09 68.283\n## + carb  1    44.602 233.72 69.628\n## + disp  1    31.639 246.68 71.356\n## <none>              278.32 73.217\n## + drat  1     9.081 269.24 74.156\n## + gear  1     1.137 277.19 75.086\n## + am    1     0.002 278.32 75.217\n## \n## Step:  AIC=63.2\n## mpg ~ wt + cyl\n## \n##        Df Sum of Sq    RSS    AIC\n## + hp    1   14.5514 176.62 62.665\n## + carb  1   13.7724 177.40 62.805\n## <none>              191.17 63.198\n## + qsec  1   10.5674 180.60 63.378\n## + gear  1    3.0281 188.14 64.687\n## + disp  1    2.6796 188.49 64.746\n## + vs    1    0.7059 190.47 65.080\n## + am    1    0.1249 191.05 65.177\n## + drat  1    0.0010 191.17 65.198\n## \n## Step:  AIC=62.66\n## mpg ~ wt + cyl + hp\n## \n##        Df Sum of Sq    RSS    AIC\n## <none>              176.62 62.665\n## + am    1    6.6228 170.00 63.442\n## + disp  1    6.1762 170.44 63.526\n## + carb  1    2.5187 174.10 64.205\n## + drat  1    2.2453 174.38 64.255\n## + qsec  1    1.4010 175.22 64.410\n## + gear  1    0.8558 175.76 64.509\n## + vs    1    0.0599 176.56 64.654## \n## Call:\n## lm(formula = mpg ~ wt + cyl + hp, data = Data)\n## \n## Coefficients:\n## (Intercept)           wt          cyl           hp  \n##    38.75179     -3.16697     -0.94162     -0.01804\nstep(regfull, scope=list(lower=regnull, upper=regfull), direction=\"backward\")\nstep(regnull, scope=list(lower=regnull, upper=regfull), direction=\"both\")## Start:  AIC=115.94\n## mpg ~ 1\n## \n##        Df Sum of Sq     RSS     AIC\n## + wt    1    847.73  278.32  73.217\n## + cyl   1    817.71  308.33  76.494\n## + disp  1    808.89  317.16  77.397\n## + hp    1    678.37  447.67  88.427\n## + drat  1    522.48  603.57  97.988\n## + vs    1    496.53  629.52  99.335\n## + am    1    405.15  720.90 103.672\n## + carb  1    341.78  784.27 106.369\n## + gear  1    259.75  866.30 109.552\n## + qsec  1    197.39  928.66 111.776\n## <none>              1126.05 115.943\n## \n## Step:  AIC=73.22\n## mpg ~ wt\n## \n##        Df Sum of Sq     RSS     AIC\n## + cyl   1     87.15  191.17  63.198\n## + hp    1     83.27  195.05  63.840\n## + qsec  1     82.86  195.46  63.908\n## + vs    1     54.23  224.09  68.283\n## + carb  1     44.60  233.72  69.628\n## + disp  1     31.64  246.68  71.356\n## <none>               278.32  73.217\n## + drat  1      9.08  269.24  74.156\n## + gear  1      1.14  277.19  75.086\n## + am    1      0.00  278.32  75.217\n## - wt    1    847.73 1126.05 115.943\n## \n## Step:  AIC=63.2\n## mpg ~ wt + cyl\n## \n##        Df Sum of Sq    RSS    AIC\n## + hp    1    14.551 176.62 62.665\n## + carb  1    13.772 177.40 62.805\n## <none>              191.17 63.198\n## + qsec  1    10.567 180.60 63.378\n## + gear  1     3.028 188.14 64.687\n## + disp  1     2.680 188.49 64.746\n## + vs    1     0.706 190.47 65.080\n## + am    1     0.125 191.05 65.177\n## + drat  1     0.001 191.17 65.198\n## - cyl   1    87.150 278.32 73.217\n## - wt    1   117.162 308.33 76.494\n## \n## Step:  AIC=62.66\n## mpg ~ wt + cyl + hp\n## \n##        Df Sum of Sq    RSS    AIC\n## <none>              176.62 62.665\n## - hp    1    14.551 191.17 63.198\n## + am    1     6.623 170.00 63.442\n## + disp  1     6.176 170.44 63.526\n## - cyl   1    18.427 195.05 63.840\n## + carb  1     2.519 174.10 64.205\n## + drat  1     2.245 174.38 64.255\n## + qsec  1     1.401 175.22 64.410\n## + gear  1     0.856 175.76 64.509\n## + vs    1     0.060 176.56 64.654\n## - wt    1   115.354 291.98 76.750## \n## Call:\n## lm(formula = mpg ~ wt + cyl + hp, data = Data)\n## \n## Coefficients:\n## (Intercept)           wt          cyl           hp  \n##    38.75179     -3.16697     -0.94162     -0.01804"},{"path":"crit.html","id":"some-comments","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"Some Comments","text":"regsubsets() step() functions consider 1st order models (interactions higher order terms).regsubsets() step() functions check regression assumptions met. still need check residual plot.regsubsets() step() functions guarantee best model identified specific goal.Notice various model selection criteria used regsubsets() can lead different models.various procedures step() function can lead different models.step() function can lead different models different starting point.step() function, R uses AIC decide stop search. textbook describes procedure using \\(F\\) statistic. choice criteria impact end result.","code":""},{"path":"crit.html","id":"practice-question-2","chapter":"9 Model Selection Criteria and Automated Search Procedures","heading":"Practice Question","text":"candidate models found , create residual plots Box Cox plots see response variable transformed. needed, transform response variable, re run regsubsets() step() functions see candidate models suggested.","code":""},{"path":"out.html","id":"out","chapter":"10 Analysis of Residuals in MLR","heading":"10 Analysis of Residuals in MLR","text":"","code":""},{"path":"out.html","id":"introduction-9","chapter":"10 Analysis of Residuals in MLR","heading":"10.1 Introduction","text":"compute sample average, data point much larger smaller rest data large influence value average. usually check see data point error (case value checked correctness) something interesting data point warrants closer look.Likewise, concerned observations high leverage, outlying, influential regression model. module, learn various measures detect observations. lot measures based residuals. Generally speaking, concerned influential observations, presence (removal) significantly alter estimated regression equation.Lastly, using residuals MLR help us assess predictor variables need transformed, , transform , order meet assumptions MLR.","code":""},{"path":"out.html","id":"terminology","chapter":"10 Analysis of Residuals in MLR","heading":"10.1.1 Terminology","text":"may noticed earlier paragraphs differentiating observations high leverage, outlying, influential. observations slightly different definitions:High leverage: observation whose predictor(s) extreme.Outlier: observation whose response variable follow general trend data.Influential: observation whose presence (removal) unduly affects part regression analysis, usually terms unduly affecting predicted response, estimated coefficients, results hypothesis tests confidence intervals.scatterplots Figure 10.1 display three examples types observations. using just one predictor ease visualization. example, 6 observations.\nFigure 10.1: Scatterplot Data high leverage, Outlying, Influential Observation\nFigure 10.1() displays scatterplot observation considered high leverage, denoted red diamond top right plot.general pattern observations positive linear association variables. \\(x\\) gets larger, \\(y\\) gets larger.high leverage observation high leverage observation, value \\(x\\) lot larger values \\(x\\) observations.high leverage observation large value \\(x\\), response value consistent general pattern observations. Therefore, considered outlier.figure, two estimated regression lines overlaid. line black regression line observations excluding high leverage observation, line red regression line high leverage observation included. lines almost indistinguishable. Therefore, high leverage observation influential.Figure 10.1(b) displays scatterplot observation considered outlier, denoted blue triangle top left plot.general pattern observations positive linear association variables. \\(x\\) gets larger, \\(y\\) gets larger.outlier high leverage observation, value \\(x\\) lot larger smaller values \\(x\\) observations.outlier small value \\(x\\), response value lot larger general pattern suggest. Therefore, considered outlier.figure, two estimated regression lines overlaid. line black regression line observations excluding outlier, line blue regression line outlier included. lines almost indistinguishable. Therefore, outlier influential.Figure 10.1(c) displays scatterplot observation considered influential, denoted orange cross bottom right plot.general pattern observations positive linear association variables. \\(x\\) gets larger, \\(y\\) gets larger.influential observation also high leverage observation, value \\(x\\) lot larger values \\(x\\) observations.influential observation large value \\(x\\), response value lot smaller general pattern suggest. Therefore, also considered outlier.figure, two estimated regression lines overlaid. line black regression line observations excluding influential observation line orange regression line influential observation included. lines different. Therefore, observation influential.Figure 10.1 , can see observation high leverage outlying guaranteed influential. general rule , observations high leverage /outlying potential influential. Observations outlying high leverage likely influential.now look measures detect observations. measures typically involve residuals, variation residuals, regression.","code":""},{"path":"out.html","id":"detecting-high-leverage-observations","chapter":"10 Analysis of Residuals in MLR","heading":"10.2 Detecting High Leverage Observations","text":"","code":""},{"path":"out.html","id":"limitation-of-residuals-in-detecting-high-leverage-observations","chapter":"10 Analysis of Residuals in MLR","heading":"10.2.1 Limitation of residuals in detecting high leverage observations","text":"intuitive way detect observations high leverage, outlying, influential, use residuals, defined :\\[\\begin{equation}\ne_i = y_i - \\hat{y_i}.\n\\tag{10.1}\n\\end{equation}\\]However, certain limitations using residuals detect observations. illustrate , let us go back Figure 10.1. Visually, residual vertical distance observation regression equation. Figures 10.1() 10.1(c), notice high leverage observation (red diamond) influential observation (orange cross) close estimated regression equation red orange respectively. observations small residuals. However, Figure 10.1(b) , outlier far away respective estimated regression equations, large residual. residuals may unable detect high leverage observations.Please view associated video mathematical explanation high leverage observations likely small residuals.","code":""},{"path":"out.html","id":"hat-matrix","chapter":"10 Analysis of Residuals in MLR","heading":"10.2.2 Hat matrix","text":"Recall MLR predicted response (fitted values), can written matrix form \\[\\begin{equation}\n\\boldsymbol{\\hat{y}} = \\boldsymbol{X\\hat{\\beta}}.\n\\tag{10.2}\n\\end{equation}\\]Using method least squares, estimated coefficients can found using\\[\\begin{equation}\n\\boldsymbol{\\hat{\\beta}} = \\left[\n\\begin{array}{c}\n   \\hat{\\beta}_0  \\\\\n   \\hat{\\beta}_1 \\\\\n   \\vdots \\\\\n   \\hat{\\beta}_k\n\\end{array}\n\\right]  =\n\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{y} .\n\\tag{10.3}\n\\end{equation}\\]Subbing (10.3) (10.2), \\[\\begin{eqnarray}\n\\boldsymbol{\\hat{y}} &=& \\boldsymbol{X} \\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{y} \\nonumber \\\\\n                     &=& \\boldsymbol{Hy},\n\\tag{10.4}\n\\end{eqnarray}\\]\\(\\boldsymbol{H} = \\boldsymbol{X (X^{\\prime}X)^{-1}X^\\prime}\\) called hat matrix. write (10.4) scalar form, \\[\\begin{eqnarray*}\n\\hat{y_1} &=& h_{11}y_1 + h_{12}y_2 + \\cdots + h_{1n}y_n \\nonumber \\\\\n\\hat{y_2} &=& h_{21}y_1 + h_{22}y_2 + \\cdots + h_{2n}y_n \\nonumber \\\\\n&=& \\vdots \\nonumber \\\\\n\\hat{y_n} &=& h_{n1}y_1 + h_{n2}y_2 + \\cdots + h_{nn}y_n \\nonumber\n\\end{eqnarray*}\\]\\(h_{ij}\\) denotes \\((,j)\\)th entry hat matrix.","code":""},{"path":"out.html","id":"leverage","chapter":"10 Analysis of Residuals in MLR","heading":"10.2.3 Leverage","text":"turns use diagonal entries hat matrix, \\(h_{ii}\\), define leverage, measures distance predictors observation \\(\\) center predictors observations.Using (10.4) scalar form, can see predicted response observation linear combination observed responses \\(y_1, y_2, \\cdots, y_n\\). leverage \\(h_{ii}\\) measures impact observation \\(y_i\\) predicting response. leverage high, observation \\(\\) high impact prediction. Leverages following properties:\\(h_{ii} = \\boldsymbol{X_{}}^{\\prime} \\left(\\boldsymbol{X^{\\prime}X} \\right)^{-1} \\boldsymbol{X_{}}\\).\\(0\\leq h_{ii}\\leq 1\\).\\(\\sum_{=1}^n h_{ii}=p\\), \\(p\\) number parameters. Therefore average leverages \\(\\frac{p}{n}\\).various recommendations determine observation high leverage. use \\(h_{ii} > \\frac{2p}{n}\\) general rule, twice average leverages. Note use guide, remember higher leverage, away predictors observation \\(\\) center predictors observations.Let us find residual leverage observations make scatterplot Figure 10.1(). Note 6 observations, high leverage observation index 6. First, take look residuals. absolute values residuals reported sorted increasing order:Notice high leverage observation residual absolute value 0.3052. observations larger residuals. residuals fail identify observation 6 high leverage. now look leverages, reported sorted increasing order:Note observation 6 leverage 0.9522, lot larger relative leverages five observations. Indeed, larger suggested criteria \\(\\frac{2p}{n} = 0.6667\\). can see leverage, residuals, measure used identify high leverage observations.","code":"\n##absolute value of residuals, sorted in increasing order\nsort(abs(result.lev$res))##         5         6         2         4         3         1 \n## 0.1561530 0.3052037 0.5633585 0.7479269 1.3792798 1.4147975\n##leverages, sorted in increasing order\nsort(lm.influence(result.lev)$hat)##         4         3         2         1         5         6 \n## 0.1667478 0.1839964 0.2132494 0.2345507 0.2492167 0.9522391\n##criteria\np<-2\nn<-dim(Data.lev)[1]\n2*p/n## [1] 0.6666667"},{"path":"out.html","id":"analysis-of-residuals-detecting-outliers","chapter":"10 Analysis of Residuals in MLR","heading":"10.3 Analysis of Residuals: Detecting Outliers","text":"previous section, noted residuals used detect high leverage observations. Another limitation numerical value residuals based unit response variable. makes residual large depends unit response variable. view limitation, consider standardizing residuals make unitless. ways standardizing residuals, useful detecting outliers.","code":""},{"path":"out.html","id":"standardized-residuals","chapter":"10 Analysis of Residuals in MLR","heading":"10.3.1 Standardized residuals","text":"Recall errors regression model, denoted \\(\\epsilon\\), variance denoted \\(\\sigma^2\\), turn estimated \\(MS_{res} = \\frac{\\sum(y_i - \\hat{y_i})^2}{n-p}\\). Therefore, standardized residuals, \\(d_i\\), found \\[\\begin{equation}\nd_{} = \\frac{e_i}{\\sqrt{MS_{res}}}.\n\\tag{10.5}\n\\end{equation}\\]dividing residuals, \\(e_i\\), standard error errors.","code":""},{"path":"out.html","id":"studentized-residuals","chapter":"10 Analysis of Residuals in MLR","heading":"10.3.2 Studentized residuals","text":"However, \\(MS_{res}\\) estimates variance errors, residuals. turns variance residuals \\[\\begin{equation}\nVar (e_i) = MS_{res}(1-h_{ii}).\n\\tag{10.6}\n\\end{equation}\\]standardizing residual using standard error residuals instead, get studentized residuals\\[\\begin{equation}\nr_i = \\frac{e_i}{\\sqrt{MS_{res}(1-h_{ii})}}.\n\\tag{10.7}\n\\end{equation}\\]Let us find residuals studentized residuals observations given Figure 10.1(b). Note 6 observations, outlying observation index 6. First, take look residuals. absolute values residuals reported sorted increasing order:Residuals identify outlying observation, index 6. However, numerical values residuals depend unit response variable, can hard ascertain value residual considered large. can look studentized residuals instead:studentized residual estimates many standard deviations predicted response actual response. predicted response outlying observation estimated 2 standard deviations away true value. Typically, studentized residuals magnitude larger 2 flagged. comparison five studentized residuals, outlier studentized residual quite lot larger others.Let us take look studentized residual observations shown Fig 10.1(c). , note 6 observations, influential observation index 6.value studentized residual influential observation 1.9409, pretty close 2. However, notice relationship studentized residuals, lot larger. may deem observation outlying, deem observation 4 also outlying, incorrect. can happen observation influential. , see limitation using studentized residuals flag outliers. refine residual can identify outliers clearly.","code":"\n##absolute value of residuals, sorted in increasing order\nsort(abs(result.out$res))##         1         2         4         5         3         6 \n##  1.259631  2.007224  2.796350  2.892751  3.754347 12.710304\n##absolute value of studentized residuals, sorted in increasing order\nsort(abs(rstandard(result.out)))##         1         2         5         3         4         6 \n## 0.2134751 0.3186190 0.5258773 0.5970578 0.8023354 1.9850133\nsort(abs(rstandard(result.both)))##         2         3         1         5         4         6 \n## 0.1180102 0.1618282 0.3298582 1.1348254 1.8125713 1.9409466"},{"path":"out.html","id":"deleted-residuals","chapter":"10 Analysis of Residuals in MLR","heading":"10.3.3 Deleted residuals","text":"seen example previous subsection influential observation also outlying studentized residuals lot larger studentized residuals. observation also high leverage (example Fig 10.1(c)), observation likely pull estimated regression equation towards , resulting small residual studentized residual lot larger rest.address issue, remove observation question estimating regression model, observation pull regression equation towards .can use deleted residual measure . defined difference value actual response observation prediction observation used estimate regression equation. deleted residual observation \\(\\) denoted \\[\\begin{equation}\ne_{()}=y_i-\\hat{y}_{()}.\n\\tag{10.8}\n\\end{equation}\\]two values subscript \\(\\hat{y}_{()}\\). value outside subscript denotes observation making prediction . subscript surrounded parenthesis indicates observation removed estimating regression equation.Let us use Figure 10.1(c) example demonstrate. influential observation denoted orange cross, observation 6:\\(y_6\\) denotes value response variable observation.\\(\\hat{y}_{6(6)}\\) denotes predicted response observation, removed estimating regression equation. Visually, Figure 10.1(c), \\(\\hat{y}_{6(6)}\\) predicted response based black regression line, since black regression line estimated without observation 6.\\(e_{(6)}\\) denotes vertical distance observation 6 (orange cross) black regression line.mathematically equivalent form (10.8) \\[\\begin{equation}\ne_{()} = \\frac{e_i}{1-h_{ii}}.\n\\tag{10.9}\n\\end{equation}\\]Note larger \\(h_{ii}\\) , deleted residual larger compared residual. Thus deleted residuals times identify outliers residuals , especially high leverage.Just like residuals, deleted residuals depend unit response variable, scale deleted residuals.Please view associated video explanation behind deleted residuals.","code":""},{"path":"out.html","id":"externally-studentized-residuals","chapter":"10 Analysis of Residuals in MLR","heading":"10.3.4 Externally studentized residuals","text":"can scale deleted residuals obtain residuals, \\[\\begin{equation}\nt_i = \\frac{e_i}{\\sqrt{MS_{res()}(1-h_{ii})}}.\n\\tag{10.10}\n\\end{equation}\\]Note \\(MS_{res()}\\) denotes \\(MS_{res}\\) regression model estimated without observation \\(\\). computational standpoint, calculating \\(t_i\\)s way requires us fit \\(n\\) regression models, order find \\(MS_{res()}\\) \\(t_i\\).computationally efficient compute \\(t_i\\) \\[\\begin{equation}\nt_i =\ne_i\\left[\\frac{n-1-p}{SS_{res}(1-h_{ii})-e_i^2}\\right]^{1/2}.\n\\tag{10.11}\n\\end{equation}\\]formulation (10.11) requires us fit one estimated regression model. reason (10.10) (10.11) equivalent based relationship \\(MS_{res}\\) \\(MS_{res()}\\):\\[\\begin{equation}\n(n-p)MS_{res}=(n-1-p)MS_{res()} + \\frac{e_i^2}{(1-h_{ii})}.\n\\tag{10.12}\n\\end{equation}\\]Let us take look externally studentized residuals scatterplots shown Figures 10.1(b) 10.1(c). , sorting based increasing order based absolute values:Notice externally studentized residuals observation 6 plots lot larger observations. now lot obvious outliers, compared using studentized residuals earlier .Generally speaking, \\(|t_i| > 3\\) decent rule use flag outliers. rule used relation \\(t_i\\)s.covered measures detect high leverage observations outliers. observations usually something interesting make “stand ” observations. necessarily influential regression setting.","code":"\n##from Fig 1b\nsort(abs(rstudent(result.out)))##          1          2          5          3          4          6 \n##  0.1859371  0.2795018  0.4720327  0.5417717  0.7585583 14.0687652\n##from Fig 1c\nsort(abs(rstudent(result.both)))##         2         3         1         5         4         6 \n## 0.1023782 0.1406084 0.2896319 1.1935239 3.7138867 6.9686966"},{"path":"out.html","id":"influential-observations","chapter":"10 Analysis of Residuals in MLR","heading":"10.4 Influential Observations","text":"regression setting, observation influential presence, removal, drastically alters regression analysis. usually quantify alteration terms much predicted response / estimated coefficients change without observation question. Generally speaking, observations high leverage outlying potential influential. example Figure 10.1, see estimated regression line drastically altered Figure 10.1(c).","code":""},{"path":"out.html","id":"dfbetas","chapter":"10 Analysis of Residuals in MLR","heading":"10.4.1 DFBETAs","text":"can look estimated coefficients change observation question removed estimating model. motivation behind DFBETAs:\\[\\begin{equation}\nDFBETAS_{j,} = \\frac{\\hat{\\beta}_j-\\hat{\\beta}_{j()}}\n{\\sqrt{MS_{res()}c_{jj}}}\n\\tag{10.13}\n\\end{equation}\\]\\(c_{jj}\\) \\(j\\)th diagonal element \n\\(\\left(\\boldsymbol{X^{\\prime}X}\\right)^{-1}\\), since \\(Var(\\boldsymbol{\\hat{\\beta}}) = \\sigma^2 \\left(\\boldsymbol{X^{\\prime}X}\\right)^{-1}\\).Notice two subscripts \\(DFBETA_{j,}\\). first subscript denotes coefficient, second denotes observation. numerator (10.13) measures change estimated coefficient, standardize change dividing standard error.things note (10.13):sign \\(DFBETAS_{j,}\\) indicates whether excluding observation leads increase decrease estimated regression coefficient.\\(DFBETAS\\) can interpreted number standard errors estimated coefficient changes observation \\(\\) removed building model.Suggested rule influential observation: \\(|(\\mbox{DFBETAS})_{j,}|>\\frac{2}{\\sqrt{n}}\\). , used guide, conjunction values \\(DFBETAS\\) coefficients observations.Let us go back scatterplot Fig 10.1(c) compute \\(DFBETAs\\):Notice information presented \\(6 \\times 2\\) matrix. dimension always \\(n \\times p\\), row index corresponds observation, column index corresponds coefficient. value row 6 column 1 informs us removing observation 6 estimating model change \\(\\hat{\\beta_0}\\) 13 standard errors, change \\(\\hat{\\beta_1}\\) 28 standard errors. comparison \\(DFBETAS\\) observations, values huge, flag observation 6 influential.Checking suggested rule influential observations:clearly observation 6 influential, since \\(DFBETAs\\) much larger 0.8165.","code":"\ndfbetas(result.both)##   (Intercept)            x\n## 1 -0.15341916   0.08625257\n## 2 -0.04957068   0.02491157\n## 3  0.05689633  -0.02049090\n## 4  1.05250538   0.03665073\n## 5 -0.66636733   0.39576044\n## 6 13.00009691 -28.26234465\n##criteria for DFBETAs\n2/sqrt(n)## [1] 0.8164966"},{"path":"out.html","id":"dffits","chapter":"10 Analysis of Residuals in MLR","heading":"10.4.2 DFFITs","text":"Another measure influential observations based change predicted response model estimated without observation question. One measure \\(DFFITs\\), defined \\[\\begin{eqnarray}\nDFFITS_i &=& \\frac{\\hat{y}_i-\\hat{y}_{()}}{\\sqrt{S^2_{()}h_{ii}}} \\tag{10.14} \\\\\n                 &=& t_i\\left(\\frac{h_{ii}}{1-h_{ii}}\\right)^{1/2}. \\tag{10.15}\n\\end{eqnarray}\\]notes \\(DFFITs\\):Using (10.14), \\(DFFITs\\) can interpreted number standard errors \\(\\hat{y}_i\\) changes observation \\(\\) removed estimating model.\\(DFFITs\\) measures influence observation \\(\\) fitted value.guide, \\(\\left|DFFITS_i\\right| > 2\\sqrt{p/n}\\) considered influential. , used conjunction \\(DFFITs\\) observations.Using (10.15), can see observations high leverage outlying likely high values \\(DFFITs\\).Let us go back scatterplot Fig 10.1(c) compute \\(DFFITs\\):\\(DFFITs\\) observation 6 around -31, lot larger magnitude compared \\(DFFITs\\). can safely say influential, since presence removal changes predicted response 31 standard errors.Checking suggested rule influential observations:magnitude \\(DFFITS_6\\) clearly larger criteria, influential based criteria. Interestingly, criteria also flag observation 4 influential, even though visually appear , based Fig 10.1(c).","code":"\ndffits(result.both)##            1            2            3            4            5            6 \n##  -0.16032698  -0.05330067   0.06676822   1.66138579  -0.68764196 -31.11630918\n##criteria for DFFITs\n2*sqrt(p/n)## [1] 1.154701"},{"path":"out.html","id":"cooks-distance","chapter":"10 Analysis of Residuals in MLR","heading":"10.4.3 Cook’s distance","text":"Another measure motivated change fitted values Cook’s Distance\\[\\begin{eqnarray}\nD_i &=& \\frac{\\left( \\boldsymbol{\\hat{y}} - \\boldsymbol{\\hat{y}_{()}} \\right)^{\\prime} \\left( \\boldsymbol{\\hat{y}} - \\boldsymbol{\\hat{y}_{()}} \\right)}{p MS_{res}} \\tag{10.16} \\\\\n    &=& \\frac{r_i^2}{p}\\frac{h_{ii}}{1-h_{ii}}. \\tag{10.17}\n\\end{eqnarray}\\]\\(\\boldsymbol{\\hat{y}_{()}}\\) denotes vector fitted values observation \\(\\) removed. comments Cook’s distance:(10.16), can see numerator Cook’s distance measures Squared Euclidean distance vector fitted values observations, \\(\\boldsymbol{\\hat{y}}\\), vector fitted values observation \\(\\) removed, \\(\\boldsymbol{\\hat{y}_{()}}\\). measures fitted values observations change, observation \\(\\) removed. unlike \\(DFFITs\\), measures change fitted value observation \\(\\).(10.16), can see numerator Cook’s distance measures Squared Euclidean distance vector fitted values observations, \\(\\boldsymbol{\\hat{y}}\\), vector fitted values observation \\(\\) removed, \\(\\boldsymbol{\\hat{y}_{()}}\\). measures fitted values observations change, observation \\(\\) removed. unlike \\(DFFITs\\), measures change fitted value observation \\(\\).(10.17), can see observations high leverage outlying likely large Cook’s distances.(10.17), can see observations high leverage outlying likely large Cook’s distances.suggested guideline observations Cook’s distance larger 1 flagged influential.suggested guideline observations Cook’s distance larger 1 flagged influential.Let us go back scatterplot Fig 10.1(c) compute Cook’s distances:Cook’s distance observation 6 37.5552, lot larger 1, lot larger Cook’s distance observations. flag influential observation.","code":"\ncooks.distance(result.both)##            1            2            3            4            5            6 \n##  0.016670353  0.001887380  0.002952539  0.328733436  0.213742357 37.555213339"},{"path":"out.html","id":"some-comments-about-flagged-observations","chapter":"10 Analysis of Residuals in MLR","heading":"10.4.4 Some comments about flagged observations","text":"tendency remove observations flagged using measures module. EXTREMELY CAREFUL deleting observations. Many times, observations provide interesting case studies always identified discussed. Something making different observations.comments:get caught guidelines flagging observations. Use guidelines relation values observations well. Also use context guide .clear observations data entry errors, fix .observations represent unusual circumstances meet objectives study, something fundamentally wrong observation, may delete , make sure clearly explain . example, may measuring weights newborn babies, 20 week old baby’s weight entered study. Clearly, 20 week old part study, can remove baby’s weight.Sometimes, observations flagged due predictor response variable lot larger rest. Log transforming corresponding variable can help make value lot closer rest.Fit model without influential observations see differently models answer questions interest.always address removal observations report. Also provide justifications removal.","code":""},{"path":"out.html","id":"partial-regression-plots","chapter":"10 Analysis of Residuals in MLR","heading":"10.5 Partial Regression Plots","text":"Another use residuals use inform us need transform predictor variables MLR.SLR, learned use scatterplots residual plots assess various regression assumptions, transform predictor response variable needed. MLR, still use residual plots way assess couple regression assumptions:errors mean 0.errors constant variance.challenge MLR assumption 1 violated, know can remedy transforming least one predictors, use scatterplots decide predictor transform, transform. main reason scatterplots take account one predictors ignoring predictors, whereas MLR fits predictors model time.partial regression plot illustrates marginal effect adding predictor others already model. can use partial regression plots decide predictor added MLR model, needed, transform predictor. Note number commonly used names partial regression plots added variable plots marginal effects plots.Partial regression plots created following manner. Suppose predictors \\(x_1, x_2, \\cdots, x_{k-1}\\) model, want decide need add, drop, transform predictor \\(x_k\\):regress \\(y\\) \\(x_1, x_2, \\cdots, x_{k-1}\\) obtain residuals. Denote residuals \\(e(y|x_1, x_2, \\cdots, x_{k-1})\\).regress predictor question, \\(x_k\\), predictors model obtain residuals. Denote residuals \\(e(x_k|x_1, x_2, \\cdots, x_{k-1})\\)., plot \\(e(x_k|x_1, x_2, \\cdots, x_{k-1})\\) \\(e(y|x_1, x_2, \\cdots, x_{k-1})\\). plot partial regression plot \\(x_k\\).Usually, assess partial regression plot following patterns: (1) Random horizontal band (pattern); (2) Linear pattern; (3) Nonlinear pattern.see random horizontal band, means can drop \\(x_k\\) model.see linear pattern, keep \\(x_k\\) model without transformation.see nonlinear pattern, need \\(x_k\\) predictor, needs transformed. use shape partial regression plot aid us transforming predictor.properties partial regression plots:estimated intercept 0.estimated slope estimated coefficient \\(x_k\\) model \\(x_1, x_2, \\cdots, x_{k}\\) predictors.Let us take look example based simulated data. simplicity, response variable two predictors, \\(x_1, x_2\\). consider MLR model , create corresponding diagnostic plots:Looking residual plot top left, assumption variance errors constant reasonably met, vertical spread residuals similar throughout plot. Assumption 1, errors mean 0, met. can see curved pattern residuals evenly scattered across horizontal axis. small large values horizontal axis, residuals almost exclusively negative mean 0. moderate values horizontal axis, residuals almost exclusively positive mean 0.know need transform least one predictor, since assumption 1 met. However, residual plot inform us predictor transform, . create partial regression plots:first plot left partial regression plot \\(x_1\\), second plot right partial regression plot \\(x_2\\).Based partial regression plot \\(x_1\\), see clear linear pattern, plots evenly scattered across blue line. \\(x_1\\) need transformed.Based partial regression plot \\(x_2\\), see non linear pattern. values horizontal axis small large, plots blue line; however, values horizontal axis moderate, plots blue line. shape pattern resembles logarithmic function, log transform \\(x_2\\), refit regression, reassess residual plot.residual plot now looks lot reasonable. residuals evenly scattered across horizontal axis constant vertical spread, assumptions 1 2 met. transformation \\(x_2\\) successful.curiosity, can look resulting partial regression plots:surprising see partial regression plots, plots evenly scattered across blue lines. fact, slopes blue lines value corresponding estimated coefficient:estimated coefficients \\(x_1\\) \\(x_2\\) MLR 3 10 respectively, match see corresponding partial regression plots.","code":"\n##create dataframe\nData<-data.frame(x1,x2,y)\n\n##MLR\nresult<-lm(y~x1+x2, data=Data)\n\n##residual plot\npar(mfrow=c(2,2))\nplot(result)\nlibrary(car)\ncar::avPlots(result)\n##need to transform x2\nx2star<-log(x2)\nData<-data.frame(Data,x2star)\n\n##regress using x2star\nresult2<-lm(y~x1+x2star, data=Data)\n\n##diagnostic plots\npar(mfrow=c(2,2))\nplot(result2)\n##partial regression plots\ncar::avPlots(result2)\nresult2## \n## Call:\n## lm(formula = y ~ x1 + x2star, data = Data)\n## \n## Coefficients:\n## (Intercept)           x1       x2star  \n##       5.898        3.014       10.072"},{"path":"out.html","id":"r-tutorial-7","chapter":"10 Analysis of Residuals in MLR","heading":"10.6 R Tutorial","text":"tutorial, go data set mid 1980s contains data mean teacher pay, mean spending public schools, state.want assess teacher pay (PAY) related spending public schools (SPEND), controlling geographic region (AREA). geographic regions North (coded 1), South (coded 2), West (coded 3).Load various packages need visualizations assessing regression assumptions. Also download data file, teacher_pay.txt:Notice AREA recorded integers, need convert factor, give descriptive names regions.create visualizations see can spot potential outliers, high leverage, / influential observations. create scatterplot two quantitative variables: PAY SPEND, different colors denote three regions. dataframe actually provides names state row, can overlay names states scatterplot via layer geom_text():Based scatterplots, note following:Within geographic region, teacher pay positive linear association spending public schools.slopes parallel, association teacher pay spending public schools varies across geographic regions.Alaska stands , teacher pay spending public schools lot higher rest states West.","code":"\nlibrary(ggplot2) ##for visuals\nlibrary(MASS) ##for boxcox\nData<-read.table(\"teacher_pay.txt\", header=TRUE, sep=\"\")\nclass(Data$AREA) ##notice its integer. Need to convert to factor## [1] \"integer\"\nData$AREA<-factor(Data$AREA) ##convert to factor\nlevels(Data$AREA)## [1] \"1\" \"2\" \"3\"\nlevels(Data$AREA) <- c(\"North\", \"South\", \"West\") ##Give names to the classes\nggplot2::ggplot(Data, aes(x=SPEND, y=PAY, color=AREA))+\n  geom_point()+\n  geom_smooth(method=lm, se=FALSE)+\n  labs(title=\"Scatterplot of Pay against Expenditure, by Area\")+\n  geom_text(label=rownames(Data))"},{"path":"out.html","id":"model-fitting-and-diagnostics","chapter":"10 Analysis of Residuals in MLR","heading":"Model fitting and Diagnostics","text":"Given noted , fit model interaction.take look residual plot assess regression assumptions:Look residuals vs leverage plot (bottom right). plot identifies Alaska Cook’s distance greater 1, DC Cook’s distance greater 0.5. based Cook’s distance one influential observation: Alaska. terribly surprising saw scatterplots clearly high leverage observation.Looking scale-location plot (bottom left), appears variance standardized residuals increasing. However, unduly influenced Alaska influential high leverage.residual plot looks fine: generally evenly scattered across horizontal axis constant vertical variation.take look Box Cox plot.Notice \\(\\lambda=1\\) slightly outside 95% CI. now, transform response variable, given , residual plot looks fine.","code":"\nresult<-lm(PAY~AREA*SPEND, data=Data)\npar(mfrow=c(2,2))\nplot(result)\nMASS::boxcox(result) "},{"path":"out.html","id":"detecting-high-leverage-observations-and-outliers","chapter":"10 Analysis of Residuals in MLR","heading":"Detecting high leverage observations and outliers","text":"calculate leverages externally studentized residuals. Leverages found extracting component called hat function lm.influence() used object created lm(). function rstudent() used obtain externally studentized residuals object created lm():compare relevant criteria.4 states high leverage. Alaska DC flagged surprising given residual vs leverage plot earlier. Let us sort leverages:Notice NJ’s leverage much higher states, even though flagged earlier. Next, use externally studentized residuals flag outliers:Michigan flagged outlier. Looking back scatterplot, notice ’s response quite bit larger estimated regression equation predict. take look externally studentized residuals sorting :couple states externally studentized lot smaller Michigan’s. time , alarmed Michigan.","code":"\nhii<-lm.influence(result)$hat ##leverages\next.student<-rstudent(result) ##ext studentized res\nn<-nrow(Data)\np<-6\nhii[hii>2*p/n]##        NY        NJ        DC        AK \n## 0.3053842 0.2581918 0.3912344 0.7487242\nsort(hii)##         KA         MN         MI         PA         IL         IA         WI \n## 0.04763314 0.04814049 0.04872688 0.05324797 0.05377504 0.05632986 0.05706558 \n##         VT         OH         NC         LA         TX         GA         VA \n## 0.05707857 0.05746450 0.05973863 0.06129159 0.06143055 0.06827941 0.06996579 \n##         ME         SC         MT         CO         NB         HA         KY \n## 0.07183791 0.07252628 0.07694962 0.07743972 0.07745833 0.07772608 0.07819689 \n##         OR         WA         CA         WV         FL         NM         OK \n## 0.07834563 0.07849313 0.08023754 0.08125084 0.08156618 0.08607902 0.08859618 \n##         MA         IN         AL         MO         RI         NH         AR \n## 0.09089505 0.09092285 0.09127547 0.09139123 0.09410454 0.09633746 0.10245416 \n##         ND         NV         SD         AZ         TE         CT         ID \n## 0.10338837 0.11028362 0.11624741 0.11760852 0.11879029 0.12437883 0.14499945 \n##         WY         MS         UT         MD         DE         NJ         NY \n## 0.15610645 0.16134343 0.16700704 0.18479604 0.22726386 0.25819177 0.30538424 \n##         DC         AK \n## 0.39123441 0.74872418\next.student[abs(ext.student)>3]##       MI \n## 3.019843\nsort(abs(ext.student))##         MD         SC         HA         NY         MO         MA         KY \n## 0.03644196 0.05262498 0.06210089 0.06476159 0.08128973 0.10835326 0.10929606 \n##         GA         WV         NC         PA         WA         MS         CO \n## 0.17250321 0.19257076 0.21152962 0.21756552 0.22063591 0.24283511 0.28475162 \n##         ND         OK         AR         VA         WI         OR         UT \n## 0.29501349 0.29572399 0.36126156 0.36637531 0.39488723 0.44073911 0.45678081 \n##         ID         OH         NB         CT         NH         IA         TX \n## 0.49053845 0.59490670 0.59806955 0.60434181 0.63829421 0.70327885 0.73994193 \n##         AZ         KA         LA         TE         NM         AL         RI \n## 0.78792273 0.82077814 0.82468153 0.89762127 0.91539379 1.04254320 1.08246221 \n##         NV         IN         FL         MN         VT         ME         SD \n## 1.10165201 1.14073928 1.15171582 1.20035013 1.31410121 1.32914423 1.43863273 \n##         NJ         WY         DE         IL         MT         CA         AK \n## 1.56668720 1.66500360 1.69610186 1.72424711 1.76080510 1.82999671 2.07828421 \n##         DC         MI \n## 2.36884012 3.01984305"},{"path":"out.html","id":"detecting-influential-observations","chapter":"10 Analysis of Residuals in MLR","heading":"Detecting influential observations","text":"","code":""},{"path":"out.html","id":"dfbetas-1","chapter":"10 Analysis of Residuals in MLR","heading":"DFBETAS","text":"Next, identify influential observations based DFBETAS, via debetas() function:lot output look , since computing DFBETAS coefficient observation.see :NJ influential coefficients.influential \\(\\hat{\\beta}_0\\), intercept.SD influential coefficients except \\(\\hat{\\beta}_4\\).DE, DC influential \\(\\hat{\\beta}_1\\) \\(\\hat{\\beta}_4\\).WY influential \\(\\hat{\\beta}_5\\).AK influential \\(\\hat{\\beta}_2\\) \\(\\hat{\\beta}_5\\).may useful actually look actual values DFBETAS states.Recall DFBETAS measure number standard errors estimated coefficients change removal observations., can use subject matter expertise decide change estimated coefficient considered practically significant alter criteria DFBETAS accordingly. decision made looking data.","code":"\nDFBETAS<-dfbetas(result)\nabs(DFBETAS)>2/sqrt(n)##    (Intercept) AREASouth AREAWest SPEND AREASouth:SPEND AREAWest:SPEND\n## ME       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## NH       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## VT       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## MA       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## RI       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## CT       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## NY       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## NJ        TRUE      TRUE     TRUE  TRUE            TRUE           TRUE\n## PA       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## OH       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## IN        TRUE     FALSE    FALSE FALSE           FALSE          FALSE\n## IL       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## MI       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## WI       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## MN       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## IA       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## MO       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## ND       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## SD        TRUE      TRUE     TRUE  TRUE           FALSE           TRUE\n## NB       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## KA       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## DE       FALSE      TRUE    FALSE FALSE            TRUE          FALSE\n## MD       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## DC       FALSE      TRUE    FALSE FALSE            TRUE          FALSE\n## VA       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## WV       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## NC       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## SC       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## GA       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## FL       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## KY       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## TE       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## AL       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## MS       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## AR       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## LA       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## OK       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## TX       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## MT       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## ID       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## WY       FALSE     FALSE    FALSE FALSE           FALSE           TRUE\n## CO       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## NM       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## AZ       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## UT       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## NV       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## WA       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## OR       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## CA       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n## AK       FALSE     FALSE     TRUE FALSE           FALSE           TRUE\n## HA       FALSE     FALSE    FALSE FALSE           FALSE          FALSE\n##see actual values for DFBETAS of these states\nDFBETAS[\"NJ\",]##     (Intercept)       AREASouth        AREAWest           SPEND AREASouth:SPEND \n##       0.7409886      -0.5257624      -0.6082822      -0.8347137       0.5404329 \n##  AREAWest:SPEND \n##       0.6968517\nDFBETAS[\"IN\",]##     (Intercept)       AREASouth        AREAWest           SPEND AREASouth:SPEND \n##       0.2952144      -0.2094670      -0.2423433      -0.2489711       0.1611956 \n##  AREAWest:SPEND \n##       0.2078509\nDFBETAS[\"SD\",]##     (Intercept)       AREASouth        AREAWest           SPEND AREASouth:SPEND \n##      -0.4584579       0.3252951       0.3763509       0.4009003      -0.2595617 \n##  AREAWest:SPEND \n##      -0.3346873\nDFBETAS[\"DE\",]##     (Intercept)       AREASouth        AREAWest           SPEND AREASouth:SPEND \n##    1.128835e-15    4.723272e-01   -6.863935e-16   -8.545510e-16   -6.035000e-01 \n##  AREAWest:SPEND \n##    6.560284e-16\nDFBETAS[\"DC\",]##     (Intercept)       AREASouth        AREAWest           SPEND AREASouth:SPEND \n##   -2.452921e-15   -1.090038e+00    1.455494e-15    1.848907e-15    1.334032e+00 \n##  AREAWest:SPEND \n##   -1.358816e-15\nDFBETAS[\"WY\",]##     (Intercept)       AREASouth        AREAWest           SPEND AREASouth:SPEND \n##    2.021103e-16   -1.615063e-16    1.694842e-01   -1.547569e-16    1.058109e-16 \n##  AREAWest:SPEND \n##   -2.807636e-01\nDFBETAS[\"AK\",]##     (Intercept)       AREASouth        AREAWest           SPEND AREASouth:SPEND \n##   -7.309710e-16    6.954223e-16   -1.577954e+00    3.368755e-16   -3.872654e-16 \n##  AREAWest:SPEND \n##    1.870692e+00"},{"path":"out.html","id":"dffits-1","chapter":"10 Analysis of Residuals in MLR","heading":"DFFITS","text":"Next, identify influential observations based DFFITS, via dffits() function:NJ, DE, DC, WY, AK influential since DFFITS greater \\(2\\sqrt{\\frac{p}{n}}\\). DFFITS AK DC lot higher rest. can also perform calculations see different predicted response changes without observations.Alaska DC flagged surprising since flagged earlier residuals vs leverage plot large Cook’s distances.predicted mean teacher pay Alaska changes $6,800 Alaska removed model. predicted mean teacher pay Wyoming changes $630 Wyoming removed model. may wish consult subject matter expert change $630 practical significance context data. can use subject matter expertise decide change predicted pay considered practically significant alter criteria DFFITS accordingly. decision made looking data.","code":"\nDFFITS<-dffits(result)\nDFFITS[abs(DFFITS)>2*sqrt(p/n)]##         NJ         DE         DC         WY         AK \n## -0.9242888 -0.9198172  1.8990186 -0.7161133  3.5874885\nsort(abs(DFFITS))##         SC         MD         HA         MO         KY         MA         NY \n## 0.01471597 0.01735062 0.01802815 0.02578098 0.03183319 0.03426140 0.04294064 \n##         GA         PA         NC         WV         WA         CO         OK \n## 0.04669810 0.05159688 0.05331808 0.05726718 0.06439360 0.08249938 0.09220164 \n##         WI         ND         VA         MS         AR         OR         OH \n## 0.09714477 0.10017873 0.10048925 0.10651115 0.12205575 0.12850053 0.14689256 \n##         IA         NB         KA         TX         ID         UT         NH \n## 0.17182516 0.17329768 0.18356009 0.18930262 0.20201008 0.20452885 0.20840847 \n##         LA         CT         MN         NM         AZ         VT         TE \n## 0.21072756 0.22777064 0.26994593 0.28093270 0.28765528 0.32331629 0.32956719 \n##         AL         FL         RI         IN         ME         NV         IL \n## 0.33041136 0.34322306 0.34888234 0.36076345 0.36977458 0.38785924 0.41104811 \n##         MT         SD         CA         MI         WY         DE         NJ \n## 0.50839566 0.52176655 0.54050695 0.68346464 0.71611334 0.91981717 0.92428875 \n##         DC         AK \n## 1.89901862 3.58748845\n##compute yhat and yhat(i) for the states found above\ny<-Data$PAY\nyhat<-y-result$res \ndel.res<-result$res/(1-hii) ##deleted residual\nyhat.i<-y-del.res ##yhat(i)\n##compare yhat, yhat(i), and compute their difference for the states found above\ncbind(yhat,yhat.i, yhat-yhat.i)[abs(DFFITS)>2*sqrt(p/n),]##        yhat   yhat.i           \n## NJ 30188.73 31239.42 -1050.6910\n## DE 27944.46 28921.02  -976.5579\n## DC 29988.89 27417.51  2571.3848\n## WY 30634.16 31264.99  -630.8240\n## AK 39194.77 32385.49  6809.2826"},{"path":"out.html","id":"cooks-distance-1","chapter":"10 Analysis of Residuals in MLR","heading":"Cook’s distance","text":"Let’s look Cook’s distance, using cooks.distance() function:Alaska flagged, surprising given earlier observations residuals vs leverage plot, Alaska observation Cook’s distance greater 1. Cook’s distance Alaska lot higher others.","code":"\nCOOKS<-cooks.distance(result)\nCOOKS[COOKS>1]##       AK \n## 1.997662\nsort(COOKS)##           SC           MD           HA           MO           KY           MA \n## 3.691126e-05 5.131276e-05 5.539532e-05 1.132772e-04 1.726836e-04 2.000336e-04 \n##           NY           GA           PA           NC           WV           WA \n## 3.142709e-04 3.714612e-04 4.533030e-04 4.840790e-04 5.585400e-04 7.060148e-04 \n##           CO           OK           WI           ND           VA           MS \n## 1.158005e-03 1.446184e-03 1.602917e-03 1.707267e-03 1.716030e-03 1.931155e-03 \n##           AR           OR           OH           IA           NB           KA \n## 2.531855e-03 2.802240e-03 3.648622e-03 4.976540e-03 5.077827e-03 5.656739e-03 \n##           TX           ID           UT           NH           LA           CT \n## 6.033246e-03 6.918088e-03 7.096810e-03 7.335614e-03 7.454007e-03 8.770292e-03 \n##           MN           NM           AZ           VT           AL           TE \n## 1.202731e-02 1.320140e-02 1.390812e-02 1.714530e-02 1.816021e-02 1.818091e-02 \n##           FL           RI           IN           ME           NV           IL \n## 1.949227e-02 2.020936e-02 2.154745e-02 2.240714e-02 2.495400e-02 2.697727e-02 \n##           MT           SD           CA           MI           WY           DE \n## 4.115665e-02 4.431989e-02 4.627581e-02 6.595379e-02 8.223120e-02 1.353651e-01 \n##           NJ           DC           AK \n## 1.379268e-01 5.451778e-01 1.997662e+00"},{"path":"out.html","id":"thoughts","chapter":"10 Analysis of Residuals in MLR","heading":"Thoughts","text":"? Really depends original research questions values DFFITS DFBETAS consider practically significant, may require consultation subject matter expert.note following:Alaska influential across measures, highest values across measures.DC influential based DFFITS DFBETAS, second largest magnitudes.Personally, given know, :Remove DC. DC like city state, think makes sense compare DC states.Remove Alaska make clear model excludes Alaska, Alaska interesting characteristics makes stand western states (high expenditure high teacher pay).Refit model two removed (clearly explained).Check diagnostic plots. Residual plot looks fine. Notice observation large Cook’s distance.Box Cox plot suggests don’t need transform response variable.Let us fit model interaction predictors, conduct general linear F test see can drop interaction terms:can drop interaction terms. complete, may want reassess regression assumptions model interactions, although removing insignificant terms usually affect residual plot lot.","code":"\ndata.no.akdc<-Data[-c(50,24),] ##remove row 50 and 24, which is AK and DC\nresult.no.akdc<-lm(PAY~AREA*SPEND, data=data.no.akdc) \npar(mfrow=c(2,2))\nplot(result.no.akdc)\nMASS::boxcox(result.no.akdc)\n##no interactions\nresult.no.akdc.reduced<-lm(PAY~AREA+SPEND, data=data.no.akdc)\n##general linear F test\nanova(result.no.akdc.reduced, result.no.akdc)## Analysis of Variance Table\n## \n## Model 1: PAY ~ AREA + SPEND\n## Model 2: PAY ~ AREA * SPEND\n##   Res.Df       RSS Df Sum of Sq      F Pr(>F)\n## 1     45 204237951                           \n## 2     43 185418192  2  18819759 2.1822 0.1251"},{"path":"out.html","id":"partial-regression-plots-1","chapter":"10 Analysis of Residuals in MLR","heading":"Partial regression plots","text":"can use avPlots() function car package create partial regression plots:Partial regression plots used assess coefficients associated quantitative predictors. partial regression plot predictor SPEND plots evenly scattered sides blue line, indicating need transform . surprising given residual plot model indicate assumption errors mean 0 violated.","code":"\nlibrary(car)\ncar::avPlots(result.no.akdc.reduced)"},{"path":"logistic1.html","id":"logistic1","chapter":"11 Logistic Regression","heading":"11 Logistic Regression","text":"","code":""},{"path":"logistic1.html","id":"introduction-10","chapter":"11 Logistic Regression","heading":"11.1 Introduction","text":"point, learning linear regression, used one quantitative response variable least one predictor. binary response variable least one predictor, use logistic regression.Common ways summarizing binary variables include using probabilities odds. example, may want estimate probability college students haven driven drunk based characteristics often party often drink alcohol. may consider using linear regression model, probability driving drunk response variable. However, linear regression model may end estimated probabilities less 0 greater 1. logistic regression model set guarantee estimated probability always 0 1.Typical questions logistic regression model can answer include partying often increase odds driving drunk? characteristics gender help us better predict odds driving drunk already know often student parties? confident estimated odds?module, learn logistic regression model. Similar learned studying linear regression model, learn interpret coefficients logistic regression model, perform various inferential procedures answer various questions interests, learn assess accuracy model. study logistic regression model, helpful compare contrast learning learned linear regression model.","code":""},{"path":"logistic1.html","id":"logistic-regression","chapter":"11 Logistic Regression","heading":"11.2 Logistic Regression","text":"introduce notation terms commonly used logistic regression:\\(\\pi\\): probability “success” (belonging class coded 1 using indicator variable denote binary response variable).\\(1-\\pi\\): probability “failure” (belonging class coded 0 using indicator variable denote binary response variable).\\(\\frac{\\pi}{1-\\pi}\\): odds “success”.\\(\\log \\left( \\frac{\\pi}{1-\\pi} \\right)\\): log-odds “success”.Using driving drunk example introduction section, response variable whether student driven drunk, binary levels “yes” “”. use indicator variable denote binary response, 1 denoting “yes”, 0 denoting “”. example:\\(\\pi\\) denotes probability student driven drunk,\\(1-\\pi\\) denotes probability student driven drunk,\\(\\frac{\\pi}{1-\\pi}\\) denotes odds student driven drunk,\\(\\log \\left( \\frac{\\pi}{1-\\pi} \\right)\\) denotes log odds student driven drunk.Notice definitions, probability odds exactly . probability small, probability odds approximately .response variable binary, can longer modeled using normal distribution (assumed linear regression). now assume response variable follows Bernoulli distribution. Another assumption make observations independent .Bernoulli variable probability distribution \\(P(y_i = 1) = \\pi_i\\) \\(P(y_i = 0) = 1-\\pi_i\\).expectation \\(E(y_i) = \\pi_i\\).variance \\(Var(y_i) = \\pi_i (1-\\pi_i)\\).may tempting try fit linear regression model framework, .e.\\[\nE(y_i) = \\pi_i = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k.\n\\]\nHowever, formulation work. estimated value \\(\\hat{\\pi_i}\\) 0 1. Nothing formulation ensures .","code":""},{"path":"logistic1.html","id":"the-logistic-regression-equation","chapter":"11 Logistic Regression","heading":"11.2.1 The logistic regression equation","text":"Instead, logistic regression equation written \\[\\begin{equation}\n\\log(\\frac{\\pi}{1-\\pi})=\\beta_0+\\beta_1x_1+...+\\beta_{k} x_{k} = \\boldsymbol{X \\beta},\n\\tag{11.1}\n\\end{equation}\\]\\(\\boldsymbol{X}\\) \\(\\boldsymbol{\\beta}\\) denote design matrix vector parameters respectively. formulation (11.1) following characteristics:estimated log odds, \\(\\log \\left( \\frac{\\hat{\\pi}}{1-\\hat{\\pi}} \\right)\\), real number.estimated probability, \\(\\hat{\\pi}\\), 0 1.log odds expressed linear combination predictors.log odds transformed version mean response.transformation \\(\\log \\left( \\frac{\\pi}{1-\\pi} \\right)\\) called logit link, denoted \\(logit(\\pi)\\).Two algebraically equivalent expressions logistic regression equation (11.1) \\[\\begin{equation}\n\\text{Odds: }\\frac{\\pi}{1-\\pi}=e^{\\beta_0+\\beta_1x_1+...+\\beta_{k} x_{k}} = \\exp(\\boldsymbol{X \\beta})\n\\tag{11.2}\n\\end{equation}\\]\\[\\begin{equation}\n\\text{Probability: }\\pi=\\frac{e^{\\beta_0+\\beta_1x_1+...+\\beta_{k} x_{k}}}{1+e^{\\beta_0+\\beta_1x_1+...+\\beta_{k} x_{k}}} = \\frac{\\exp(\\boldsymbol{X \\beta})}{1+\\exp(\\boldsymbol{X \\beta})}.\n\\tag{11.3}\n\\end{equation}\\]Suppose just one predictor, \\(x\\), wish plot probability \\(x\\), (assuming \\(\\beta_1\\) positive)see positive coefficient predictor, probability increases predictor increases. However, increase linear. log odds increases linearly predictor, probability.","code":""},{"path":"logistic1.html","id":"thought-question-1","chapter":"11 Logistic Regression","heading":"11.2.1.1 Thought question","text":"probability odds related? words, given odds, can quickly calculate probability?","code":""},{"path":"logistic1.html","id":"coefficient-estimation-in-logistic-regression","chapter":"11 Logistic Regression","heading":"11.3 Coefficient Estimation in Logistic Regression","text":"Recall used method least squares estimate coefficients linear regression model. reasonable thought apply similar idea logistic regression framework, .e. minimize\\[\n\\sum_{=1}^n (y_i - \\boldsymbol{x_i^{\\prime} \\hat{\\beta}})^2\n\\]respect \\(\\boldsymbol{\\hat{\\beta}}\\), using rule fitted response \\(\\boldsymbol{x_i^{\\prime} \\hat{\\beta}}\\) rounded either 0 1.purely predictive standpoint, idea work. However, statistical inference (intervals, hypothesis tests) reliable distribution response variable now Bernoulli normal. Although linear regression model robust normality assumption, still need distribution continuous, discrete.logistic regression, coefficients estimated using different method, called method maximum likelihood.","code":""},{"path":"logistic1.html","id":"maximum-likelihood-estimation-for-logistic-regression","chapter":"11 Logistic Regression","heading":"11.3.1 Maximum likelihood estimation for logistic regression","text":"subsection give brief overview maximum likelihood estimation logistic regression.motivation behind maximum likelihood estimation model parameters estimated maximizing likelihood process described model produced observed data.Since assume response variable follows Bernoulli distribution, probability distribution observation \\[\\begin{equation}\nf_i(y_i) = \\pi_i^{y_i} (1 - \\pi_i)^{1-y_i}\n\\tag{11.4}\n\\end{equation}\\]\\(y_i\\) either 0 1. Since assume observations independent, likelihood function \\[\\begin{equation}\nL(y_1, \\cdots, y_n, \\boldsymbol{\\beta}) = \\prod_{=1}^n f_i(y_i) = \\prod_{=1}^n \\pi_i^{y_i} (1 - \\pi_i)^{1-y_i}\n\\tag{11.5}\n\\end{equation}\\]want maximize likelihood function (11.5) respect \\(\\boldsymbol{\\beta}\\). Since maximizing function maximizing log function, can instead maximize log-likelihood function\\[\\begin{equation}\n\\log L(y_1, \\cdots, y_n, \\boldsymbol{\\beta}).\n\\tag{11.6}\n\\end{equation}\\]turns logistic regression, computationally efficient maximize log-likelihood function (11.6) instead likelihood function (11.5).Using \\(\\pi_i=\\frac{\\exp(\\boldsymbol{x_i^{\\prime}\\beta})}{1 + \\exp(\\boldsymbol{x_i^{\\prime}\\beta})}\\) (11.3) algebra log-likelihood function (11.6), maximize following respect \\(\\boldsymbol{\\beta}\\)\\[\\begin{equation}\n\\log L(\\boldsymbol{y}, \\boldsymbol{\\beta}) = \\sum_{=1}^n y_i \\boldsymbol{x_i^{\\prime}\\beta} - \\sum_{=1}^n \\log [ 1 + \\exp(\\boldsymbol{x_i^{\\prime}\\beta}) ]\n\\tag{11.7}\n\\end{equation}\\]closed-form solution maximizing (11.7). numerical method called iteratively reweighted least squares used. details beyond scope class.Please view video thorough explanation maximum likelihood estimation.","code":""},{"path":"logistic1.html","id":"interpreting-coefficients-in-logistic-regression","chapter":"11 Logistic Regression","heading":"11.4 Interpreting Coefficients in Logistic Regression","text":"equivalent interpretations coefficient predictor. \\(\\beta_1\\), :one-unit increase predictor \\(x_1\\), log odds changes \\(\\beta_1\\), holding predictors constant.one-unit increase predictor \\(x_1\\), odds multiplied factor \\(\\exp(\\beta_1)\\), holding predictors constant.Let us go back driving drunk example. response variable whether student driven drunk, 1 denoting “yes” 0 denoting “”. Let us consider two predictors: \\(x_1\\): PartyNum denotes number days student parties month, average, categorical predictor Gender. estimated coefficients shown :estimated logistic regression equation \\[\n\\log \\left( \\frac{\\hat{\\pi}}{1-\\hat{\\pi}} \\right) = -1.2433 + 0.1501x_1 + 0.4136I_1,\n\\]\\(I_1\\) 1 male students 0 female students.estimated coefficient \\(x_1\\) \\(\\hat{\\beta_1} = 0.1501\\), can interpreted asThe estimated log odds driven drunk college students increases 0.1501 additional day partying, controlling gender.estimated odds driven drunk college students multiplied \\(\\exp(0.1501) = 1.1620\\) additional day partying, controlling gender.estimated coefficient \\(I_1\\) \\(\\hat{\\beta_2} = 0.4136\\), can interpreted asThe estimated log odds driven drunk college students 0.4136 higher males females, controlling number days party.estimated odds driven drunk male college students \\(\\exp(0.4136) = 1.5123\\) times odds female college students, controlling number days party.Please view video thorough explanation interpreting coefficients logistic regression.","code":"\nresult2<-glm(DrivDrnk~PartyNum+Gender, family=binomial, data=train)\nresult2## \n## Call:  glm(formula = DrivDrnk ~ PartyNum + Gender, family = binomial, \n##     data = train)\n## \n## Coefficients:\n## (Intercept)     PartyNum   Gendermale  \n##     -1.2433       0.1501       0.4136  \n## \n## Degrees of Freedom: 121 Total (i.e. Null);  119 Residual\n##   (2 observations deleted due to missingness)\n## Null Deviance:       169.1 \n## Residual Deviance: 151.9     AIC: 157.9"},{"path":"logistic1.html","id":"inference-in-logistic-regression","chapter":"11 Logistic Regression","heading":"11.5 Inference in Logistic Regression","text":"number hypothesis tests can conduct logistic regression. tests analogous versions linear regression.","code":""},{"path":"logistic1.html","id":"wald-test","chapter":"11 Logistic Regression","heading":"11.5.1 Wald test","text":"can assess whether drop single term, Wald test, basically \\(Z\\) test. can test \\(H_0: \\beta_j = 0\\) Z statistic\\[\\begin{equation}\nZ = \\frac{\\hat{\\beta}_j - 0}{se(\\hat{\\beta}_j)},\n\\tag{11.8}\n\\end{equation}\\]compared standard normal distribution. standard normal distribution denoted \\(N(0,1)\\), read normal distribution mean 0 variance 1. Let us look Wald tests earlier drink driving example:assess coefficient PartyNum, \\(H_0: \\beta_1 = 0, H_a: \\beta_1 \\neq 0\\).Test statistic \\(Z = \\frac{\\hat{\\beta}_1 - 0}{se(\\hat{\\beta}_1)} = \\frac{0.15012}{0.04216} = 3.560\\).P-value 0.00037 (using 2*(1-pnorm(abs(3.560))) R).Reject null. drop PartyNum logistic regression model.assess coefficient Gender, large p-value, means can drop Gender model leave PartyNum .","code":"\nsummary(result2)## \n## Call:\n## glm(formula = DrivDrnk ~ PartyNum + Gender, family = binomial, \n##     data = train)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.1357  -1.0211  -0.1941   1.0701   1.7302  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -1.24333    0.38795  -3.205  0.00135 ** \n## PartyNum     0.15012    0.04216   3.560  0.00037 ***\n## Gendermale   0.41363    0.40466   1.022  0.30670    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 169.13  on 121  degrees of freedom\n## Residual deviance: 151.93  on 119  degrees of freedom\n##   (2 observations deleted due to missingness)\n## AIC: 157.93\n## \n## Number of Fisher Scoring iterations: 3"},{"path":"logistic1.html","id":"pratice","chapter":"11 Logistic Regression","heading":"11.5.1.1 Pratice","text":"Verify Z statistic p-value testing \\(H_0: \\beta_2 = 0, H_a: \\beta_2 \\neq 0\\) example.","code":""},{"path":"logistic1.html","id":"confidence-intervals-for-coefficients","chapter":"11 Logistic Regression","heading":"11.5.2 Confidence intervals for coefficients","text":"\\((1-\\alpha) \\times 100\\%\\) confidence interval \\(\\beta_j\\) \\[\\begin{equation}\n\\hat{\\beta}_j \\pm Z_{1- \\alpha/2} se(\\hat{\\beta}_j).\n\\tag{11.9}\n\\end{equation}\\]Going back drink driving example, \\(95\\%\\) CI \\(\\beta_1\\) :\\[\\begin{eqnarray*}\n\\hat{\\beta_1} &\\pm& Z_{0.975} se(\\hat{\\beta}_1) \\nonumber \\\\\n= 0.15012 &\\pm& 1.96 \\times 0.04216 \\nonumber \\\\\n= (0.0675&,& 0.2328). \\nonumber\n\\end{eqnarray*}\\]Note \\(Z_{0.975}\\) found using qnorm(0.975). Based confidence interval, can say :95% confident true \\(\\beta_1\\) 0.0675 0.2328.CI excludes 0, coefficient significant. drop term model.Consistent conclusion 2-sided hypothesis test \\(\\alpha = 0.05\\).log odds driven drunk college students increases 0.0675 0.2328 additional day partying, controlling gender.odds driven drunk college students multiplied factor \\(\\exp(0.0675) = 1.0698\\) \\(\\exp(0.2328) = 1.2621\\) additional day partying, controlling gender.","code":""},{"path":"logistic1.html","id":"likelihood-ratio-tests-in-logistic-regression","chapter":"11 Logistic Regression","heading":"11.5.3 Likelihood ratio tests in logistic regression","text":"Likelihood ratio tests (LRTs) allow us compare full reduced model, denoted \\(F\\) \\(R\\) respectively. test statistic measures difference deviances models, .e.\\[\\begin{equation}\n\\Delta G^2 = D(R) - D(F),\n\\tag{11.10}\n\\end{equation}\\]\\(D(R)\\) \\(D(F)\\) denote deviance reduced model deviance full model respectively. deviance model analogous \\(SS_{res}\\) linear regression model.test statistic \\(\\Delta G^2\\) compared chi-squared distribution, denoted \\(\\chi^2_{df}\\), df denotes number parameters dropping get reduced model.deviance model labeled residual deviance R. null deviance R deviance intercept-model. Going back drink driving example, predictors PartyNum Gender, residual deviance 151.93, null deviance 169.13. can compare model intercept-model.\\(H_0: \\beta_1 = \\beta_2 = 0, H_a: \\text{ least one coefficient null nonzero.}\\)\\(\\Delta G^2 = D(R) - D(F) = 169.13 - 151.93 = 17.2\\).P-value 1-pchisq(17.2,2) close 0.Critical value qchisq(0.95,2) 5.9915.reject null hypothesis support two predictor intercept-model.","code":""},{"path":"logistic1.html","id":"r-tutorial-8","chapter":"11 Logistic Regression","heading":"11.6 R Tutorial","text":"tutorial, learn fit (binary) logistic regression model R. Logistic regression used response variable binary. model log odds “success” linear combination coefficients predictors:\\[\n\\log(\\frac{\\pi}{1-\\pi}) = \\beta_0 + \\beta_1 x_1 + \\cdots \\beta_{k} x_{k}.\n\\]\ndataset, students.txt, contains information 250 college students large public university study party habits. variables :Gender: gender studentSmoke: whether student smokesMarijuan: whether student uses marijuanaDrivDrnk: whether student driven drunkGPA: student’s GPAPartyNum: number times student parties monthDaysBeer: number days student drinks least 2 beers monthStudyHrs: number hours students studies weekSuppose want relate likelihood student driving drunk variables. Notice response variable, DrivDrnk binary variable yes levels. response variable binary quantitative, use logistic regression instead linear regression.Let us read data :going perform basic data wrangling dataframe:Remove first column, just index.Keep observations missing values variable. dozen observations missing values least one variable.Apply factor() categorical variables. reminder, done categorical variables want change reference class.going split dataset equal sets: one training set, another test set. Recall training set used build model, test set used assess model performs new observations. use set.seed() function can replicate split time block code run. integer needs supplied function.","code":"\nlibrary(tidyverse)\nData<-read.table(\"students.txt\", header=T, sep=\"\")\n##first column is index, remove it\nData<-Data[,-1]\n##some NAs in data. Remove them\nData<-Data[complete.cases(Data),]\n\n##convert categorical to factors. needed for contrasts\nData$Gender<-factor(Data$Gender)\nData$Smoke<-factor(Data$Smoke)\nData$Marijuan<-factor(Data$Marijuan)\nData$DrivDrnk<-factor(Data$DrivDrnk)\n##set seed so results (split) are reproducible\nset.seed(6021)\n\n##evenly split data into train and test sets\nsample.data<-sample.int(nrow(Data), floor(.50*nrow(Data)), replace = F)\ntrain<-Data[sample.data, ]\ntest<-Data[-sample.data, ]"},{"path":"logistic1.html","id":"visualizations-with-logistic-regression","chapter":"11 Logistic Regression","heading":"Visualizations with Logistic Regression","text":"Given response variable, DrivDrnk, categorical, use slightly different visualizations linear regression.","code":""},{"path":"logistic1.html","id":"barcharts","chapter":"11 Logistic Regression","heading":"Barcharts","text":"Barcharts useful visualize categorical predictors may related categorical response variable. Since three categorical predictors, Gender, Smoke, Marijuan, creating three barcharts:Instead displaying barcharts individually, can display simultaneously, using grid.arrange() function gridExtra package. display 3 barcharts 2 2 matrix:can see slightly higher proportion male students driven drunk, compared female students.much higher proportion smokers driven drunk, compared non smokers.Similarly, higher proportion students use marijuana driven drunk, compared non users.categorical predictors relationship whether student driven drunk.","code":"\nchart1<-ggplot2::ggplot(train, aes(x=Gender, fill=DrivDrnk))+\n  geom_bar(position = \"fill\")+\n  labs(x=\"Gender\", y=\"Proportion\",\n       title=\"Proportion of Driven Drunk by Gender\")\n\nchart2<-ggplot2::ggplot(train, aes(x=Smoke, fill=DrivDrnk))+\n  geom_bar(position = \"fill\")+\n  labs(x=\"Smokes?\", y=\"Proportion\",\n       title=\"Proportion of Driven Drunk by Smoking Status\")\n\nchart3<-ggplot2::ggplot(train, aes(x=Marijuan, fill=DrivDrnk))+\n  geom_bar(position = \"fill\")+\n  labs(x=\"Use Marijuana?\", y=\"Proportion\",\n       title=\"Proportion of Driven Drunk by Marijuana Status\")\n##put barcharts in a matrix\nlibrary(gridExtra)\ngridExtra::grid.arrange(chart1, chart2, chart3, ncol = 2, nrow = 2)"},{"path":"logistic1.html","id":"two-way-tables-1","chapter":"11 Logistic Regression","heading":"Two way tables","text":"can create two way tables summarize relationship categorical predictor whether students driven drunk:can display tables via proportions instead counts:55% male students driven drunk, compared 40% female students.88% smokers driven drunk, compared 35% non smokers.68% marijuana users driven drunk, compared 27% non users.earlier barcharts visualizations tables.","code":"\n##two way tables of counts\ntable(train$Gender, train$DrivDrnk)##         \n##          No Yes\n##   female 40  27\n##   male   23  28\ntable(train$Smoke, train$DrivDrnk)##      \n##       No Yes\n##   No  60  32\n##   Yes  3  23\ntable(train$Marijuan, train$DrivDrnk)##      \n##       No Yes\n##   No  45  17\n##   Yes 18  38\n##two way tables using proportions\nprop.table(table(train$Gender, train$DrivDrnk),1)##         \n##                 No       Yes\n##   female 0.5970149 0.4029851\n##   male   0.4509804 0.5490196\nprop.table(table(train$Smoke, train$DrivDrnk),1)##      \n##              No       Yes\n##   No  0.6521739 0.3478261\n##   Yes 0.1153846 0.8846154\nprop.table(table(train$Marijuan, train$DrivDrnk),1)##      \n##              No       Yes\n##   No  0.7258065 0.2741935\n##   Yes 0.3214286 0.6785714"},{"path":"logistic1.html","id":"quantitative-predictors","chapter":"11 Logistic Regression","heading":"Quantitative predictors","text":"see quantitative predictors, GPA, PartyNum, DaysBeer, StudyHrs may differ driven drunk , can compare distributions quantitative variables two groups:Among driven drunk, see higher proportion students lower GPAs (3.0). driven drunk, higher proportion students higher GPAs (3.0).higher proportion students driven drunk party less 10 days month, compared students driven drunk.Similarly, higher proportion students driven drunk spend less 10 days drinking beer, compared students driven drunk.much high proportion students driven drunk spend less 15 hours studying week, compared students drive drunk.quantitative predictors may related whether students driven drunk. looks like lower GPAs, partying , drinking , studying less associated increased likelihood driven drunk.","code":"\ndp1<-ggplot2::ggplot(train,aes(x=GPA, color=DrivDrnk))+\n  geom_density()+\n  labs(title=\"Density Plot of GPA by Driven Drunk\")\n\ndp2<-ggplot2::ggplot(train,aes(x=PartyNum, color=DrivDrnk))+\n  geom_density()+\n  labs(title=\"Density Plot of Number of Party Days by Driven Drunk\")\n\ndp3<-ggplot2::ggplot(train,aes(x=DaysBeer, color=DrivDrnk))+\n  geom_density()+\n  labs(title=\"Density Plot of Number of Days drank Beer by Driven Drunk\")\n\ndp4<-ggplot2::ggplot(train,aes(x=StudyHrs, color=DrivDrnk))+\n  geom_density()+\n  labs(title=\"Density Plot of Study Hours by Driven Drunk\")\n\ngridExtra::grid.arrange(dp1, dp2, dp3, dp4, ncol = 2, nrow = 2)"},{"path":"logistic1.html","id":"correlations-between-quantitative-predictors","chapter":"11 Logistic Regression","heading":"Correlations between quantitative predictors","text":"can quickly check correlations quantitative predictors:Notice DaysBeer PartyNum highly correlated. Probably surprising since drinking probably done parties, student parties likely drink .","code":"\n##correlations between quantitative predictors\nround(cor(train[,5:8]),3)##             GPA PartyNum DaysBeer StudyHrs\n## GPA       1.000   -0.259   -0.361    0.207\n## PartyNum -0.259    1.000    0.773   -0.014\n## DaysBeer -0.361    0.773    1.000   -0.186\n## StudyHrs  0.207   -0.014   -0.186    1.000"},{"path":"logistic1.html","id":"fit-logistic-regression","chapter":"11 Logistic Regression","heading":"Fit Logistic Regression","text":"Based visualizations, suspect predictors may influence response variable. use glm() function fit logistic regression:Notice specify argument family = \"binomial\". specified logistic regression. specified, linear regression fitted instead. function glm() uses maximum likelihood estimation whereas lm() uses ordinary least squares. can look Wald tests deviance model using summary():Notice predictors highly insignificant coefficients: Gender, GPA, PartyNum, StudyHrs. PartyNum insignificant surprising since noted highly correlated DaysBeer. can also assess variance inflation factors (VIFs) logistic regression like :surprisingly, evidence multicollinearity.","code":"\n##fit logistic regression\nresult<-glm(DrivDrnk~., family=binomial, data=train)\nsummary(result)## \n## Call:\n## glm(formula = DrivDrnk ~ ., family = binomial, data = train)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.4529  -0.7418  -0.4578   0.6672   2.0707  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -1.93400    1.94169  -0.996 0.319231    \n## Gendermale   0.20055    0.47903   0.419 0.675462    \n## SmokeYes     2.36384    0.71312   3.315 0.000917 ***\n## MarijuanYes  0.94538    0.49745   1.900 0.057374 .  \n## GPA          0.08094    0.55353   0.146 0.883751    \n## PartyNum    -0.02539    0.07337  -0.346 0.729282    \n## DaysBeer     0.13664    0.06921   1.974 0.048346 *  \n## StudyHrs    -0.02221    0.03065  -0.724 0.468764    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 163.04  on 117  degrees of freedom\n## Residual deviance: 114.52  on 110  degrees of freedom\n## AIC: 130.52\n## \n## Number of Fisher Scoring iterations: 5\nlibrary(faraway)\nfaraway::vif(result)##  Gendermale    SmokeYes MarijuanYes         GPA    PartyNum    DaysBeer \n##    6.644806   10.308653    7.281197    8.420435   20.957260   23.110089 \n##    StudyHrs \n##    9.156448"},{"path":"logistic1.html","id":"likelihood-ratio-tests","chapter":"11 Logistic Regression","heading":"Likelihood ratio tests","text":"Let us see can drop Gender, GPA, PartyNum, StudyHrs model, via likelihood ratio test (LRT). \\(\\Delta G^2\\) test statistic found finding difference deviances two models. deviance can found extracting component deviance object created glm():null hypothesis supports dropping 4 predictors, alternative hypothesis supports dropping 4 predictors.\\(\\Delta G^2\\) test statistic 1.008, compared \\(\\chi^2_4\\) distribution, .e. chi-squared distribution 4 degrees freedom. degrees freedom equal number terms dropping.p-value 0.9086, critical value us 9.4877.fail reject null hypothesis. Data support using full model predictors, drop Gender, GPA, PartyNum, StudyHrs use reduced model.Let us take look estimated coefficients model:logistic regression equation \\[\n\\log \\left( \\frac{\\hat{\\pi}}{1-\\hat{\\pi}}   \\right) = -2.0246 + 2.3938I_1 + 0.9584I_2 + 0.1248 DaysBeer,\n\\]\n\\(I_1 = 1\\) student smokes, \\(I_2 = 1\\) student uses marijuana.Given coefficients positive, smoking, using marijuana, drinking days associated higher likelihood driven drunk.odds driving drunk smokers \\(\\exp(2.3938) = 10.955\\) times odds non smokers, controlling marijuana use days drinking.odds driving drunk marijuana users \\(\\exp(0.9584) = 2.6075\\) times odds non users, controlling smoking days drinking.odds driving drunk multiplied factor \\(\\exp(0.1248) = 1.1329\\) additional day drinking, controlling smoking marijuana use.","code":"\nreduced<-glm(DrivDrnk~Smoke+Marijuan+DaysBeer, family=binomial, data=train)\n\n##test to compare reduced and full model\n##test stat\nTS<-reduced$deviance-result$deviance\nTS## [1] 1.007939\n##pvalue\n1-pchisq(TS,4)## [1] 0.9085899\n##critical value\nqchisq(1-0.05,4)## [1] 9.487729\nsummary(reduced)## \n## Call:\n## glm(formula = DrivDrnk ~ Smoke + Marijuan + DaysBeer, family = binomial, \n##     data = train)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.3922  -0.7793  -0.4980   0.7128   2.0730  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -2.02459    0.43398  -4.665 3.08e-06 ***\n## SmokeYes     2.39376    0.68727   3.483 0.000496 ***\n## MarijuanYes  0.95839    0.47955   1.999 0.045659 *  \n## DaysBeer     0.12480    0.04353   2.867 0.004143 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 163.04  on 117  degrees of freedom\n## Residual deviance: 115.53  on 114  degrees of freedom\n## AIC: 123.53\n## \n## Number of Fisher Scoring iterations: 5"},{"path":"logistic1.html","id":"predicted-log-odds-and-probabilities","chapter":"11 Logistic Regression","heading":"Predicted log odds and probabilities","text":"can use predict() function calculate predicted log odds test data, using reduced model:find probabilities instead, supply type=\"response\" within predict() function:observation index 2, student’s predicted log odds driving drunk -2.024593, corresponding predicted probability 0.1166449.","code":"\n##predicted log odds for test data\nlogodds<-predict(reduced,newdata=test)\nhead(logodds)##         2         3         5         6         7         9 \n## -2.024593 -1.525404  3.199523  1.951552 -2.024593  1.429744\n##predicted probabilities for test data\npreds<-predict(reduced,newdata=test, type=\"response\")\nhead(preds)##         2         3         5         6         7         9 \n## 0.1166449 0.1786671 0.9608163 0.8756158 0.1166449 0.8068613"},{"path":"logistic2.html","id":"logistic2","chapter":"12 Logistic Regression 2","heading":"12 Logistic Regression 2","text":"","code":""},{"path":"logistic2.html","id":"introduction-11","chapter":"12 Logistic Regression 2","heading":"12.1 Introduction","text":"previous module, learned logistic regression model, used binary response variable least one predictor. learned interpret model carry relevant inferential procedures answer various questions interest. module, learn assess well logistic regression model classifying test data using Receiver Operating Characteristic (ROC) curve, Area ROC Curve (AUC), confusion matrices.Going back drink driving dataset previous module, fitted logistic regression model estimate log odds probability driving drunk among college students, based number predictors. evaluate predictive ability logistic regression model data?access data, split data two portions: training data test data. use training data fit logistic regression model. use model estimate probability observations test data class. use decision rule threshold classify observations test data (example, estimated probability greater 0.5, classify response ``Yes”).found using Smoke, Marijuan, DaysBeer predictors preferred using predictors, via likelihood ratio test. fit model, use model estimate predicted probabilities test data, use threshold 0.5 classify test data.output, can read values test data.first column displays actual response.second column displays predicted probability student driven drunk based model.last column displays whether predicted probability greater threshold 0.5.Row 1 corresponds index 2 original dataframe. student’s actual response student driven drunk. Based logistic regression, student’s predicted probability driven drunk 0.1166, less threshold 0.5, student predicted driven drunk (FALSE last column). student classified correctly based logistic regression chosen threshold 0.5.However, notice row 2 output (corresponds index 3 original dataframe). student’s predicted probability 0.1787 classified driven drunk, based threshold 0.5. However, student actually driven drunk. incorrect classification., want summarize number correct incorrect classifications, based test data. done via confusion matrix.","code":"\n##fit model\nreduced<-glm(DrivDrnk~Smoke+Marijuan+DaysBeer, family=binomial, data=train)\n\n##predicted survival rate for test data based on training data\npreds<-predict(reduced,newdata=test, type=\"response\")\n\n##add predicted probabilities and classification based on threshold\ntest.new<-data.frame(test,preds,preds>0.5)\n##disply actual response, predicted prob, and classification based on threshold\nhead(test.new[,c(4,9,10)], )##   DrivDrnk     preds preds...0.5\n## 2       No 0.1166449       FALSE\n## 3      Yes 0.1786671       FALSE\n## 5      Yes 0.9608163        TRUE\n## 6       No 0.8756158        TRUE\n## 7       No 0.1166449       FALSE\n## 9      Yes 0.8068613        TRUE"},{"path":"logistic2.html","id":"confusion-matrix","chapter":"12 Logistic Regression 2","heading":"12.2 Confusion Matrix","text":"confusion matrix two two matrix (table) lists possible combinations true response classification based model threshold, shown Figure 12.1 :\nFigure 12.1: Confusion Matrix\ntable based dummy coding used binary response variable. Given response variable binary, four possible combinations:true negative (TN) observation classified 0 based logistic regression, truly 0. drink driving example, true negatives students classified driven drunk model, truly driven drunk.true negative (TN) observation classified 0 based logistic regression, truly 0. drink driving example, true negatives students classified driven drunk model, truly driven drunk.false positive (FP) observation classified 1 based logistic regression, truly 0. drink driving example, false positives students classified driven drunk model, truly driven drunk.false positive (FP) observation classified 1 based logistic regression, truly 0. drink driving example, false positives students classified driven drunk model, truly driven drunk.false negative (FN) observation classified 0 based logistic regression, truly 1. drink driving example, false negatives students classified driven drunk model, truly driven drunk.false negative (FN) observation classified 0 based logistic regression, truly 1. drink driving example, false negatives students classified driven drunk model, truly driven drunk.true positive (TP) observation classified 1 based logistic regression, truly 1. drink driving example, true positives students classified driven drunk model, truly driven drunk.true positive (TP) observation classified 1 based logistic regression, truly 1. drink driving example, true positives students classified driven drunk model, truly driven drunk.","code":""},{"path":"logistic2.html","id":"metrics-from-confusion-matrices","chapter":"12 Logistic Regression 2","heading":"12.2.1 Metrics from confusion matrices","text":"number metrics confusion matrices:Error rate: proportion incorrect classifications. Table 5.1, calculated \\(\\frac{FP + FN}{n}\\), \\(n\\) denotes sample size test data sum entries confusion matrix. Notice FP FN numbers -diagonal entries confusion matrix. probability notation, denoted \\(P(\\hat{y} \\neq y)\\).Error rate: proportion incorrect classifications. Table 5.1, calculated \\(\\frac{FP + FN}{n}\\), \\(n\\) denotes sample size test data sum entries confusion matrix. Notice FP FN numbers -diagonal entries confusion matrix. probability notation, denoted \\(P(\\hat{y} \\neq y)\\).Accuracy: proportion correct classifications. complement error rate, since accuracy error rate add 1. Table 5.1 , calculated \\(\\frac{TN + TP}{n}\\). Notice TN TP numbers diagonal entries confusion matrix. probability notation, denoted \\(P(\\hat{y} = y)\\).Accuracy: proportion correct classifications. complement error rate, since accuracy error rate add 1. Table 5.1 , calculated \\(\\frac{TN + TP}{n}\\). Notice TN TP numbers diagonal entries confusion matrix. probability notation, denoted \\(P(\\hat{y} = y)\\).False positive rate (FPR): proportion true 0s incorrectly classified 1s model. Table 5.1, calculated \\(\\frac{FP}{TN + FP}\\). probability notation, denoted \\(P(\\hat{y} = 1 | y = 0)\\).False positive rate (FPR): proportion true 0s incorrectly classified 1s model. Table 5.1, calculated \\(\\frac{FP}{TN + FP}\\). probability notation, denoted \\(P(\\hat{y} = 1 | y = 0)\\).False negative rate (FNR): proportion true 1s incorrectly classified 0s model. Table 5.1, calculated \\(\\frac{FN}{FN + TP}\\). probability notation, denoted \\(P(\\hat{y} = 0 | y = 1)\\).False negative rate (FNR): proportion true 1s incorrectly classified 0s model. Table 5.1, calculated \\(\\frac{FN}{FN + TP}\\). probability notation, denoted \\(P(\\hat{y} = 0 | y = 1)\\).Sensitivity: proportion true 1s correctly classified 1s model. Also sometimes called true positive rate (TPR). Note complement FNR, sensitivity FNR add 1. Table 5.1, calculated \\(\\frac{TP}{FN + TP}\\). probability notation, denoted \\(P(\\hat{y} = 1 | y = 1)\\).Sensitivity: proportion true 1s correctly classified 1s model. Also sometimes called true positive rate (TPR). Note complement FNR, sensitivity FNR add 1. Table 5.1, calculated \\(\\frac{TP}{FN + TP}\\). probability notation, denoted \\(P(\\hat{y} = 1 | y = 1)\\).Specificity: proportion true 0s correctly classified 0s model. Also sometimes called true negative rate (TNR). Note complement FPR, specificity FPR add 1. Table 5.1, calculated \\(\\frac{TN}{TN + FP}\\). probability notation, denoted \\(P(\\hat{y} = 0 | y = 0)\\).Specificity: proportion true 0s correctly classified 0s model. Also sometimes called true negative rate (TNR). Note complement FPR, specificity FPR add 1. Table 5.1, calculated \\(\\frac{TN}{TN + FP}\\). probability notation, denoted \\(P(\\hat{y} = 0 | y = 0)\\).Precision: proportion observations classified 1s truly 1s. Table 5.1, calculated \\(\\frac{TP}{FP + TP}\\). probability notation, denoted \\(P(y = 1 | \\hat{y} = 1)\\).Precision: proportion observations classified 1s truly 1s. Table 5.1, calculated \\(\\frac{TP}{FP + TP}\\). probability notation, denoted \\(P(y = 1 | \\hat{y} = 1)\\).Let us look confusion matrix drink driving example, using 0.5 threshold:Sample size test data \\(n = 46+12+22+39 = 119\\)Error rate \\(\\frac{12+22}{119} = 0.2857143\\)Accuracy \\(\\frac{46+39}{119} = 0.7142857\\)\\(FPR\\) \\(\\frac{12}{46+12} = 0.2068966\\)\\(FNR\\) \\(\\frac{22}{22+39} = 0.3606557\\)\\(TPR\\) \\(\\frac{39}{22+39} = 0.6393443\\)\\(TNR\\) \\(\\frac{46}{46+12} = 0.7931034\\)Precision \\(\\frac{39}{12+39} = 0.7647059\\)","code":"\n##confusion matrix with 0.5 threshold\ntable(test$DrivDrnk, preds>0.5)##      \n##       FALSE TRUE\n##   No     46   12\n##   Yes    22   39"},{"path":"logistic2.html","id":"practice-question-3","chapter":"12 Logistic Regression 2","heading":"12.2.1.1 Practice question","text":"Suppose change threshold 0.7 logistic regression. subsequent confusion matrix obtained:confusion matrix, find error rate, accuracy, FPR, FNR, TPR, TNR, precision.View associated video review practice question.","code":"\n##confusion matrix with 0.7 threshold\ntable(test$DrivDrnk, preds>0.7)##      \n##       FALSE TRUE\n##   No     50    8\n##   Yes    38   23"},{"path":"logistic2.html","id":"choice-of-threshold","chapter":"12 Logistic Regression 2","heading":"12.2.2 Choice of threshold","text":"worked practice question, may realize changing threshold changes values various metrics. Raising threshold makes difficult test observation classified 1 model. values first column confusion matrix increase, values second column decrease. Therefore raising threshold:reduces FPR,increases FNR,reduces TPR,raises TNRDepending context problem, may interested reducing one FPR FNR. Reducing one metric comes expense increasing metric.drink driving example, reducing FPR means students truly driven drunk less likely incorrectly predicted driven drunk. less likely falsely accuse innocent student driving drunk.reduction FPR comes expense increasing FNR. means students driven drunk less likely incorrectly predicted driven drunk. less likely identify students drive drunk.can see two different consequences incorrect predictions. either falsely accuse innocent students, fail intervene behaviors likely offenders. probably take consultation subject matter expert decide two errors worse, may want balance errors manner.clear neither two errors consequential , likely want focus reducing error rate. threshold 0.5 minimizes error rate, average.","code":""},{"path":"logistic2.html","id":"roc-curve-and-auc","chapter":"12 Logistic Regression 2","heading":"12.3 ROC Curve and AUC","text":"previous section, may realize confusion matrix depends threshold. may bit cumbersome create confusion matrices calculate metrics across possible thresholds. can actually perform calculations display results visually receiver operating characteristic (ROC) curve, summarize results using area curve (AUC).","code":""},{"path":"logistic2.html","id":"roc-curve","chapter":"12 Logistic Regression 2","heading":"12.3.1 ROC curve","text":"name receiver operating characteristic (ROC) derived initial use World War II analyzing radar signals. Users radar wanted distinguish signals due enemy aircraft signals due noise flock birds.ROC curve two-dimensional plot, sensitivity (TPR) y-axis \\(1 - \\text{ specificity}\\) (FPR) x-axis.ROC curve plots associated TPR FPR every possible value threshold (.e., 0 1).Let us produce ROC curve logistic regression drunk driving based three predictors Smoke, Marijuan, DaysBeer:ROC curve black curve plot, displays TPR FPR logistic regression vary threshold. Usually, diagonal line displayed well (red plot), represents model classifies random. comparing black curve red diagonal line, compare model model classifies random.model classifies random ROC curve lies diagonal. words, model classifies random \\(TPR = FPR\\).\ncommon misconception model classifies random means test observation 50-50 chance classified correctly 50-50 chance classified 1. incorrect.\nmodel classifies random can viewed model use information data make classification. definition implies \\(P(\\hat{y} = 1 | y = 1) = P(\\hat{y} = 1 | y = 0)\\), .e. \\(TPR = FPR\\). probability classifying observation 1 changed data telling model.\nmodel classifies random ROC curve lies diagonal. words, model classifies random \\(TPR = FPR\\).common misconception model classifies random means test observation 50-50 chance classified correctly 50-50 chance classified 1. incorrect.model classifies random can viewed model use information data make classification. definition implies \\(P(\\hat{y} = 1 | y = 1) = P(\\hat{y} = 1 | y = 0)\\), .e. \\(TPR = FPR\\). probability classifying observation 1 changed data telling model.model classifies observations correctly sensitivity specificity 1, belong (0,1) position (.e. top left) plot. curve diagonal closer top left plot, better model classifying observations correctly. curve diagonal indicates model better random guessing.model classifies observations correctly sensitivity specificity 1, belong (0,1) position (.e. top left) plot. curve diagonal closer top left plot, better model classifying observations correctly. curve diagonal indicates model better random guessing.model classifies observations incorrectly sensitivity specificity 0, belong (1,0) position (.e. bottom right) plot. curve diagonal closer bottom right plot, worse model classifying observations. curve diagonal indicates model worse random guessing.model classifies observations incorrectly sensitivity specificity 0, belong (1,0) position (.e. bottom right) plot. curve diagonal closer bottom right plot, worse model classifying observations. curve diagonal indicates model worse random guessing.Going back ROC curve, ROC diagonal better random guessing.Going back ROC curve, ROC diagonal better random guessing.also added point solid blue displays TPR FPR logistic regression threshold 0.5, based calculations performed earlier. TPR 0.6393443 FPR 0.2068966. Remember black curve displays TPR FPR logistic regression vary threshold.also added point solid blue displays TPR FPR logistic regression threshold 0.5, based calculations performed earlier. TPR 0.6393443 FPR 0.2068966. Remember black curve displays TPR FPR logistic regression vary threshold.","code":"\nlibrary(ROCR)\n\n##produce the numbers associated with classification table\nrates<-ROCR::prediction(preds, test$DrivDrnk)\n\n##store the true positive and false positive rates\nroc_result<-ROCR::performance(rates,measure=\"tpr\", x.measure=\"fpr\")\n\n##plot ROC curve and overlay the diagonal line for random guessing\nplot(roc_result, main=\"ROC Curve for Reduced Model\")\nlines(x = c(0,1), y = c(0,1), col=\"red\")\npoints(x=0.2068966, y=0.6393443, col=\"blue\", pch=16)"},{"path":"logistic2.html","id":"practice-question-4","chapter":"12 Logistic Regression 2","heading":"12.3.1.1 Practice question","text":", can locate point ROC curve corresponds logistic regression threshold 0.7?View associated video review practice question.","code":""},{"path":"logistic2.html","id":"auc","chapter":"12 Logistic Regression 2","heading":"12.3.2 AUC","text":"area curve (AUC) , name suggests, area ROC curve. numerical summary corresponding ROC curve.model randomly guesses, AUC 0.5.AUC closer 1 indicates model better random guessing classifying observations. AUC 1 indicates model classifies observations correctly.AUC closer 0 indicates model worse random guessing.Let us look AUC logistic regression:AUC around 0.7742, greater 0.5, better random guessing.","code":"\n##compute the AUC\nauc<-performance(rates, measure = \"auc\")\nauc@y.values## [[1]]\n## [1] 0.7741662"},{"path":"logistic2.html","id":"cautions-with-classification","chapter":"12 Logistic Regression 2","heading":"12.4 Cautions with Classification","text":"look ROC curve closely, realize couple places black ROC curve coincides red diagonal line. exist thresholds logistic regression performs well random guessing. ROC curve shows true positive false positive rates threshold varied. immediately inform true positive false positive rates specific threshold. possible even though ROC diagonal line, model performs well random guessing certain thresholds, may proposing use.AUC, just like ROC, summary predictive performance possible values threshold. common misconception AUC measure accuracy. ! simply area ROC curve.careful just producing ROC curve AUC. sure check confusion matrix various metrics threshold proposing.","code":""},{"path":"logistic2.html","id":"unbalanced-sample-sizes","chapter":"12 Logistic Regression 2","heading":"12.4.1 Unbalanced sample sizes","text":"Another situation pay attention binary response variable unbalanced. unbalanced proportions two levels different, .e. one level common, another level rare. example, trying classify whether email receive spam . Chances , emails receive legitimate, emails spam. variable whether email spam unbalanced.response variable unbalanced, high accuracy threshold yet model performing random guessing.Let us consider spam email example. Suppose receive 10,000 emails, 20 spam. Suppose use classifier use information emails, decides classify every single email spam. resulting confusion matrix shown Figure 12.2:\nFigure 12.2: Confusion Matrix Unbalanced Response\naccuracy \\(\\frac{9980}{10000} = 0.998\\) extremely high! just looked accuracy, may think model great detecting spam email, reality fails flag spam email.confusion matrix, TPR 0, FPR also 0, implies model guessing random.see unbalanced response variable, need look metrics TPR FPR, solely rely accuracy, assess well model classifying. Accuracy may misleading unbalanced response variable.workarounds include:Adjusting threshold reassess TPR FPR.Finding better predictors can distinguish two levels.Adjusting population interest response variable bit balanced.unbalanced response variable always bad? necessarily:guaranteed accuracy misleading. TPR high FPR low. just need double check.Recall two main uses models prediction association. Prediction can challenging unbalanced data, can still gain insights predictors related response variable.","code":""},{"path":"logistic2.html","id":"separation","chapter":"12 Logistic Regression 2","heading":"12.4.2 Separation","text":"Another issue pay attention whether separation classification. Separation occurs predictors almost perfectly predict binary response variable. perfect complete separation predictors perfectly predict binary response variable.scatterplot shows example perfect separation (using simulated data):can easily draw straight line scatterplot (say \\(x1 = 2\\)) perfectly separates group 1 group 2. left line group 1, right group 2.Recall two main uses models prediction association. Separation want using model prediction. However, separation poses challenges want gain insights predictors related response variable.main issue separation standard errors estimated coefficients get large. implies confidence intervals associated coefficients wide, making interpretation coefficients challenging.Suppose try fit logistic regression scatterplot. glm() function print warning message estimated probabilities exactly 0 1. warning message indication perfect separation exists.things consider separation exists data?first thing consider whether separation expected exist population interest. small sample, may possible separation exists small sample, exist population. case, collecting data likely break separation.first thing consider whether separation expected exist population interest. small sample, may possible separation exists small sample, exist population. case, collecting data likely break separation.using logistic regression prediction exploring predictors relate response, nothing, interested interpreting coefficients. fact, separation great prediction.using logistic regression prediction exploring predictors relate response, nothing, interested interpreting coefficients. fact, separation great prediction.check whether predictors just another version binary response variable. example, classifying whether newborn babies premature . include predictor duration pregnancy, predictor perfectly classify whether baby premature , since premature pregnancies based duration pregnancies. may still interested variables relate premature births, duration pregnancy needs removed predictor model.check whether predictors just another version binary response variable. example, classifying whether newborn babies premature . include predictor duration pregnancy, predictor perfectly classify whether baby premature , since premature pregnancies based duration pregnancies. may still interested variables relate premature births, duration pregnancy needs removed predictor model.","code":"\nresult.sep<-glm(G~., family=binomial, data=df)## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred"},{"path":"logistic2.html","id":"r-tutorial-9","chapter":"12 Logistic Regression 2","heading":"12.5 R Tutorial","text":"tutorial, learn evaluate predictive ability logistic regression model, via confusion matrices, ROC curve, AUC.continue use students.txt dataset previous tutorial, contains information 250 college students large public university study party habits. variables :Gender: gender studentSmoke: whether student smokesMarijuan: whether student uses marijuanaDrivDrnk: whether student driven drunkGPA: student’s GPAPartyNum: number times student parties monthDaysBeer: number days student drinks least 2 beers monthStudyHrs: number hours students studies weekSuppose want relate likelihood student driving drunk variables.Let us read data :going perform basic data wrangling dataframe:Remove first column, just index.Keep observations missing values variable. dozen observations missing values least one variable.Apply factor() categorical variables. reminder, done categorical variables want change reference class.going split dataset equal portions: one training set, another test set. Recall training set used build model, test set used assess model performs new observations. use set.seed() function can replicate split time block code run. integer needs supplied set.seed() function.set.seed() function allows us reproducible results. randomly split data training test set, get split time run block code.sample.int() function allows us sample vector random integers.first value maximum value integer want randomly generate, case, number observations data frame.second value represents number random integers want generate. case, 50% number observations, since want 50-50 split training test data.last value, replace=F, means want sample without replacement, means integer sampled, sampled .Recall previous tutorial use three predictors, Smoke, Marijuan, DaysBeer instead . fit logistic regression:","code":"\nlibrary(tidyverse)\nData<-read.table(\"students.txt\", header=T, sep=\"\")\n##first column is index, remove it\nData<-Data[,-1]\n##some NAs in data. Remove them\nData<-Data[complete.cases(Data),]\n\n##convert categorical to factors. needed for contrasts\nData$Gender<-factor(Data$Gender)\nData$Smoke<-factor(Data$Smoke)\nData$Marijuan<-factor(Data$Marijuan)\nData$DrivDrnk<-factor(Data$DrivDrnk)\n##set seed so results (split) are reproducible\nset.seed(6021)\n\n##evenly split data into train and test sets\nsample.data<-sample.int(nrow(Data), floor(.50*nrow(Data)), replace = F)\ntrain<-Data[sample.data, ]\ntest<-Data[-sample.data, ]\n##fit model\nreduced<-glm(DrivDrnk~Smoke+Marijuan+DaysBeer, family=binomial, data=train)"},{"path":"logistic2.html","id":"confusion-matrices","chapter":"12 Logistic Regression 2","heading":"Confusion Matrices","text":"create confusion matrix, need find predicted probabilities test data, use table() function tabulate number true positives, true negatives, false positives, false negatives via confusion matrix:placed actual response rows, whether predicted probability greater threshold 0.5 (classification based model) columns.model threshold 0.5, :46 observations truly driven drunk correctly classified driven drunk (true negative).12 observations truly driven drunk incorrectly classified driven drunk (false positive).22 observations truly driven drunk correctly classified driven drunk (true positive).39 observations truly driven drunk incorrectly classified driven drunk (false negative).","code":"\n##predicted probs for test data\npreds<-predict(reduced,newdata=test, type=\"response\")\n\n##confusion matrix with threshold of 0.5\ntable(test$DrivDrnk, preds>0.5)##      \n##       FALSE TRUE\n##   No     46   12\n##   Yes    22   39"},{"path":"logistic2.html","id":"roc-curve-1","chapter":"12 Logistic Regression 2","heading":"ROC Curve","text":"ROC curve plots true positive rate (TPR) false positive rate (FPR) logistic regression threshold varied 0 1. need couple functions ROCR package, load use prediction() performance() functions create ROC curve:prediction() function transforms vector estimated probabilities vector response test set format can used performance() function create ROC curve.object roc_result stores values true positive rate false positive rate via performance() function. arguments measure x.measure used plot true positive rate y-axis false positive rate x-axis respectively.function lines() used overlay diagonal line plot ease comparison random guessing model’s classification ability.reminder, ROC curve gives us possible combinations TPRs FPRs threshold varied 0 1. perfect classification result curve TPR 1 FPR 0, .e. appear top left plot.red diagonal line represents classifier randomly guesses binary outcome without using information predictors. ROC curve diagonal indicates logistic regression better random guessing; ROC curve diagonal worse random guessing. can see model better classifier randomly guesses.","code":"\nlibrary(ROCR)\n##produce the numbers associated with classification table\nrates<-ROCR::prediction(preds, test$DrivDrnk)\n\n##store the true positive and false postive rates\nroc_result<-ROCR::performance(rates,measure=\"tpr\", x.measure=\"fpr\")\n\n##plot ROC curve and overlay the diagonal line for random guessing\nplot(roc_result, main=\"ROC Curve for Reduced Model\")\nlines(x = c(0,1), y = c(0,1), col=\"red\")"},{"path":"logistic2.html","id":"auc-1","chapter":"12 Logistic Regression 2","heading":"AUC","text":"Another measure AUC. name suggests, simply area (ROC) curve. perfect classifier AUC 1. classifier randomly guesses AUC 0.5. Thus, AUCs closer 1 desirable. obtain AUC ROCThe AUC ROC curve 0.7742, means logistic regression better random guessing.","code":"\n##compute the AUC\nauc<-ROCR::performance(rates, measure = \"auc\")\nauc@y.values## [[1]]\n## [1] 0.7741662"},{"path":"logistic2.html","id":"practice","chapter":"12 Logistic Regression 2","heading":"Practice","text":", fit logistic regression using predictors. obtain confusion matrix threshold 0.5, well corresponding ROC curve AUC model. Compare obtained model just three predictors: Smoke, Marijuan, DaysBeer. model better based ? answer surprising?","code":""},{"path":"intro.html","id":"intro","chapter":"13 Introduction","heading":"13 Introduction","text":"can label chapter section titles using {#label} , e.g., can reference Chapter 13. manually label , automatic labels anyway, e.g., Chapter 14.Figures tables captions placed figure table environments, respectively.\nFigure 13.1: nice figure!\nReference figure code chunk label fig: prefix, e.g., see Figure 13.1. Similarly, can reference tables generated knitr::kable(), e.g., see Table 13.1.Table 13.1: nice table!can write citations, . example, using bookdown package (Xie 2023) sample book, built top R Markdown knitr (Xie 2015).","code":"\npar(mar = c(4, 4, .1, .1))\nplot(pressure, type = 'b', pch = 19)\nknitr::kable(\n  head(iris, 20), caption = 'Here is a nice table!',\n  booktabs = TRUE\n)"},{"path":"methods.html","id":"methods","chapter":"14 Methods","heading":"14 Methods","text":"describe methods chapter.Math can added body using usual syntax like ","code":""},{"path":"methods.html","id":"math-example","chapter":"14 Methods","heading":"14.1 math example","text":"\\(p\\) unknown expected around 1/3. Standard error approximated\\[\nSE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027\n\\]can also use math footnotes like this1.approximate standard error 0.0272","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
