[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"","code":""},{"path":"index.html","id":"who-is-this-book-for","chapter":"Preface","heading":"Who is this book for?","text":"many books linear models, various expectations different levels familiarity statistical, mathematical, coding concepts. books generally fall one two camps:Little familiarity statistical mathematical concepts, fairly familiar coding. books tend written programmers want get data science. books tend explain linear models trying avoid statistical mathematical concepts much, covering concepts absolutely necessary. books tend present linear models recipe format giving readers directions build models.drawback books readers get much understanding underlying concepts linear models. impossible give directions covering every possible scenario real world real data messy. Practitioners data science often think outside box order make linear models work particular data, difficult without understanding mathematical framework linear models.Familiarity mathematical notation introductory statistical concepts statistical inference, little familiarity coding. books tend written mathematicians (anyone strong background mathematics) want get data science. books cover mathematical framework linear models thoroughly.drawback books readers must comfortable mathematical notation. limits audience books people fairly thorough training mathematics. People without training get lost trying read books, understand need know mathematical foundations use linear models data science.book meant readable groups readers. foundational mathematical knowledge presented, written readable anyone. book also explain knowledge mean context data science. Practical advice, based foundational mathematical knowledge, also given.book accompanies course STAT 6021: Linear Models Data Science, Masters Data Science (MSDS) program University Virginia School Data Science.introductory statistics introductory programming pre-requisites entering MSDS program, book assumes basic knowledge statistical inference coding. Review materials covering concepts provided separately enrolled students.","code":""},{"path":"index.html","id":"data-sets-used","chapter":"Preface","heading":"Data sets used","text":"tried use many open source data sets much possible readers can work various examples provided . However, data sets may open source come experience teaching class since 2019 (variations class since 2013), used data sets shared statistics data science educators. goal eventually use open source data sets.","code":""},{"path":"index.html","id":"chapters","chapter":"Preface","heading":"Chapters","text":"chapters book follows:","code":""},{"path":"index.html","id":"other-resources","chapter":"Preface","heading":"Other resources","text":"resources readers may want check :OpenIntro Statistics, 4th ed. Diez, Cetinkaya-Rundel, Barr, OpenIntro. Get free PDF version https://leanpub.com/os, just set price want pay $0. good book introductory statistics.OpenIntro Statistics, 4th ed. Diez, Cetinkaya-Rundel, Barr, OpenIntro. Get free PDF version https://leanpub.com/os, just set price want pay $0. good book introductory statistics.Linear Models R, 2nd ed. Faraway. probably one books balances two camps wrote earlier. require familiarity matrices linear algebra though.Linear Models R, 2nd ed. Faraway. probably one books balances two camps wrote earlier. require familiarity matrices linear algebra though.Introduction Linear Regression Analysis, 5th 6th ed. Montgomery, Peck, Vining. may able access e-version book university library affiliated university. book mathematically rigorous useful interested mathematical proofs covered.Introduction Linear Regression Analysis, 5th 6th ed. Montgomery, Peck, Vining. may able access e-version book university library affiliated university. book mathematically rigorous useful interested mathematical proofs covered.Applied Linear Statistical Models (ALSM), Kutner, Nachtsheim, Neter, Li, 5th ed. book covers wide range topics linear models also mathematically rigorous.Applied Linear Statistical Models (ALSM), Kutner, Nachtsheim, Neter, Li, 5th ed. book covers wide range topics linear models also mathematically rigorous.Applied Linear Regression Models (ALRM), Kutner, Nachtsheim, Neter, 4th ed. ALRM first 14 chapters ALSM. second part ALSM covers topics Design Experiments, highly recommend interested topics.Applied Linear Regression Models (ALRM), Kutner, Nachtsheim, Neter, 4th ed. ALRM first 14 chapters ALSM. second part ALSM covers topics Design Experiments, highly recommend interested topics.","code":""},{"path":"wrangling.html","id":"wrangling","chapter":"1 Data Wrangling with R","heading":"1 Data Wrangling with R","text":"","code":""},{"path":"wrangling.html","id":"introduction","chapter":"1 Data Wrangling with R","heading":"1.1 Introduction","text":"data structure dealing often data frames. read data R, typically stored data frame. data frame can viewed like EXCEL spreadsheet, data stored rows columns. performing analysis, want data frame basic structure:row data frame corresponds observation.column data frame corresponds variable.Sometimes, data structured way, transform data take basic data structure. process called data wrangling. common basic operations transform data :Selecting subset columns data frame.Selecting subset rows data frame based criteria.Change column names.Find missing data.Create new variables based existing variables.Combine multiple data frames.explore two approaches data wrangling:Using functions already come pre-loaded R (sometimes called base R).Using functions dplyr package.two approaches quite different can achieve goals data wrangling. user R usually ends preferred way performing data wrangling operations, important know approaches able work broader audience.","code":""},{"path":"wrangling.html","id":"data-wrangling-using-base-r-functions","chapter":"1 Data Wrangling with R","heading":"1.2 Data Wrangling using Base R Functions","text":"use dataset ClassDataPrevious.csv example. data collected introductory statistics class UVa previous semester. Download dataset Canvas read R.check number rows columns dataframe.298 rows 8 columns: 298 students 8 variables. can also check names column.variables :Year: year student inSleep: much sleep student averages night (hours)Sport: student’s favorite sportCourses: many courses student taking semesterMajor: student’s majorAge: student’s age (years)Computer: operating system student uses (Mac PC)Lunch: much student usually spends lunch (dollars)","code":"\nData<-read.csv(\"ClassDataPrevious.csv\", header=TRUE)\ndim(Data)## [1] 298   8\ncolnames(Data)## [1] \"Year\"     \"Sleep\"    \"Sport\"    \"Courses\"  \"Major\"    \"Age\"      \"Computer\"\n## [8] \"Lunch\""},{"path":"wrangling.html","id":"view-specific-rows-andor-columns-of-a-data-frame","chapter":"1 Data Wrangling with R","heading":"1.2.1 View specific row(s) and/or column(s) of a data frame","text":"can view specific rows /columns data frame using square brackets [], example:row index listed first, column index, square brackets. means first student sleeps 8 hours night. can also view multiple rows columns, example:view 1st, 5th, 8th variables observations 1, 3, 4.several ways view specific column. example, view 1st column (variable called Year):Note comma separates indices row column. empty value comma means want rows, specific column. view multiple columns, example first four columns:view values certain rows, can useto view values observations 1 3. empty value comma means want columns specific rows.","code":"\nData[1,2] ##row index first, then column index## [1] 8\nData[c(1,3,4),c(1,5,8)]##     Year                            Major Lunch\n## 1 Second                         Commerce    11\n## 3 Second Cognitive science and psychology    10\n## 4  First                         Pre-Comm     4\nData$Year ##or\nData[,1] ##or\nData[,-c(2:8)]\nData[,1:4]\nData[,c(1,2,3,4)]\nData[c(1,3),]"},{"path":"wrangling.html","id":"select-observations-by-conditions","chapter":"1 Data Wrangling with R","heading":"1.2.2 Select observations by condition(s)","text":"may want analyze certain subsets data, based conditions. example, may want analyze students whose favorite sport soccer. () function R helps us find indices associated condition met. example:informs us rows belong observations whose favorite sport soccer, .e. 3rd, 20th, 25th () students. can create new data frame contains students whose favorite sport soccer:extracting rows satisfy condition, favorite sport soccer, storing rows new data frame called SoccerPeeps. can see new data frame 52 observations.Suppose want data frame satisfies two conditions: favorite sport soccer 2nd years UVa. can type:new data frame SoccerPeeps_2nd 25 observations.can also set conditions based numeric variables, example, want students sleep eight hours nightWe can also create data frame contains students satisfy least one two conditions, example, favorite sport soccer sleep 8 hours night:","code":"\nwhich(Data$Sport==\"Soccer\")##  [1]   3  20  25  26  31  32  33  38  44  46  48  50  51  64  67  71  87  92  98\n## [20]  99 118 122 124 126 128 133 136 137 143 146 153 159 165 174 197 198 207 211\n## [39] 214 226 234 241 255 259 260 266 274 278 281 283 294 295\nSoccerPeeps<-Data[which(Data$Sport==\"Soccer\"),]\ndim(SoccerPeeps)## [1] 52  8\nSoccerPeeps_2nd<-Data[which(Data$Sport==\"Soccer\" & Data$Year==\"Second\"),]\ndim(SoccerPeeps_2nd)## [1] 25  8\nSleepy<-Data[which(Data$Sleep>8),]\nSleepy_or_Soccer<-Data[which(Data$Sport==\"Soccer\" | Data$Sleep>8),]"},{"path":"wrangling.html","id":"change-column-names","chapter":"1 Data Wrangling with R","heading":"1.2.3 Change column name(s)","text":"datasets, names columns complicated make sense. always give descriptive names columns make sense. dataset, names self-explanatory really need change . example, suppose want change name 7th column Computer Comp:change names multiples columns (example, 1st 7th columns), type:","code":"\nnames(Data)[7]<-\"Comp\"\nnames(Data)[c(1,7)]<-c(\"Yr\",\"Computer\")"},{"path":"wrangling.html","id":"find-and-remove-missing-data","chapter":"1 Data Wrangling with R","heading":"1.2.4 Find and remove missing data","text":"ways locate missing data. Using .na() function directly data frame produces lot output can messy view:hand, using complete.cases() function pleasing view:code extract rows complete cases, words, rows missing entries. output informs us observation 103 missing value Sleep, observation 206 missing value Lunch.want remove observations missing value, can use one following two lines code create new data frames rows missing values removed:word caution: lines code remove entire row long least column missing entries. noted earlier, observation 103 missing value Sleep variable. observation still provides information variables, now removed.","code":"\nis.na(Data)\nData[!complete.cases(Data),]##         Yr Sleep      Sport Courses                                      Major\n## 103 Second    NA Basketball       7 psychology and youth and social innovation\n## 206 Second     8       None       4                          Cognitive Science\n##     Age Computer Lunch\n## 103  19      Mac    10\n## 206  19      Mac    NA\nData_nomiss<-na.omit(Data) ##or\nData_nomiss2<-Data[complete.cases(Data),]"},{"path":"wrangling.html","id":"summarizing-variables","chapter":"1 Data Wrangling with R","heading":"1.2.5 Summarizing variable(s)","text":"often, want obtain characteristics data. common way summarize numerical variable find mean. four numerical variables data frame, columns 2, 4, 6, 8. find mean four numerical variables, can use apply() function:Notice due missing values, first line NA variables. second line includes optional argument, na.rm=T, remove observations NA value variable calculation mean.least 3 arguments supplied apply() function:first argument data frame containing variables want find mean . case, want columns 2, 4, 6, 8 data frame Data.first argument data frame containing variables want find mean . case, want columns 2, 4, 6, 8 data frame Data.second argument takes value 1 2. Since want find mean columns, rather rows, type 2. want mean row, type 1.second argument takes value 1 2. Since want find mean columns, rather rows, type 2. want mean row, type 1.third argument specifies name function want apply columns supplied data frame. case, want mean. can change find median, standard deviation, etc, numeric variables want .third argument specifies name function want apply columns supplied data frame. case, want mean. can change find median, standard deviation, etc, numeric variables want .notice means variables suspiciously high, looking medians informative.","code":"\napply(Data[,c(2,4,6,8)],2,mean)##     Sleep   Courses       Age     Lunch \n##        NA  5.016779 19.573826        NA\napply(Data[,c(2,4,6,8)],2,mean,na.rm=T)##      Sleep    Courses        Age      Lunch \n## 155.559259   5.016779  19.573826 156.594175\napply(Data[,c(2,4,6,8)],2,median,na.rm=T)##   Sleep Courses     Age   Lunch \n##     7.5     5.0    19.0     9.0"},{"path":"wrangling.html","id":"summarizing-variable-by-groups","chapter":"1 Data Wrangling with R","heading":"1.2.6 Summarizing variable by groups","text":"Sometimes want summarize variable groups. Suppose want find median amount sleep separately 1st years, 2nd years, 3rd years, 4th years get. can use tapply() function:informs us median amount sleep first years get 8 hours night; fourth years median amount 7 hours night.least 3 arguments supplied tapply() function:first argument contains vector want summarize.first argument contains vector want summarize.second argument contains factor use subset data. example, want subset according Yr.second argument contains factor use subset data. example, want subset according Yr.third argument function want apply subset data.third argument function want apply subset data.fourth argument optional, case, want remove observations missing values calculation mean.fourth argument optional, case, want remove observations missing values calculation mean.Notice output orders factor levels alphabetical order. context, better rearrange levels First, Second, Third, Fourth using factor() function:output makes lot sense context.want summarize variable groups formed one variable, need adjust second argument tapply() function creating list. Suppose want find median sleep hour based Yr preferred operating system observations,Interestingly, looks like observations specify operating system use, hence extra column output.","code":"\ntapply(Data$Sleep,Data$Yr,median,na.rm=T)##  First Fourth Second  Third \n##    8.0    7.0    7.5    7.0\nData$Yr<-factor(Data$Yr, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\"))\n\nlevels(Data$Yr)## [1] \"First\"  \"Second\" \"Third\"  \"Fourth\"\ntapply(Data$Sleep,Data$Yr,median,na.rm=T) ##much nicer##  First Second  Third Fourth \n##    8.0    7.5    7.0    7.0\ntapply(Data$Sleep,list(Data$Yr,Data$Computer),median,na.rm=T)##           Mac   PC\n## First  NA 8.0 7.50\n## Second  7 7.5 7.50\n## Third  NA 7.5 7.00\n## Fourth NA 7.0 7.25"},{"path":"wrangling.html","id":"create-a-new-variable-based-on-existing-variables","chapter":"1 Data Wrangling with R","heading":"1.2.7 Create a new variable based on existing variable(s)","text":"Depending context analysis, may need create new variables based existing variables. variations task, based type variable want create, type variable based .","code":""},{"path":"wrangling.html","id":"create-a-numeric-variable-based-on-another-numeric-variable","chapter":"1 Data Wrangling with R","heading":"1.2.7.1 Create a numeric variable based on another numeric variable","text":"variable Sleep number hours. Suppose need convert values Sleep number minutes, can simply perform following mathematical operation:store transformed variable vector called Sleep_mins.","code":"\nSleep_mins<-Data$Sleep * 60"},{"path":"wrangling.html","id":"create-a-binary-variable-based-on-a-numeric-variable","chapter":"1 Data Wrangling with R","heading":"1.2.7.2 Create a binary variable based on a numeric variable","text":"Suppose want create binary variable (categorical variable two levels), called deprived. observation obtain value “yes” sleep less 7 hours night, “” otherwise. ifelse() function useful creating binary variables:3 arguments associated ifelse() function:first argument condition wish use.first argument condition wish use.second argument value observation condition true.second argument value observation condition true.third argument value observation condition false.third argument value observation condition false.","code":"\ndeprived<-ifelse(Data$Sleep<7, \"yes\", \"no\")"},{"path":"wrangling.html","id":"create-a-categorical-variable-based-on-a-numeric-variable","chapter":"1 Data Wrangling with R","heading":"1.2.7.3 Create a categorical variable based on a numeric variable","text":"Suppose want create categorical variable based number courses student takes. call new variable CourseLoad, takes following values:light 3 courses less,regular 4 5 courses,heavy 5 courses .cut() function used situationThere three arguments applied cut() function:first argument vector basing new variable .first argument vector basing new variable .argument breaks lists want set intervals associated Data$Courses. case, creating three intervals: one \\((-\\infty, 3]\\), another \\((3, 5]\\), last interval \\((5, \\infty]\\).argument breaks lists want set intervals associated Data$Courses. case, creating three intervals: one \\((-\\infty, 3]\\), another \\((3, 5]\\), last interval \\((5, \\infty]\\).argument labels gives label CourseLoad associated interval.argument labels gives label CourseLoad associated interval.","code":"\nCourseLoad<-cut(Data$Courses, breaks = c(-Inf, 3, 5, Inf), \n                labels = c(\"light\", \"regular\", \"heavy\"))"},{"path":"wrangling.html","id":"collapse-levels","chapter":"1 Data Wrangling with R","heading":"1.2.7.4 Collapse levels","text":"Sometimes, categorical variable levels need analysis, want collapse levels. example, variable Yr four levels: First, Second, Third, Fourth. Perhaps interested comparing upperclassmen underclassmen, want collapse First Second years underclassmen, Third Fourth years upperclassmen:levels associated variable Yr ordered First, Second, Third, Fourth. character vector new.levels first two characters, upper last two characters correspond original levels variable Yr. new variable called Year2.","code":"\nlevels(Data$Yr)## [1] \"First\"  \"Second\" \"Third\"  \"Fourth\"\nnew.levels<-c(\"und\", \"und\", \"up\",\"up\")\nYear2<-factor(new.levels[Data$Yr])\nlevels(Year2)## [1] \"und\" \"up\""},{"path":"wrangling.html","id":"combine-data-frames","chapter":"1 Data Wrangling with R","heading":"1.2.8 Combine data frames","text":"created four new variables, Sleep_mins, deprived, CourseLoad, Year2, based previously existing variables. Since variables based observations, can combine existing data frame using data.frame() function:Notice since listed four new variables Data data.frame() function, appear original columns data frame.Alternatively, can use cbind() function gives data frame:combining data frames different observations columns, can merge using rbind():","code":"\nData<-data.frame(Data,Sleep_mins,deprived,CourseLoad,Year2)\nhead(Data)##       Yr Sleep      Sport Courses                            Major Age Computer\n## 1 Second     8 Basketball       6                         Commerce  19      Mac\n## 2 Second     7     Tennis       5                       Psychology  19      Mac\n## 3 Second     8     Soccer       5 Cognitive science and psychology  21      Mac\n## 4  First     9 Basketball       5                         Pre-Comm  19      Mac\n## 5 Second     4 Basketball       6                      Statistics   19       PC\n## 6  Third     7       None       4                       Psychology  20       PC\n##   Lunch Sleep_mins deprived CourseLoad Year2\n## 1    11        480       no      heavy   und\n## 2    10        420       no    regular   und\n## 3    10        480       no    regular   und\n## 4     4        540       no    regular   und\n## 5     0        240      yes      heavy   und\n## 6    11        420       no    regular    up\nData2<-cbind(Data,Sleep_mins,deprived,CourseLoad,Year2)\ndat1<-Data[1:3,1:3]\ndat3<-Data[6:8,1:3]\nres.dat2<-rbind(dat1,dat3)\nhead(res.dat2)##       Yr Sleep      Sport\n## 1 Second     8 Basketball\n## 2 Second     7     Tennis\n## 3 Second     8     Soccer\n## 6  Third     7       None\n## 7 Second     7 Basketball\n## 8  First     7 Basketball"},{"path":"wrangling.html","id":"export-data-frame-in-r-to-a-.csv-file","chapter":"1 Data Wrangling with R","heading":"1.2.9 Export data frame in R to a .csv file","text":"export data frame .csv file, type:file newdata.csv created working directory. Note default, argument row.names set TRUE. add column dataframe index number. find step useful analyses almost always set row.names FALSE.","code":"\nwrite.csv(Data, file=\"newdata.csv\", row.names = FALSE)"},{"path":"wrangling.html","id":"sort-data-frame-by-column-values","chapter":"1 Data Wrangling with R","heading":"1.2.10 Sort data frame by column values","text":"sort data frame ascending order Age:sort descending order Age:sort ascending order Age first, Sleep:","code":"\nData_by_age<-Data[order(Data$Age),]\nData_by_age_des<-Data[order(-Data$Age),]\nData_by_age_sleep<-Data[order(Data$Age, Data$Sleep),]"},{"path":"wrangling.html","id":"data-wrangling-using-dplyr-functions","chapter":"1 Data Wrangling with R","heading":"1.3 Data Wrangling Using dplyr Functions","text":"previous section, performing data wrangling operations using functions built base R. module, using functions mostly package called dplyr, can perform operations well.performing data wrangling operations, let us clear environment, previously declared objects removed. allows us start clean slate, often desirable starting new analysis. done via:dplyr package subset tidyverse package, can access functions installing loading either package. installing tidyverse package, load typing:dplyr package developed make syntax intuitive broader range R users, primarily use pipes. However, code involved functions dlpyr tends longer code involved base R functions, functions learn dplyr.find lot articles internet various R users believes one approach superior . fussy approach use long can perform necessary operations. benefit familiar approaches can work broader range R users.continue use dataset ClassDataPrevious.csv example. Download dataset Canvas read R:examples , performing operations previous section, using dplyr functions instead base R functions.","code":"\nrm(list = ls())\n##library(dplyr) or\nlibrary(tidyverse) \nData<-read.csv(\"ClassDataPrevious.csv\", header=TRUE)"},{"path":"wrangling.html","id":"select-specific-columns-of-a-data-frame","chapter":"1 Data Wrangling with R","heading":"1.3.1 Select specific column(s) of a data frame","text":"select() function used select specific columns. couple ways use function. First:select column Year data frame called Data.","code":"\nselect(Data,Year)"},{"path":"wrangling.html","id":"pipes","chapter":"1 Data Wrangling with R","heading":"1.3.1.1 Pipes","text":"Alternatively, can use pipes:Pipes R typed using %>% pressing Ctrl + Shift + M keyboard. think operations , can read code astake data frame called Dataand select column named Year.can interpret pipe “”. Commands pipe placed new line (press enter). Pipes especially useful want execute several commands sequence, see later examples.","code":"\nData%>%\n  select(Year)"},{"path":"wrangling.html","id":"select-observations-by-conditions-1","chapter":"1 Data Wrangling with R","heading":"1.3.2 Select observations by condition(s)","text":"filter() function allows us subset data based conditions, example, select students whose favorite sport soccer:can create new data frame called SoccerPeeps contains students whose favorite sport soccer:Suppose want data frame, called SoccerPeeps_2nd, satisfies two conditions: favorite sport soccer 2nd years UVa:can also set conditions based numeric variables, example, want students sleep eight hours night:can also create data frame contains observations long satisfy least one two conditions: favorite sport soccer sleep 8 hours night:","code":"\nfilter(Data, Sport==\"Soccer\")\nSoccerPeeps<-Data%>%\n  filter(Sport==\"Soccer\")\nSoccerPeeps_2nd<-Data%>%\n  filter(Sport==\"Soccer\" & Year==\"Second\")\nSleepy<-Data%>%\n  filter(Sleep>8)\nSleepy_or_Soccer<-Data%>%\n  filter(Sport==\"Soccer\" | Sleep>8)"},{"path":"wrangling.html","id":"change-column-names-1","chapter":"1 Data Wrangling with R","heading":"1.3.3 Change column name(s)","text":"straightforward change names columns using rename() function. example:allows us change name two columns: Year Computer Yr Comp.","code":"\nData<-Data%>%\n  rename(Yr=Year, Comp=Computer)"},{"path":"wrangling.html","id":"summarizing-variables-1","chapter":"1 Data Wrangling with R","heading":"1.3.4 Summarizing variable(s)","text":"summarize() function allows us summarize column. Suppose want find mean value numeric columns: Sleep, Courses, Age, Lunch:output looks bit cumbersome. can give names summaryAs mentioned previously, means look suspiciously high couple variables, looking medians may informative:Note: lot functions dplyr package, using American spelling British spelling works. can use summarise() instead summarize().","code":"\nData%>%\n  summarize(mean(Sleep,na.rm = T),mean(Courses),mean(Age),mean(Lunch,na.rm = T))##   mean(Sleep, na.rm = T) mean(Courses) mean(Age) mean(Lunch, na.rm = T)\n## 1               155.5593      5.016779  19.57383               156.5942\nData%>%\n  summarize(avgSleep=mean(Sleep,na.rm = T),avgCourse=mean(Courses),avgAge=mean(Age),\n            avgLun=mean(Lunch,na.rm = T))##   avgSleep avgCourse   avgAge   avgLun\n## 1 155.5593  5.016779 19.57383 156.5942\nData%>%\n  summarize(medSleep=median(Sleep,na.rm = T),medCourse=median(Courses),\n            medAge=median(Age),medLun=median(Lunch,na.rm = T))##   medSleep medCourse medAge medLun\n## 1      7.5         5     19      9"},{"path":"wrangling.html","id":"summarizing-variable-by-groups-1","chapter":"1 Data Wrangling with R","heading":"1.3.5 Summarizing variable by groups","text":"Suppose want find median amount sleep 1st years, 2nd years, 3rd years, 4th years get. can use group_by() function:way read code isGet data frame called Data,group observations Yr,find median amount sleep Yr store median vector called medSleep.seen previously, ordering factor levels alphabetical order. context, better rearrange levels First, Second, Third, Fourth. can use mutate() function whenever want transform create new variable. case, transforming variable Yr reordering factor levels fct_relevel() function:Get data frame called Data,transform variable called Yr,reorder factor levels., use pipes, group_by(), summarize() functions like :output makes lot sense context.summarize variable groups formed one variable, just add variables group_by() function:","code":"\nData%>%\n  group_by(Yr)%>%\n  summarize(medSleep=median(Sleep,na.rm=T))## # A tibble: 4 × 2\n##   Yr     medSleep\n##   <chr>     <dbl>\n## 1 First       8  \n## 2 Fourth      7  \n## 3 Second      7.5\n## 4 Third       7\nData<- Data%>%\n  mutate(Yr = Yr%>%\n           fct_relevel(c(\"First\",\"Second\",\"Third\",\"Fourth\")))\nData%>%\n  group_by(Yr)%>%\n  summarize(medSleep=median(Sleep,na.rm=T))## # A tibble: 4 × 2\n##   Yr     medSleep\n##   <fct>     <dbl>\n## 1 First       8  \n## 2 Second      7.5\n## 3 Third       7  \n## 4 Fourth      7\nData%>%\n  group_by(Yr,Comp)%>%\n  summarize(medSleep=median(Sleep,na.rm=T))## `summarise()` has grouped output by 'Yr'. You can override using the `.groups`\n## argument.## # A tibble: 9 × 3\n## # Groups:   Yr [4]\n##   Yr     Comp  medSleep\n##   <fct>  <chr>    <dbl>\n## 1 First  \"Mac\"     8   \n## 2 First  \"PC\"      7.5 \n## 3 Second \"\"        7   \n## 4 Second \"Mac\"     7.5 \n## 5 Second \"PC\"      7.5 \n## 6 Third  \"Mac\"     7.5 \n## 7 Third  \"PC\"      7   \n## 8 Fourth \"Mac\"     7   \n## 9 Fourth \"PC\"      7.25"},{"path":"wrangling.html","id":"create-a-new-variable-based-on-existing-variables-1","chapter":"1 Data Wrangling with R","heading":"1.3.6 Create a new variable based on existing variable(s)","text":"mentioned previously, mutate() function used transform variable create new variable. variations task, based type variable want create, type variable based .","code":""},{"path":"wrangling.html","id":"create-a-numeric-variable-based-on-another-numeric-variable-1","chapter":"1 Data Wrangling with R","heading":"1.3.6.1 Create a numeric variable based on another numeric variable","text":"variable Sleep number hours. Suppose need convert values Sleep number minutes, can simply perform following mathematical operation:store transformed variable called Sleep_mins add Sleep_mins data frame called Data.","code":"\nData<-Data%>%\n  mutate(Sleep_mins = Sleep*60)"},{"path":"wrangling.html","id":"create-a-binary-variable-based-on-a-numeric-variable-1","chapter":"1 Data Wrangling with R","heading":"1.3.6.2 Create a binary variable based on a numeric variable","text":"Suppose want create binary variable , called deprived. observation obtain value yes sleep less 7 hours night, otherwise. add variable deprived data frame called Data:","code":"\nData<-Data%>%\n  mutate(deprived=ifelse(Sleep<7, \"yes\", \"no\"))"},{"path":"wrangling.html","id":"create-a-categorical-variable-based-on-a-numeric-variable-1","chapter":"1 Data Wrangling with R","heading":"1.3.6.3 Create a categorical variable based on a numeric variable","text":"Suppose want create categorical variable based number courses student takes. call new variable CourseLoad, takes following values:light 3 courses less,regular 4 5 courses,heavy 5 coursesand add CourseLoad data frame Data. can use case_when() function dplyr package, instead cut() function:Notice names categorical variable supplied specific condition specified.","code":"\nData<-Data%>%\n  mutate(CourseLoad=case_when(Courses <= 3 ~ \"light\", \n                              Courses >3 & Courses <=5 ~ \"regular\", \n                              Courses > 5 ~ \"heavy\"))"},{"path":"wrangling.html","id":"collapsing-levels","chapter":"1 Data Wrangling with R","heading":"1.3.6.4 Collapsing levels","text":"Sometimes, categorical variable levels need analysis, want collapse levels. example, variable Yr four levels: First, Second, Third, Fourth. Perhaps interested comparing upperclassmen underclassmen, want collapse First Second Yrs underclassmen, Third Fourth Yrs upperclassmen. use fct_collapse() function:creating new variable called UpUnder, done collapsing First Second new factor called , collapsing Third Fourth new factor called . UpUnder also added data frame Data.","code":"\nData<-Data%>%\n  mutate(UpUnder=fct_collapse(Yr,under=c(\"First\",\"Second\"),up=c(\"Third\",\"Fourth\")))"},{"path":"wrangling.html","id":"combine-data-frames-1","chapter":"1 Data Wrangling with R","heading":"1.3.7 Combine data frames","text":"combine data frames different observations columns, can combine using bind_rows():bind_rows() works way rbind(). Likewise, can use bind_cols() instead cbind().","code":"\ndat1<-Data[1:3,1:3]\ndat3<-Data[6:8,1:3]\nres.dat2<-bind_rows(dat1,dat3)\nhead(res.dat2)##       Yr Sleep      Sport\n## 1 Second     8 Basketball\n## 2 Second     7     Tennis\n## 3 Second     8     Soccer\n## 4  Third     7       None\n## 5 Second     7 Basketball\n## 6  First     7 Basketball"},{"path":"wrangling.html","id":"sort-data-frame-by-column-values-1","chapter":"1 Data Wrangling with R","heading":"1.3.8 Sort data frame by column values","text":"sort data frame ascending order Age:sort descending order Age:sort ascending order Age first, Sleep:","code":"\nData_by_age<-Data%>%\n  arrange(Age)\nData_by_age_des<-Data%>%\n  arrange(desc(Age))\nData_by_age_sleep<-Data%>%\n  arrange(Age,Sleep)"},{"path":"viz.html","id":"viz","chapter":"2 Data Visualization with R Using ggplot2","heading":"2 Data Visualization with R Using ggplot2","text":"","code":""},{"path":"viz.html","id":"introduction-1","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.1 Introduction","text":"Data visualizations tools summarize data. Consider visuals CDC covid tracker dashboard external site. Without actually access actual data, sense trends associated hospitalizations deaths. Good visualizations easy interpret wide variety audiences, easier explain statistical models.module, learn create common data visualizations. choice data visualization almost always determined whether variable(s) involved categorical quantitative. Discrete variables interesting depending circumstance, can viewed either categorical quantitative context data visualizations.using functions ggplot2 package create visualizations. ggplot2 package enables users create various kinds data visualizations, beyond visualizations can made base R. ggplot2 package automatically loaded load tidyverse package, although can load ggplot2 .use dataset ClassDataPrevious.csv example. data collected introductory statistics class UVa previous semester. Download dataset Canvas read R.variables :Year: year student inSleep: much sleep student averages night (hours)Sport: student’s favorite sportCourses: many courses student taking semesterMajor: student’s majorAge: student’s age (years)Computer: operating system student uses (Mac PC)Lunch: much student usually spends lunch (dollars)","code":"\nlibrary(tidyverse)\nData<-read.csv(\"ClassDataPrevious.csv\", header=TRUE)"},{"path":"viz.html","id":"visualizations-with-a-single-categorical-variable","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.2 Visualizations with a Single Categorical Variable","text":"","code":""},{"path":"viz.html","id":"frequency-tables","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.2.1 Frequency tables","text":"Frequency tables common tool summarize categorical variables. tables give us number observations (sometimes called counts) belong class categorical variable. tables created using table() function. Suppose want see number students year data:Notice order years rearranged make sense:83 first years, 139 second years, 46 third years, 30 fourth years dataset.can report numbers using proportions instead counts, using prop.table():percentages multiplying 100:round percentages two decimal places, use round() function:27.85% students first years, 46.64% second years, 15.44% third years, 10.07% fourth years.","code":"\ntable(Data$Year)## \n##  First Fourth Second  Third \n##     83     30    139     46\nData$Year<-factor(Data$Year, levels=c(\"First\",\"Second\",\"Third\",\"Fourth\"))\nlevels(Data$Year)## [1] \"First\"  \"Second\" \"Third\"  \"Fourth\"\nmytab<-table(Data$Year)\nmytab## \n##  First Second  Third Fourth \n##     83    139     46     30\nprop.table(mytab)## \n##     First    Second     Third    Fourth \n## 0.2785235 0.4664430 0.1543624 0.1006711\nprop.table(mytab) * 100## \n##    First   Second    Third   Fourth \n## 27.85235 46.64430 15.43624 10.06711\nround(prop.table(mytab) * 100, 2)## \n##  First Second  Third Fourth \n##  27.85  46.64  15.44  10.07"},{"path":"viz.html","id":"bar-charts","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.2.2 Bar charts","text":"Bar charts simple way visualize categorical data, can viewed visual representation frequency tables. create bar chart years students, use:can read number students first, second, third, fourth years reading corresponding value vertical axis.two lines code, can see basic structure creating data visualizations ggplot() function:Use ggplot() function, supply name data frame, x- /y- variables via aes() function. End line + operator, press enter.Use ggplot() function, supply name data frame, x- /y- variables via aes() function. End line + operator, press enter.next line, specify type graph want create (called geoms). bar chart, type geom_bar().next line, specify type graph want create (called geoms). bar chart, type geom_bar().describe lines code two layers code. two layers must supplied data visualizations ggplot().Additional optional layers can added (usually deal details visuals). Suppose want change orientation bar chart, can add optional line, layer:recommended layer typed line previous layer. + sign used end layer add another layer .change color bars:different color outline bars:","code":"\nggplot(Data, aes(x=Year))+\n  geom_bar()\nggplot(Data, aes(x=Year))+\n  geom_bar()+\n  coord_flip()\nggplot(Data, aes(x=Year))+\n  geom_bar(fill=\"blue\")\nggplot(Data, aes(x=Year))+\n  geom_bar(fill=\"blue\",color=\"orange\")"},{"path":"viz.html","id":"customize-title-and-labels-of-axes-in-bar-charts","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.2.2.1 Customize title and labels of axes in bar charts","text":"change orientation labels horizontal axis, add extra layer called theme. useful many classes /labels long names.rotate labels horizontal 90 degrees:create visualizations, good practice give short meaningful descriptive names axis provide title. can change labels x- y- axes, well add title bar chart adding another layer called labs:can also adjust position title, example, center-justify via theme:","code":"\nggplot(Data, aes(x=Year))+\n  geom_bar()+\n  theme(axis.text.x = element_text(angle = 90))\nggplot(Data, aes(x=Year))+\n  geom_bar()+\n  theme(axis.text.x = element_text(angle = 90))+\n  labs(x=\"Year\", y=\"Number of Students\", title=\"Dist of Years\")\nggplot(Data, aes(x=Year))+\n  geom_bar()+\n  theme(axis.text.x = element_text(angle = 90), \n        plot.title = element_text(hjust = 0.5))+\n  labs(x=\"Year\", y=\"Number of Students\", title=\"Dist of Years\")"},{"path":"viz.html","id":"create-a-bar-chart-using-proportions","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.2.2.2 Create a bar chart using proportions","text":"Suppose want create bar chart y-axis displays proportions, rather counts level. steps produce bar chart. First, create new dataframe, row represents year, add proportion year new column:code following:Creates new data frame called newData taking data frame called Data,groups observations Year,counts number observations Year stores values vector called Counts,creates new vector called Percent using mathematical operations specified mutate(). Percent added newData.can take look contents newData:create bar chart using proportions:Note following:first layer, use newData instead old data frame. aes(), specified y-variable, want Percent.second layer, specified stat=\"identity\" inside geom_bar().","code":"\nnewData<-Data%>%\n  group_by(Year)%>%\n  summarize(Counts=n())%>%\n  mutate(Percent=Counts/nrow(Data))\nnewData## # A tibble: 4 × 3\n##   Year   Counts Percent\n##   <fct>   <int>   <dbl>\n## 1 First      83   0.279\n## 2 Second    139   0.466\n## 3 Third      46   0.154\n## 4 Fourth     30   0.101\nggplot(newData, aes(x=Year, y=Percent))+\n  geom_bar(stat=\"identity\")+\n  theme(axis.text.x = element_text(angle = 90), \n        plot.title = element_text(hjust = 0.5))+\n  labs(x=\"Year\", y=\"Percent of Students\", title=\"Dist of Years\")"},{"path":"viz.html","id":"visualizations-with-a-single-quantitative-variable","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.3 Visualizations with a Single Quantitative Variable","text":"","code":""},{"path":"viz.html","id":"number-summary","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.3.1 5 number summary","text":"summary() function, applied quantitative variable, produces 5 number summary: minimum, first quartile (25th percentile), median (50th percentile), third quartile (75th percentile), maximum, well mean. example, obtain 5 number summary ages students:average age observations dataset 19.57 years old. Notice first quartile median 19 years old, means least quarter observations 19 years old. Also note maximum 51 years old, student quite lot older rest.","code":"\nsummary(Data$Age)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   18.00   19.00   19.00   19.57   20.00   51.00"},{"path":"viz.html","id":"boxplots","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.3.2 Boxplots","text":"boxplot graphical representation 5 number summary. create generic boxplot, following two lines code using ggplot() function:Note still using structure creating data visualizations ggplot():Use ggplot() function, supply name data frame, x- /y- variables via aes() function. End line + operator, press enter.Use ggplot() function, supply name data frame, x- /y- variables via aes() function. End line + operator, press enter.next line, specify type graph want create (called geoms). boxplot, type geom_boxplot.next line, specify type graph want create (called geoms). boxplot, type geom_boxplot.Notice outliers (observations lot older younger) denoted dots. One 51 year old, 22 year olds deemed outliers. rule used \\(1.5 \\times IQR\\) rule.Similar bar charts, can change orientation boxplots adding additional layer :can change color box outliers similarly:","code":"\nggplot(Data, aes(y=Age))+\n  geom_boxplot()\nggplot(Data, aes(y=Age))+\n  geom_boxplot()+\n  coord_flip()\nggplot(Data, aes(y=Age))+\n  geom_boxplot(color=\"blue\", outlier.color = \"orange\" )"},{"path":"viz.html","id":"histograms","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.3.3 Histograms","text":"histogram displays number observations within bin x-axis:Notice warning message displayed creating basic histogram. fix , use binwidth argument within geom_histogram. try binwidth=1 now, means width bin 1 unit:ages students mostly young, 19 20 years olds commonly occuring.well-known drawback histograms width bins can drastically affect visual. example, suppose change binwidth 2:bar now contains two ages: first bar contains 18 19 year olds. Notice shape changed little bit previous histogram different binwidth?","code":"\nggplot(Data,aes(x=Age))+\n  geom_histogram()## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nggplot(Data,aes(x=Age))+\n  geom_histogram(binwidth = 1,fill=\"blue\",color=\"orange\")\nggplot(Data,aes(x=Age))+\n  geom_histogram(binwidth = 2,fill=\"blue\",color=\"orange\")"},{"path":"viz.html","id":"density-plots","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.3.4 Density plots","text":"Density plots variation histograms, plot attempts use smooth mathematical function approximate shape histogram, unaffected binwidth:can see 19 20 year olds common ages data. careful interpreting values veritical axis: represent proportions. characteristic density plots area plot always one.","code":"\nggplot(Data,aes(x=Age))+\n  geom_density()"},{"path":"viz.html","id":"bivariate-visualizations","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.4 Bivariate Visualizations","text":"now look visualizations can create explore relationship two variables. term bivariate means looking two variables.using new dataset example, clear environment:using dataset, gapminder, gapminder package. Install load gapminder package. Also load tidyverse package (automatically loads ggplot2 package):can take look gapminder dataset:Per documentation, variables arecountrycontinentyear: 1952 2007 increments 5 yearslifeExp: life expectancy birth, yearspop: population countrygdpPercap: GDP per capita US dollars, adjusted inflationWe notice data collected country across number different years: 1952 2007 increments five years. example, mainly focus data recent year, 2007:specific visuals use depend type variables using, whether categorical quantitative.","code":"\nrm(list = ls())\nlibrary(tidyverse)\nlibrary(gapminder)\ngapminder[1:15,]## # A tibble: 15 × 6\n##    country     continent  year lifeExp      pop gdpPercap\n##    <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## 11 Afghanistan Asia       2002    42.1 25268405      727.\n## 12 Afghanistan Asia       2007    43.8 31889923      975.\n## 13 Albania     Europe     1952    55.2  1282697     1601.\n## 14 Albania     Europe     1957    59.3  1476505     1942.\n## 15 Albania     Europe     1962    64.8  1728137     2313.\nData<-gapminder%>%\n  filter(year==2007)"},{"path":"viz.html","id":"compare-quantitative-variable-across-categories","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.4.1 Compare quantitative variable across categories","text":"","code":""},{"path":"viz.html","id":"side-by-side-boxplots","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.4.1.1 Side by side boxplots","text":"Side side boxplots useful compare quantitative variable across different classes categorical variable. example, want compare life expectancies across different continents year 2007:Countries Oceania region long life expectancies little variation. Comparing Americas Asia, median life expectancies similar, spread larger Asia.","code":"\nggplot(Data, aes(x=continent, y=lifeExp))+\n  geom_boxplot(fill=\"Blue\")+\n  labs(x=\"Continent\", y=\"Life Exp\", title=\"Dist of Life Expectancies by Continent\")"},{"path":"viz.html","id":"violin-plots","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.4.1.2 Violin plots","text":"Violin plots alternative boxplots. create plots compare life expectancies across different continents year 2007:width violin informs us values commonly occuring. example, look violin Europe. violin wider higher life expectancies, longer life expectancies common European countries.","code":"\nggplot(Data, aes(x=continent, y=lifeExp))+\n  geom_violin()+\n  labs(x=\"Continent\", y=\"Life Exp\", title=\"Dist of Life Expectancies by Continent\")"},{"path":"viz.html","id":"summarizing-two-categorical-variables","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.4.2 Summarizing two categorical variables","text":"example, create new binary variable called expectancy, denoted low life expectancy country less 70 years, high otherwise:","code":"\nData<-Data%>%\n  mutate(expectancy=ifelse(lifeExp<70,\"Low\",\"High\"))"},{"path":"viz.html","id":"two-way-tables","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.4.2.1 Two-way tables","text":"Suppose want see expectancy varies across continents. two-way table can created produce counts two categorical variables involved:first variable table() placed rows, second variable placed columns.table, can see 22 countries Americas high life expectancies, 3 countries Americas low life expectancies.may interested looking proportions, instead counts, countries continent high low life expectancies. convert table proportions, can use prop.table():want proportions continent, want proportions row add 1. Therefore, second argument prop.table() 1. Enter 2 argument want proportions column add 1., convert percentages round 2 decimal places:","code":"\nmytab2<-table(Data$continent, Data$expectancy)\n##continent in rows, expectancy in columns\nmytab2##           \n##            High Low\n##   Africa      7  45\n##   Americas   22   3\n##   Asia       22  11\n##   Europe     30   0\n##   Oceania     2   0\nprop.table(mytab2, 1) ##           \n##                 High       Low\n##   Africa   0.1346154 0.8653846\n##   Americas 0.8800000 0.1200000\n##   Asia     0.6666667 0.3333333\n##   Europe   1.0000000 0.0000000\n##   Oceania  1.0000000 0.0000000\nround(prop.table(mytab2, 1) * 100, 2)##           \n##              High    Low\n##   Africa    13.46  86.54\n##   Americas  88.00  12.00\n##   Asia      66.67  33.33\n##   Europe   100.00   0.00\n##   Oceania  100.00   0.00"},{"path":"viz.html","id":"bar-charts-1","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.4.2.2 Bar charts","text":"stacked bar chart can used display relationship binary variable expectancy across continents:can see many countries exist continent, many countries continent high low life expectancies. example, 25 countries Americas majority high life expectancies.can change way bar chart displayed changing position geom_bar() position = \"dodge\" position = \"fill\", latter useful proportions instead counts:","code":"\nggplot(Data, aes(x=continent, fill=expectancy))+\n  geom_bar(position = \"stack\")+\n  labs(x=\"Continent\", y=\"Count\", title=\"Life Expectancies by Continent\")\nggplot(Data, aes(x=continent, fill=expectancy))+\n  geom_bar(position = \"dodge\") \nggplot(Data, aes(x=continent, fill=expectancy))+\n  geom_bar(position = \"fill\")+\n  labs(x=\"Continent\", y=\"Proportion\", \n       title=\"Proportion of Life Expectancies by Continent\")"},{"path":"viz.html","id":"summarizing-two-quantitative-variables","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.4.3 Summarizing two quantitative variables","text":"","code":""},{"path":"viz.html","id":"scatterplots","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.4.3.1 Scatterplots","text":"Scatterplots standard visualization two quantitative variables involved. create scatterplot life expectancy GDP per capita:see curved relationship life expectancies GDP per capita. Countries higher GDP per capita tend longer life expectancies.many observations, plots scatterplot may actually overlap . sense many exist, can add transparency scale called alpha=0.2 inside geom_point():default value alpha 1, means points transparent. closer value 0, transparent points . darker point indicates observations specific values variables.","code":"\nggplot(Data, aes(x=gdpPercap,y=lifeExp))+\n  geom_point()\nggplot(Data, aes(x=gdpPercap,y=lifeExp))+\n  geom_point(alpha=0.2)+\n  labs(x=\"GDP\", y=\"Life Exp\", \n       title=\"Scatterplot of Life Exp against GDP\")"},{"path":"viz.html","id":"multivariate-visualizations","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.5 Multivariate Visualizations","text":"now look visualizations can create explore relationship multiple (two) variables. term multivariate means looking two variables.","code":""},{"path":"viz.html","id":"bar-charts-2","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.5.1 Bar charts","text":"Previously, created bar chart look expectancy varies across continents. Suppose want see bar graphs vary across years, use year variable third variable via layer facet_wrap:Notice three categorical variables summarized bar chart. something can done improve bar chart? make improvement?","code":"\n##another data frame across all years plus a binary variable \n##for expectancy\nData.all<-gapminder%>%\n  mutate(expectancy=ifelse(lifeExp<70,\"Low\",\"High\"))\n\nggplot(Data.all,aes(x=continent, fill=expectancy))+\n  geom_bar(position = \"fill\")+\n  facet_wrap(~year)"},{"path":"viz.html","id":"scatterplots-1","chapter":"2 Data Visualization with R Using ggplot2","heading":"2.5.2 Scatterplots","text":"Previously, created scatterplot life expectancy GDP per capita. can include another quantitative variable scatterplot, using size plots. can use size plots denote population countries. supplied via size aes():can adjust size plots adding layer scale_size():scatterplot summarizes three quantitative variables.can use different-colored plots denote continent point belongs :scatterplot summarizes three quantitative variables one categorical variable.can adjust plots changing shape making translucent via shape alpha aes():","code":"\nggplot(Data, aes(x=gdpPercap, y=lifeExp, size=pop))+\n  geom_point()\nggplot(Data, aes(x=gdpPercap, y=lifeExp, size=pop))+\n  geom_point()+\n  scale_size(range = c(0.1,12))\nggplot(Data, aes(x=gdpPercap, y=lifeExp, size=pop, color=continent))+\n  geom_point()+\n  scale_size(range = c(0.1,12))\nggplot(Data, aes(x=gdpPercap, y=lifeExp, size=pop, fill=continent))+\n  geom_point(shape=21, alpha=0.5)+\n  scale_size(range = c(0.1,12))+\n  labs(x=\"GDP\", y=\"Life Exp\", title=\"Scatterplot of Life Exp against GDP\")"},{"path":"slr.html","id":"slr","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3 Basics with Simple Linear Regression (SLR)","text":"","code":""},{"path":"slr.html","id":"introduction-2","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.1 Introduction","text":"start module introducing simple linear regression model. Simple linear regression uses term “simple,” concerns study one predictor variable one quantitative response variable. contrast, multiple linear regression, study future modules, uses term “multiple,” concerns study two predictor variables one quantitative response variable. start simple linear regression much easier visualize concepts regression models one predictor variable.time , consider predictor variables quantitative. consider predictor variables categorical future modules.common way visualizing relationship one quantitative predictor variable one quantitative response variable scatter plot. simulated example , data 6000 UVa undergraduate students amount time spend studying week (minutes), many courses taking semester (3 4 credit courses).\nFigure 3.1: Scatterplot Study Time Number Courses Taken\nQuestions may include:study time number courses taken related one another?strong relationship?use data make prediction study time student scatterplot?confident prediction?questions can answered using simple linear regression.Note learning models just one response variable. cover multivariate regression, used one response variable. may confusion “multiple” linear regression “multivariate” regression due closeness terminology.","code":"\n##create dataframe\ndf<-data.frame(study,courses)\n\n##fit regression\nresult<-lm(study~courses, data=df)\n##create scatterplot with regression line overlaid\nplot(df$courses, df$study, xlab=\"# of Courses\", ylab=\"Study Time (Mins)\")\nabline(result)"},{"path":"slr.html","id":"basic-ideas-with-statistics","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.1.1 Basic Ideas with Statistics","text":"","code":""},{"path":"slr.html","id":"population-vs-sample","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.1.1.1 Population vs Sample","text":"Statistical methods usually used make inferences population based information sample.sample collection units actually measured surveyed study.population includes units interest.study time example , population UVa undergraduate students, sample 6000 students data displayed scatterplot.","code":""},{"path":"slr.html","id":"parameters-vs-statistics","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.1.1.2 Parameters vs Statistics","text":"Parameters numerical quantities describe population.Statistics numerical quantities describe sample.study time example, example parameter average study time among UVa undergraduate students (called population mean), example statistic average study time among 6000 UVa students data (called sample mean).Notice real life, rarely know actual numerical value parameter. use numerical value statistic estimate unknown numerical value corresponding parameter.also different notation parameters statistics. example,population mean denoted \\(\\mu\\).sample mean denoted \\(\\bar{x}\\).say \\(\\bar{x}\\) estimator \\(\\mu\\).important pay attention whether describing statistic (known value can calculated) parameter (unknown value).","code":""},{"path":"slr.html","id":"motivation","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.1.2 Motivation","text":"Linear regression models generally two primary uses:Prediction: Predict future value response variable, using information predictor variables.Association: Quantify relationship variables. change predictor variable change value response variable?always distinguish response variable, denoted \\(y\\), predictor variable, denoted \\(x\\). statistical models, say response variable can approximated mathematical function, denoted \\(f\\), predictor variable, .e.\\[\ny \\approx f(x).\n\\]\nOftentimes, write relationship \\[\ny = f(x) + \\epsilon,\n\\]\\(\\epsilon\\) denotes random error term, mean 0. error term predicted based data .various statistical methods estimate \\(f\\). estimate \\(f\\), can use method prediction / association.Using study time example :prediction example: student intends take 4 courses semester. student’s predicted study time, average?association example: want see taking courses increases study time.","code":""},{"path":"slr.html","id":"practice-questions","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.1.2.1 Practice questions","text":"examples , using regression model prediction association?early morning heading rest day. want know weather forecast rest day know wear.early morning heading rest day. want know weather forecast rest day know wear.executive sports league wants assess increasing length commercial breaks may impact enjoyment sports fans watch games TV.executive sports league wants assess increasing length commercial breaks may impact enjoyment sports fans watch games TV.Education Secretary like evaluate certain factors use technology classrooms investment teacher training teacher pay associated reading skills students.Education Secretary like evaluate certain factors use technology classrooms investment teacher training teacher pay associated reading skills students.buying home, prospective buyer like know home - - priced, given characteristics.buying home, prospective buyer like know home - - priced, given characteristics.","code":""},{"path":"slr.html","id":"simple-linear-regression-slr","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.2 Simple Linear Regression (SLR)","text":"simple linear regression (SLR), function \\(f\\) relates predictor variable response variable typically \\(\\beta_0 + \\beta_1 x\\). Mathematically, express \\[\ny \\approx \\beta_0 + \\beta_1 x,\n\\]words, response variable approximately linear relationship predictor variable.SLR, relationship explicitly formulated simple linear regression equation:\\[\\begin{equation}\nE(y|x)=\\beta_0+\\beta_{1}x.\n\\tag{3.1}\n\\end{equation}\\]\\(\\beta_0\\) \\(\\beta_1\\) parameters SLR equation, want estimate .\\(\\beta_0\\) \\(\\beta_1\\) parameters SLR equation, want estimate .parameters sometimes called regression coefficients.parameters sometimes called regression coefficients.\\(\\beta_1\\) also called slope. denotes change \\(y\\), average, \\(x\\) increases one unit.\\(\\beta_1\\) also called slope. denotes change \\(y\\), average, \\(x\\) increases one unit.\\(\\beta_0\\) also called intercept. denotes average \\(y\\) \\(x=0\\).\\(\\beta_0\\) also called intercept. denotes average \\(y\\) \\(x=0\\).notation left hand side (3.1) denotes expected value response variable, fixed value predictor variable. (3.1) implies , value predictor variable \\(x\\), expected value response variable \\(y\\) \\(\\beta_0+\\beta_{1}x\\). expected value also population mean. Applying (3.1) study time example, implies :\nstudents take 3 courses, expected study time equal \\(\\beta_0 + 3\\beta_1\\),\nstudents take 4 courses, expected study time equal \\(\\beta_0 + 4\\beta_1\\),\nstudents take 5 courses, expected study time equal \\(\\beta_0 + 5\\beta_1\\).\nnotation left hand side (3.1) denotes expected value response variable, fixed value predictor variable. (3.1) implies , value predictor variable \\(x\\), expected value response variable \\(y\\) \\(\\beta_0+\\beta_{1}x\\). expected value also population mean. Applying (3.1) study time example, implies :students take 3 courses, expected study time equal \\(\\beta_0 + 3\\beta_1\\),students take 4 courses, expected study time equal \\(\\beta_0 + 4\\beta_1\\),students take 5 courses, expected study time equal \\(\\beta_0 + 5\\beta_1\\).\\(f(x) = \\beta_0 + \\beta_1x\\) gives us value expected value response variable specific value predictor variable. , value predictor variable, value response variable constant. say value \\(x\\), response variable \\(y\\) variance. variance response variable value \\(x\\) variance error term, \\(\\epsilon\\). Thus simple linear regression model\\[\\begin{equation}\ny=\\beta_0+\\beta_{1} x + \\epsilon.\n\\tag{3.2}\n\\end{equation}\\]need make assumptions error term \\(\\epsilon\\). Generally, assumptions :errors mean 0.errors variance denoted \\(\\sigma^2\\). Notice variance constant.errors independent.errors normally distributed.(3.2), notice another parameter, \\(\\sigma^2\\).go detail assumptions mean, assess whether met, module 5.assumptions mean value predictor variable \\(x\\), response variable:follows normal distribution,mean equal \\(\\beta_0+\\beta_{1} x\\),variance equal \\(\\sigma^2\\).Using study time example, means :students take 3 courses, distribution study times \\(N(\\beta_0 + 3\\beta_1, \\sigma^2)\\).students take 4 courses, distribution study times \\(N(\\beta_0 + 4\\beta_1, \\sigma^2)\\).students take 5 courses, distribution study times \\(N(\\beta_0 + 5\\beta_1, \\sigma^2)\\).subset dataframe three subsets, one students take 3 courses, another subset students take 4 courses, another subset students take 5 courses, create density plot study times subset, density plot follow normal distribution, different means, spread.Let us take look density plots next.Notice plots normal, different means (centers), similar spreads.Please see associated video explanation distribution response variable, value predictor variable, SLR setting.","code":"\nlibrary(tidyverse)\n##subset dataframe\nx.3<-df[which(df$courses==3),]\n##density plot of study time for students taking 3 courses\nggplot(x.3,aes(x=study))+\n  geom_density()+\n  labs(x=\"Study Time (Mins)\", title=\"Dist of Study Times with 3 Courses\")\n##subset dataframe\nx.4<-df[which(df$courses==4),]\n##density plot of study time for students taking 4 courses\nggplot(x.4,aes(x=study))+\n  geom_density()+\n  labs(x=\"Study Time (Mins)\", title=\"Dist of Study Times with 4 Courses\")\n##subset dataframe\nx.5<-df[which(df$courses==5),]\n##density plot of study time for students taking 5 courses\nggplot(x.5,aes(x=study))+\n  geom_density()+\n  labs(x=\"Study Time (Mins)\", title=\"Dist of Study Times with 5 Courses\")"},{"path":"slr.html","id":"estimating-regression-coefficients-in-slr","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.3 Estimating Regression Coefficients in SLR","text":"(3.1) (3.2), noted estimate regression coefficients \\(\\beta_0, \\beta_1\\) well parameter \\(\\sigma^2\\) associated error term. mentioned earlier, unable obtain numerical values parameters data entire population. use data sample estimate parameters.estimate \\(\\beta_0,\\beta_1\\) using \\(\\hat{\\beta}_0,\\hat{\\beta}_1\\) based sample observations \\((x_i,y_i)\\) size \\(n\\).subscripts associated response predictor variables denote data point value belongs . Let us take look first rows data frame study time example:example, \\(x_1\\) denotes number courses taken student number 1 dataframe, 3. \\(y_4\\) denotes study time student number 4 dataframe, 378.0196456.Following (3.1) (3.2), sample versions \\[\\begin{equation}\n\\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1 x\n\\tag{3.3}\n\\end{equation}\\]\\[\\begin{equation}\ny=\\hat{\\beta}_0+\\hat{\\beta}_1 x + e\n\\tag{3.4}\n\\end{equation}\\]respectively. (3.3) called estimated SLR equation, fitted SLR equation. (3.4) called estimated SLR model.\\(\\hat{\\beta}_1,\\hat{\\beta}_0\\) estimators \\(\\beta_1,\\beta_0\\) respectively. estimators can interpreted following manner:\\(\\hat{\\beta}_1\\) denotes change predicted \\(y\\) \\(x\\) increases 1 unit. Alternatively, denotes change \\(y\\), average, \\(x\\) increases 1 unit.\\(\\hat{\\beta}_0\\) denotes predicted \\(y\\) \\(x=0\\). Alternatively, denotes average \\(y\\) \\(x=0\\).(3.4), notice use \\(e\\) denote residual, words, “error” sample.(3.3) (3.4), following quantities can compute:\\[\\begin{equation}\n\\text{Predicted/Fitted values: } \\hat{y}_i = \\hat{\\beta}_0+\\hat{\\beta}_1 x_i.\n\\tag{3.5}\n\\end{equation}\\]\\[\\begin{equation}\n\\text{Residuals: } e_i = y_i-\\hat{y}_i.\n\\tag{3.6}\n\\end{equation}\\]\\[\\begin{equation}\n\\text{Sum Squared Residuals: } SS_{res} =  \\sum\\limits_{=1}^n(y_i-\\hat{y}_i)^2.\n\\tag{3.7}\n\\end{equation}\\]compute estimated coefficients \\(\\hat{\\beta}_1,\\hat{\\beta}_0\\) using method least squares, .e. choose numerical values \\(\\hat{\\beta}_1,\\hat{\\beta}_0\\) minimize \\(SS_{res}\\) given (3.7).minimizing \\(SS_{res}\\) respect \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\), estimated coefficients simple linear regression equation \\[\\begin{equation}\n\\hat{\\beta}_1 = \\frac{\\sum\\limits_{=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum\\limits_{=1}^n(x_i-\\bar{x})^2}\n\\tag{3.8}\n\\end{equation}\\]\\[\\begin{equation}\n\\hat{\\beta}_0 = \\bar{y}- \\hat{\\beta}_1 \\bar{x}\n\\tag{3.9}\n\\end{equation}\\]\\(\\hat{\\beta}_1, \\hat{\\beta}_0\\) called least squares estimators.minimization \\(SS_{res}\\) respect \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\) done taking partial derivatives (3.7) respect \\(\\hat{\\beta}_1\\) \\(\\hat{\\beta}_0\\), setting two partial derivatives equal 0, solving two equations \\(\\hat{\\beta}_1\\) \\(\\hat{\\beta}_0\\).Let’s take look estimated coefficients study time example:sample 6000 students, \\(\\hat{\\beta}_1\\) = 120.3930985. predicted study time increases 120.3930985 minutes additional course taken.\\(\\hat{\\beta}_0\\) = 58.4482853. predicted study time 58.4482853 courses taken. Notice value make sense, student taking 0 courses. look data, number courses taken 3, 4, 5. use regression \\(3 \\leq x \\leq 5\\). use values \\(x\\) outside range data. Making predictions response variable predictors outside range data called extrapolation done.","code":"\nhead(df)##      study courses\n## 1 429.8311       3\n## 2 458.4588       3\n## 3 391.9406       3\n## 4 378.0196       3\n## 5 397.9856       3\n## 6 405.7145       3\n##fit regression\nresult<-lm(study~courses, data=df)\n##print out the estimated coefficients\nresult## \n## Call:\n## lm(formula = study ~ courses, data = df)\n## \n## Coefficients:\n## (Intercept)      courses  \n##       58.45       120.39"},{"path":"slr.html","id":"estimating-variance-of-errors-in-slr","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.4 Estimating Variance of Errors in SLR","text":"estimator \\(\\sigma^2\\), variance error terms (also variance probability distribution \\(y\\) given \\(x\\)) \n\\[\\begin{equation}\ns^2 = MS_{res} = \\frac{SS_{res}}{n-2} = \\frac{\\sum\\limits_{=1}^n e_i^2}{n-2},\n\\tag{3.10}\n\\end{equation}\\]\\(MS_{res}\\) called mean squared residuals.\\(\\sigma^2\\), variance error terms, measures spread response variable, value \\(x\\). smaller , closer data points regression equation.","code":""},{"path":"slr.html","id":"practice-questions-1","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.4.1 Practice questions","text":"Take look scatterplot study time number courses taken, Figure 3.1. plot, label following:estimated SLR equationthe fitted value \\(x=3\\), \\(x=4\\), \\(x=5\\).residual data point plot choosing.Try first, view associated video see labeled plot correctly!","code":""},{"path":"slr.html","id":"assessing-linear-association","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.5 Assessing Linear Association","text":"noted earlier, variance error terms inform us close data points estimated SLR equation. smaller variance error terms, closer data points estimated SLR equation. turn implies linear relationship variables stronger.learn common measures used quantify strength linear relationship response predictor variables. , need define terms.","code":""},{"path":"slr.html","id":"sum-of-squares","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.5.1 Sum of squares","text":"\\[\\begin{equation}\n\\text{Total Sum Squares: } SS_T = \\sum\\limits_{=1}^{n} (y_i - \\bar{y})^{2}.\n\\tag{3.11}\n\\end{equation}\\]Total sum squares defined total variance response variable. larger value , larger spread response variable.\\[\\begin{equation}\n\\text{Regression sum squares: } SS_R = \\sum\\limits_{=1}^{n} (\\hat{y_i} - \\bar{y})^{2}.\n\\tag{3.12}\n\\end{equation}\\]Regression sum squares defined variance response variable can explained regression.also residual sum squares, \\(SS_{res}\\). mathematical formulation given (3.7). defined variance response variable explained regression.can shown \\[\\begin{equation}\nSS_T = SS_R + SS_{res}.\n\\tag{3.13}\n\\end{equation}\\]sums squares associated degrees freedom (df):df \\(SS_R\\): \\(df_R = 1\\)df \\(SS_{res}\\): \\(df_{res} = n-2\\)df \\(SS_T\\): \\(df_T = n-1\\)Please see associated video explanation concept behind degrees freedom.","code":""},{"path":"slr.html","id":"anova-table","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.5.2 ANOVA Table","text":"Information regarding sums squares usually presented form ANOVA (analysis variance) table:Note:Dividing sum square corresponding degrees freedom gives corresponding mean square.last column, report \\(F\\) statistic, equal \\(\\frac{MS_R}{MS_{res}}\\). \\(F\\) statistic associated ANOVA F test, look detail next subsection.obtain ANOVA table study time example:Notice R print information line regarding \\(SS_T\\).","code":"\nanova(result)## Analysis of Variance Table\n## \n## Response: study\n##             Df   Sum Sq  Mean Sq F value    Pr(>F)    \n## courses      1 57977993 57977993   65404 < 2.2e-16 ***\n## Residuals 5998  5317017      886                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"slr.html","id":"anova-f-test","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.5.3 ANOVA \\(F\\) Test","text":"SLR, ANOVA \\(F\\) statistic ANOVA table can used test slope SLR equation 0 . words, means whether linear association variables . slope 0, means changes value predictor variable change value response variable, average; hence variables linearly associated.null alternative hypotheses :\\[\nH_0: \\beta_1 = 0, H_a: \\beta_1 \\neq 0.\n\\]\ntest statistic \\[\\begin{equation} \\label{eq:ANOVA}\nF = \\frac{MS_R}{MS_{res}}\n\\end{equation}\\]compared \\(F_{1,n-2}\\) distribution. Note \\(F_{1,n-2}\\) read F distribution 1 \\(n-2\\) degrees freedom.Going back study time example, \\(F\\) statistic 6.5403586^{4}. critical value can found usingSince test statistic larger critical value, reject null hypothesis. data support claim slope different 0, words, linear association study time number courses taken.","code":"\nqf(1-0.05, 1, 6000-2)## [1] 3.84301"},{"path":"slr.html","id":"coefficient-of-determination","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.5.4 Coefficient of determination","text":"coefficient determination, \\(R^2\\), \\[\\begin{equation}\nR^{2} = \\frac{SS_R}{SS_T} = 1 - \\frac{SS_{res}}{SS_T}.\n\\tag{3.14}\n\\end{equation}\\]\\(R^{2}\\) indication well data fits model. context simple linear regression, denotes proportion variance response variable explained predictor.notes \\(R^2\\):\\(0 \\leq R^2 \\leq 1\\).Values closer 1 indicate better fit; values closer 0 indicate poorer fit.Sometimes reported percentage.obtain \\(R^2\\) study time example:implies proportion variance study time can explained number courses taken 0.9159963.","code":"\nanova.tab<-anova(result)\n##SST not provided, so we add up SSR and SSres\nSST<-sum(anova.tab$\"Sum Sq\")\n##R2\nanova.tab$\"Sum Sq\"[1]/SST## [1] 0.9159963"},{"path":"slr.html","id":"correlation","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.5.5 Correlation","text":"measure used quantify strength linear association two quantitative variables sample correlation. sample correlation, \\(\\mbox{Corr}(x,y)\\) \\(r\\), given \\[\\begin{equation}\nr = \\frac{\\sum\\limits_{=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum\\limits_{=1}^{n}(x_i - \\bar{x})^{2}(y_i - \\bar{y})^{2}}}.\n\\tag{3.15}\n\\end{equation}\\]notes \\(r\\):\\(-1 \\leq r \\leq 1\\).Sign correlation indicates direction association. positive value indicates positive linear association: predictor variable increases, response variable, average. negative value indicates negative linear association: predictor variable increases, response variable decreases, average.Values closer 1 -1 indicate stronger linear association; values closer 0 indicate weaker linear association.SLR, turns \\(r^2 = R^2\\).Using study time example, correlation study time number courses taken isThis value indicates strong positive linear association study time number courses taken (remember simulated data real).","code":"\ncor(df$study, df$courses)## [1] 0.9570769"},{"path":"slr.html","id":"how-strong-is-strong","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.5.5.1 How strong is strong?","text":"question often raised large magnitude sample correlation considered strong? answer : depends context. conducting experiment governed scientific laws (e.g experiment verifying Newton’s 2nd law \\(F = ma\\)), expect extremely high correlation. correlation 0.9 instance may considered weak. value correlation compared correlations similar studies domain determine strong .","code":""},{"path":"slr.html","id":"a-word-of-caution","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.6 A Word of Caution","text":"able use measures learned (correlation, \\(R^2\\)) interpret estimated regression coefficients, must verify via scatterplot association two variables approximately linear. see non linear pattern scatterplot, use interpret values. learn remedy situation see non linear pattern scatterplot module 5.Please see associated video demonstration looking scatterplot can lead misleading interpretations.","code":""},{"path":"slr.html","id":"r-tutorial","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"3.7 R Tutorial","text":"tutorial, work dataset elmhurst openintro package R.Type ?openintro::elmhurst read documentation datasets R. Always seek understand background data! key pieces information :random sample 50 students (freshman 2011 class Elmhurst College).Family income student (units missing).Gift aid, $1000s.want explore family income may related gift aid, simple linear regression framework.","code":"\nlibrary(tidyverse)\nlibrary(openintro)\nData<-openintro::elmhurst"},{"path":"slr.html","id":"visualization","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"Visualization","text":"always verify scatterplot relationship (approximately) linear proceeding correlation simple linear regression!note observations fairly evenly scattered sides regression line, linear association exists. see negative linear association. family income increases, gift aid, average, decreases.also see observation weird values may warrant investigation.","code":"\n##scatterplot of gift aid against family income\nggplot2::ggplot(Data, aes(x=family_income,y=gift_aid))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se=FALSE)+\n  labs(x=\"Family Income\", y=\"Gift Aid\", title=\"Scatterplot of Gift Aid against Family\")"},{"path":"slr.html","id":"regression","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"Regression","text":"use lm() function fit regression model:Use summary() function display relevant information regression:see following values:\\(\\hat{\\beta}_1 =\\) -0.0430717. estimated slope informs us predicted gift aid decreases 0.0430717 thousands dollars ($43.07) per unit increase family income.\\(\\hat{\\beta}_0 =\\) 24.319329. students family income, predicted gift aid $24 319.33. Note: scatterplot, observation 0 family income. must careful extrapolating making predictions regression. make predictions family incomes minimum maximum values family incomes data.\\(s\\) = 4.7825989, estimate standard deviation error terms. reported residual standard error R. Squaring gives estimated variance.\\(F\\) = 15.8772043. value ANOVA \\(F\\) statistic. corresponding p-value reported. Since p-value small, reject null hypothesis. data support claim linear association gift aid family income.\\(R^2 =\\) 0.2485582. coefficient determination informs us 24.86% variation gift aid can explained family income.","code":"\n##regress gift aid against family income\nresult<-lm(gift_aid~family_income, data=Data)\n##look at information regarding regression\nsummary(result)## \n## Call:\n## lm(formula = gift_aid ~ family_income, data = Data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.1128  -3.6234  -0.2161   3.1587  11.5707 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   24.31933    1.29145  18.831  < 2e-16 ***\n## family_income -0.04307    0.01081  -3.985 0.000229 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.783 on 48 degrees of freedom\n## Multiple R-squared:  0.2486, Adjusted R-squared:  0.2329 \n## F-statistic: 15.88 on 1 and 48 DF,  p-value: 0.0002289"},{"path":"slr.html","id":"extract-values-from-r-objects","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"Extract values from R objects","text":"can actually extract values reported summary(result). see can extracted R object, use names() function:extract estimated coefficients:Notice information presented table. extract specific value, can specify row column indices:, extract values residual standard error, ANOVA F statistic, \\(R^2\\).","code":"\n##see what can be extracted from summary(result)\nnames(summary(result))##  [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n##  [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n##  [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\"\n##extract coefficients\nsummary(result)$coefficients##                  Estimate Std. Error   t value     Pr(>|t|)\n## (Intercept)   24.31932901 1.29145027 18.831022 8.281020e-24\n## family_income -0.04307165 0.01080947 -3.984621 2.288734e-04\n##extract slope\nsummary(result)$coefficients[2,1]## [1] -0.04307165\n##extract intercept\nsummary(result)$coefficients[1,1]## [1] 24.31933"},{"path":"slr.html","id":"prediction","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"Prediction","text":"use regression models prediction. Suppose want predict gift aid student family income 50 thousand dollars (assuming unit thousands dollars). use predict() function:student’s predicted gift aid $22 165.75. Alternatively, calculated plugging \\(x=50\\) estimated SLR equation:","code":"\n##create data point for prediction\nnewdata<-data.frame(family_income=50)\n##predicted gift aid when x=50\npredict(result,newdata)##        1 \n## 22.16575\nsummary(result)$coefficients[1,1] + summary(result)$coefficients[2,1]*50## [1] 22.16575"},{"path":"slr.html","id":"anova-table-1","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"ANOVA table","text":"use anova() function display ANOVA tableThe report \\(F\\) statistic value reported earlier summary(result).first line output gives \\(SS_{R}\\), second line gives \\(SS_{res}\\). function doesn’t provide \\(SS_T\\), know \\(SS_T = SS_{R} + SS_{res}\\)., see can extracted anova.tab:\\(SS_T\\) can easily calculated:\\(R^2\\) reported 0.2485582. verify using ANOVA table:","code":"\nanova.tab<-anova(result)\nanova.tab## Analysis of Variance Table\n## \n## Response: gift_aid\n##               Df  Sum Sq Mean Sq F value    Pr(>F)    \n## family_income  1  363.16  363.16  15.877 0.0002289 ***\n## Residuals     48 1097.92   22.87                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nnames(anova.tab)## [1] \"Df\"      \"Sum Sq\"  \"Mean Sq\" \"F value\" \"Pr(>F)\"\nSST<-sum(anova.tab$\"Sum Sq\")\nSST## [1] 1461.079\nanova.tab$\"Sum Sq\"[1]/SST## [1] 0.2485582"},{"path":"slr.html","id":"correlation-1","chapter":"3 Basics with Simple Linear Regression (SLR)","heading":"Correlation","text":"use cor() function find correlation two quantitative variables:correlation -0.4985561. moderate, negative linear association family income gift aid.","code":"\n##correlation\ncor(Data$family_income,Data$gift_aid)## [1] -0.4985561"},{"path":"inf.html","id":"inf","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4 Inference with Simple Linear Regression (SLR)","text":"","code":""},{"path":"inf.html","id":"introduction-3","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.1 Introduction","text":"Oftentimes, data collect come random sample representative population interest. common example election poll presidential election. Random sampling allows sample representative population. However, obtain another random sample, characteristics new sample unlikely exactly first sample. example, sample proportion vote certain party unlikely random samples. tells us even representative samples, sample proportions unlikely equal population proportion, sample proportions vary sample sample.Dr. W. Edwards Deming’s Red Bead experiment illustrates concept. video experiment can found .video, number red beads, represent bad products, varies time worker obtains random sample 50 beads. fact number red beads increases second sample indicate performed task worse, increase due random variation associated samples.Note: Deming’s Red Bead experiment developed illustrate concepts associated management. best known work developing Japanese economy World War II. able find many blogs/articles discussing experiment World Wide Web. Although many articles discuss experiment applies management, can used illustrate concepts variation.idea extends slope intercept regression line. estimated slope intercept vary sample sample unlikely equal population slope intercept. inferential statistics, use hypothesis tests confidence intervals aid us accounting random variation. module, learn account quantify random variation associated estimated regression model, interpret estimated regression model accounting random variation.","code":""},{"path":"inf.html","id":"review-from-previous-module","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.1.1 Review from previous module","text":"simple linear regression model written \\[\\begin{equation}\ny=\\beta_0+\\beta_{1} x + \\epsilon.\n\\tag{4.1}\n\\end{equation}\\]make assumptions error term \\(\\epsilon\\). :errors mean 0.errors variance denoted \\(\\sigma^2\\). Notice variance constant.errors independent.errors normally distributed.assumptions allow us derive distributional properties associated least squares estimators \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\), enables us compute reliable confidence intervals perform hypothesis tests SLR reliably.\\(\\hat{\\beta}_1,\\hat{\\beta}_0\\) estimators \\(\\beta_1,\\beta_0\\) respectively. estimators can interpreted following manner:\\(\\hat{\\beta}_1\\) denotes change predicted \\(y\\) \\(x\\) increases 1 unit. Alternatively, denotes change \\(y\\), average, \\(x\\) increases 1 unit.\\(\\hat{\\beta}_0\\) denotes predicted \\(y\\) \\(x=0\\). Alternatively, denotes average \\(y\\) \\(x=0\\).values estimators vary sample sample?","code":""},{"path":"inf.html","id":"hypothesis-testing-in-slr","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.2 Hypothesis Testing in SLR","text":"","code":""},{"path":"inf.html","id":"distribution-of-least-squares-estimators","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.2.1 Distribution of least squares estimators","text":"Gauss Markov Theorem: assumptions regression model, least squares estimators \\(\\hat{\\beta}_1\\) \\(\\hat{\\beta}_0\\) unbiased minimum variance among unbiased linear estimators.Thus, least squares estimators following properties:\\(\\mbox{E}(\\hat{\\beta}_1) = \\beta_1\\), \\(\\mbox{E}(\\hat{\\beta}_0) = \\beta_0\\)Note: estimator unbiased expected value exactly equal parameter estimating.variance \\(\\hat{\\beta}_1\\) \\[\\begin{equation}\n\\mbox{Var}(\\hat{\\beta}_1) = \\frac{\\sigma^{2}}{\\sum{(x_{}-\\bar{x})^{2}}}\n\\tag{4.2}\n\\end{equation}\\]variance \\(\\hat{\\beta}_0\\) \\[\\begin{equation}\n\\mbox{Var}(\\hat{\\beta}_0) = \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i -\\bar{x})^2}\\right]\n\\tag{4.3}\n\\end{equation}\\]\\(\\hat{\\beta}_1\\) \\(\\hat{\\beta}_0\\) follow normal distribution.Note (4.2) (4.3), use \\(s^2 = MS_{res}\\) estimate \\(\\sigma^2\\) since \\(\\sigma^2\\) unknown value.imply standardize \\(\\hat{\\beta}_1\\) \\(\\hat{\\beta}_0\\), standardized quantities follow \\(t_{n-2}\\) distribution, .e.\\[\\begin{equation}\n\\frac{\\hat{\\beta}_1 - \\beta_1}{se(\\hat{\\beta}_1)}\\sim t_{n-2}\n\\tag{4.4}\n\\end{equation}\\]\\[\\begin{equation}\n\\frac{\\hat{\\beta}_0 - \\beta_0}{se(\\hat{\\beta}_0)}\\sim t_{n-2},\n\\tag{4.5}\n\\end{equation}\\]\\[\\begin{equation}\nse(\\hat{\\beta}_1) = \\sqrt{\\frac{MS_{res}}{\\sum{(x_{}-\\bar{x})^{2}}}} = \\frac{s}{\\sqrt{\\sum{(x_{}-\\bar{x})^{2}}}}\n\\tag{4.6}\n\\end{equation}\\]\\[\\begin{equation}\nse(\\hat{\\beta}_0) = \\sqrt{MS_{res}\\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i -\\bar{x})^2}\\right]} = s \\sqrt{\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i -\\bar{x})^2}}\n\\tag{4.7}\n\\end{equation}\\]Note:\\(se(\\hat{\\beta}_1)\\) read standard error \\(\\hat{\\beta}_1\\). standard error estimator essentially sample standard deviation estimator, measures spread estimator.\\(se(\\hat{\\beta}_1)\\) read standard error \\(\\hat{\\beta}_1\\). standard error estimator essentially sample standard deviation estimator, measures spread estimator.\\(t_{n-2}\\) distribution read \\(t\\) distribution \\(n-2\\) degrees freedom.\\(t_{n-2}\\) distribution read \\(t\\) distribution \\(n-2\\) degrees freedom.","code":""},{"path":"inf.html","id":"testing-regression-coefficients","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.2.2 Testing regression coefficients","text":"Hypothesis testing used investigate population parameter different specific value. context SLR, usually want test \\(\\beta_1\\) 0 . \\(\\beta_1 = 0\\), linear relationship variables.general steps hypothesis testing :Step 1: State null alternative hypotheses.Step 2: test statistic calculated using sample, assuming null true. value test statistic measures sample deviates null.Step 3: Make conclusion, using either critical values p-values.previous module, introduced ANOVA \\(F\\) test. SLR, tests slope SLR equation 0 . turns can also perform \\(t\\) test slope. \\(t\\) test slope, null alternative hypotheses :\\[\nH_0: \\beta_1 = 0, H_a: \\beta_1 \\neq 0.\n\\]\ntest statistic \\[\\begin{equation}\nt = \\frac{\\hat{\\beta}_1 - \\text{ value null}}{se(\\hat{\\beta}_1)}\n\\tag{4.8}\n\\end{equation}\\]compared \\(t_{n-2}\\) distribution. Notice (4.8) comes (4.4).Let us go back simulated example saw last module. data 6000 UVa undergraduate students amount time spend studying week (minutes), many courses taking semester (3 4 credit courses).\\(t\\) statistic testing \\(H_0: \\beta_1 = 0, H_a: \\beta_1 \\neq 0\\) reported 255.7412482, can calculated using (4.8): \\(t= \\frac{120.39310 - 0}{0.4707614}\\). reported p-value virtually 0, reject null hypothesis. data support claim linear association study time number courses taken.","code":"\n##create dataframe\ndf<-data.frame(study,courses)\n\n##fit regression\nresult<-lm(study~courses, data=df)\n##look at regression coefficients\nsummary(result)$coefficients##              Estimate Std. Error   t value      Pr(>|t|)\n## (Intercept)  58.44829  1.9218752  30.41211 4.652442e-189\n## courses     120.39310  0.4707614 255.74125  0.000000e+00"},{"path":"inf.html","id":"confidence-intervals-for-regression-coefficients","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.3 Confidence Intervals for Regression Coefficients","text":"Confidence intervals (CIs) similar hypothesis testing sense also based distributional properties estimator. CIs may differ use following ways:assessing parameter different specific value.interested exploring plausible range values unknown parameter.CIs hypothesis tests based distributional properties estimator, conclusions consistent (long significance level ).Recall general form CIs:\\[\\begin{equation}\n\\mbox{estimator} \\pm (\\mbox{multiplier} \\times \\mbox{s.e estimator}).\n\\tag{4.9}\n\\end{equation}\\]following components CIestimator (statistic): numerical quantity describes samplemultiplier: determined confidence level relevant probability distributionstandard error estimator: measure variance estimator (basically square root variance estimator)Following (4.9) (4.4), \\(100(1-\\alpha)\\%\\) CI \\(\\beta_1\\) \\[\\begin{equation}\n\\hat{\\beta}_1 \\pm t_{1-\\alpha/2;n-2}  se(\\hat{\\beta}_1) = \\hat{\\beta}_1 \\pm t_{1-\\alpha/2;n-2} \\frac{s}{\\sqrt{\\sum(x_i - \\bar{x})^{2}}}.\n\\tag{4.10}\n\\end{equation}\\]Going back study time example, 95% CI \\(\\beta_1\\) (119.470237, 121.3159601).interpretation CI 95% confidence true slope \\(\\beta_1\\) lies (119.470237, 121.3159601). words, additional course taken, predicted study time increases 119.470237 121.3159601 minutes.","code":"\n##CI for coefficients\nconfint(result,level = 0.95)[2,]##    2.5 %   97.5 % \n## 119.4702 121.3160"},{"path":"inf.html","id":"thought-questions","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.3.1 Thought questions","text":"conclusion 95% CI consistent hypothesis test \\(H_0: \\beta_1 = 0\\) previous section 0.05 significance level?conclusion 95% CI consistent hypothesis test \\(H_0: \\beta_1 = 0\\) previous section 0.05 significance level?presented hypothesis tests CIs slope, \\(\\beta_1\\).\ncalculate \\(t\\) statistic wanted test \\(H_0: \\beta_0 = 0, H_0: \\beta_0 \\neq 0\\)?\ncalculate 95% CI intercept \\(\\beta_0\\)?\npresented hypothesis tests CIs slope, \\(\\beta_1\\).calculate \\(t\\) statistic wanted test \\(H_0: \\beta_0 = 0, H_0: \\beta_0 \\neq 0\\)?calculate \\(t\\) statistic wanted test \\(H_0: \\beta_0 = 0, H_0: \\beta_0 \\neq 0\\)?calculate 95% CI intercept \\(\\beta_0\\)?calculate 95% CI intercept \\(\\beta_0\\)?Generally, usually interested slope intercept.","code":""},{"path":"inf.html","id":"ci-of-the-mean-response","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.4 CI of the Mean Response","text":"established least squares estimators \\(\\hat{\\beta}_1,\\hat{\\beta}_0\\) associated variances. Since estimated SLR equation \\[\\begin{equation}\n\\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1 x,\n\\tag{4.11}\n\\end{equation}\\]stands reason \\(\\hat{y}\\) associated variance well, since function \\(\\hat{\\beta}_1,\\hat{\\beta}_0\\).two interpretations \\(\\hat{y}\\):estimates mean \\(y\\) \\(x=x_0\\);predicts value \\(y\\) new observation \\(x=x_0\\).Note: \\(x_0\\) denotes specific numerical value predictor variable.Depending interpretation want, two different intervals based \\(\\hat{y}\\). first interpretation associated confidence interval mean response, \\(\\hat{\\mu}_{y|x_0}\\), given predictor. used interested average value response variable, predictor equal specific value. CI \\[\\begin{equation}\n\\hat{\\mu}_{y|x_0}\\pm t_{1-\\alpha/2,n-2}s\\sqrt{\\frac{1}{n} +\n\\frac{(x_0-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}}.\n\\tag{4.12}\n\\end{equation}\\]Going back study time example, suppose want average study time students take 5 courses, 95% CI isWe 95% confidence average study time students take 5 courses 659.2223688 661.605187 minutes.","code":"\n##CI for mean y when x=5\nnewdata<-data.frame(courses=5)\npredict(result, newdata, level=0.95, interval=\"confidence\")##        fit      lwr      upr\n## 1 660.4138 659.2224 661.6052"},{"path":"inf.html","id":"pi-of-a-new-response","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.5 PI of a New Response","text":"Previously, found CI mean \\(y\\) given specific value \\(x\\), (4.12). CI gives us idea location regression line specific \\(x\\).Instead, may interest finding interval new value \\(\\hat{y}_0\\), new observation \\(x=x_0\\). called prediction interval (PI) future observation \\(y_0\\) predictor specific value. interval follows second interpretation \\(\\hat{y}\\).PI \\(\\hat{y}_0\\) takes account:Variation location distribution \\(y\\) (.e. center distribution \\(y\\)?).Variation within probability distribution \\(y\\).comparison, confidence interval mean response (4.12) takes account first element. PI \\[\\begin{equation}\n\\hat{y}_0\\pm t_{1-\\alpha/2,n-2}s \\sqrt{1+\\frac{1}{n} +\n\\frac{(x_0-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}}.\n\\tag{4.13}\n\\end{equation}\\]Going back study time example, suppose newly enrolled student wishes take 5 courses, student wants predict study timeWe 95% confidence study time student 602.0347305 718.7928253 minutes.","code":"\n##PI for y when x=5\npredict(result, newdata, level=0.95, interval=\"prediction\")##        fit      lwr      upr\n## 1 660.4138 602.0347 718.7928"},{"path":"inf.html","id":"thought-questions-1","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.5.1 Thought questions","text":"following two scenarios, decide interested CI mean response given predictor (4.12), PI future response given predictor (4.13).\nwish estimate waiting time, average, DMV customers 10 people line DMV.\nenter DMV notice 10 people line. want estimate waiting time.\nfollowing two scenarios, decide interested CI mean response given predictor (4.12), PI future response given predictor (4.13).wish estimate waiting time, average, DMV customers 10 people line DMV.wish estimate waiting time, average, DMV customers 10 people line DMV.enter DMV notice 10 people line. want estimate waiting time.enter DMV notice 10 people line. want estimate waiting time.Look standard errors associated intervals given (4.12) (4.13). related ?Look standard errors associated intervals given (4.12) (4.13). related ?","code":""},{"path":"inf.html","id":"supplemental-notes-on-statistical-inference","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6 Supplemental Notes on Statistical Inference","text":"","code":""},{"path":"inf.html","id":"hypothesis-statements","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6.1 Hypothesis statements","text":"Let’s consider \\(t\\) test regression parameter, \\(\\beta_1\\). Depending context, following null alternative hypotheses\\(H_0: \\beta_1 = 0, H_a: \\beta_1 \\neq 0\\).\\(H_0: \\beta_1 = 0, H_a: \\beta_1 > 0\\).\\(H_0: \\beta_1 = 0, H_a: \\beta_1 < 0\\).null hypothesis stated statement equality. concept holds true hypothesis tests general. books / resources might state \\(H_0: \\beta_1 = 0, H_a: \\beta_1 \\neq 0\\).\\(H_0: \\beta_1 \\leq 0, H_a: \\beta_1 > 0\\).\\(H_0: \\beta_1 \\geq 0, H_a: \\beta_1 < 0\\).prefer using equality statement null hypothesis following reasons (theoretical, pedagogical, practical):null hypothesis equality aligns definition p-value.p-value probability observing sample estimate (value extreme), null hypothesis true (.e. \\(\\beta_1\\) truly 0). assuming calculation test statistic.People tend get confused null alternative hypotheses involve inequalities (alternative hypothesis trying support).Conclusions made terms supporting (supporting) alternative hypothesis.","code":""},{"path":"inf.html","id":"sample-size-and-statistical-inference","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6.2 Sample size and statistical inference","text":"Generally speaking, relationship sample size statistical inference (assuming characteristics remain sample randomly obtained representative population interest):Larger sample sizes (typically) lead narrower confidence intervals (precise intervals).Sample estimates based larger samples likely closer true parameters.Larger sample (typically) lead evidence null hypothesis.\nmeans larger sample size leads powerful test. power test probability hypothesis test able correctly reject null hypothesis.\nmeans larger sample size leads powerful test. power test probability hypothesis test able correctly reject null hypothesis.","code":""},{"path":"inf.html","id":"small-sample-sizes","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6.2.1 Small sample sizes","text":"Small sample sizes tend result :Confidence intervals wide.Sample estimates likely away true parameters.Hypothesis tests likely incorrectly fail reject null hypothesis alternative hypothesis true.larger sample sizes advantages, also disadvantages sample sizes extremely large.","code":""},{"path":"inf.html","id":"large-sample-sizes","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6.2.2 Large sample sizes","text":"“statistically significant” result necessarily mean result practical consequences. Suppose 95% confidence interval \\(\\beta_1\\) \\((0.001, 0.002)\\). interval excludes 0, “statistically significantly” different 0 (!), result practical consequences? narrow CI barely excludes null value can happen large sample size.one conduct corresponding hypothesis test, reject null hypothesis \\(\\beta_1 = 0\\). large sample sizes, hypothesis tests sensitive small departures null hypothesis.instances, may worth considering hypothesis tests involving different value null hypothesis, one makes sense question. example, practically significant slope may need greater specific numerical value certain context.Statistical inference assess statistical significance.Subject area knowledge assess practical significance.","code":""},{"path":"inf.html","id":"questions","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6.2.3 Questions","text":"following results statistically significant? , results also practically significant? Assume two-sided test null value 0 (made examples):assessing studying associated better test scores, SLR carried test scores (100 points) study time (hours). 95% confidence interval slope \\(\\beta_1\\) (5.632, 7.829).assessing studying associated better test scores, SLR carried test scores (100 points) study time (hours). 95% confidence interval slope \\(\\beta_1\\) (5.632, 7.829).SLR carried explore linear relationship number years school income (thousands dollars). 95% confidence interval slope \\(\\beta_1\\) (0.051, 0.243).SLR carried explore linear relationship number years school income (thousands dollars). 95% confidence interval slope \\(\\beta_1\\) (0.051, 0.243).","code":""},{"path":"inf.html","id":"cautions-using-slr-and-correlation","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6.3 Cautions using SLR and Correlation","text":"Simple linear regression correlation meant assessing linear relationships. relationship linear, need transform variable(s) (transformed variables linear relationship. explore Module 5).Always verify via scatterplot relationship least approximately linear.high correlation significant estimated slope prove strong linear relationship variables. Conversely, correlation close 0 insignificant estimated slope also proof relationship variables.","code":""},{"path":"inf.html","id":"outliers","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6.3.1 Outliers","text":"SLR correlation sensitive outliers / influential observations. Generally speaking, data “far away” different rest observations. data points can visually inspected scatterplot. potential considerations dealing data points:Investigate observations. usually something making ``stand ” rest data.Data entry errors can corrected. sure mention report.Revisit data sampled. Perhaps data point part population interest. , data point can removed (legitimate), sure mention report.regards regression analysis:Exclusion data points must clearly documented.Fit regression without data points question, see similar different conclusions become.data points large value(s) predictor /response, log transformation variable can pull large values.Consider subsetting data create separate models subset; focus subset make clear analysis subset.Knowing data context can help lot decisions.","code":""},{"path":"inf.html","id":"association-and-causation","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6.3.2 Association and causation","text":"Two correlated variables mean one variable causes variable change. example, consider plot ice cream consumption deaths drowning various months. may positive correlation, clearly, eating ice cream cause drownings. correlation can explained third (lurking) variable: weather.lurking variable variable impact relationship variables studied, studied.carefully designed randomized experiment can control lurking variables, causal relationships can established. Typically, experiments include:control group treatment group.Random assignment large number observations treatment control groups. Due random assignment, general characteristics subjects group similar.Lurking variables always issue observational studies. Researchers observational studies intervene observations simply observe data observations generate. Causal relationships much difficult establish observational studies.","code":""},{"path":"inf.html","id":"questions-1","chapter":"4 Inference with Simple Linear Regression (SLR)","heading":"4.6.3.3 Questions","text":"Consider palmerpenguins dataset working . data contain size measurements three different species penguins three islands Palmer Archipelago, Antarctica three years. observational study randomized experiment?Consider palmerpenguins dataset working . data contain size measurements three different species penguins three islands Palmer Archipelago, Antarctica three years. observational study randomized experiment?fertilizer company wishes evaluate effective new fertilizer terms improving yield crops. large field divided many smaller plots, smaller plot randomly assigned receive either new fertilizer standard fertilizer. observational study randomized experiment?fertilizer company wishes evaluate effective new fertilizer terms improving yield crops. large field divided many smaller plots, smaller plot randomly assigned receive either new fertilizer standard fertilizer. observational study randomized experiment?professor wishes evaluate effectiveness various teaching methods (traditional vs flipped classroom). professor uses traditional approach section meets Mondays, Wednesdays, Fridays 9 10am uses flipped classroom approach section meets Mondays, Wednesdays, Fridays 2 3pm. Students free choose whichever section wanted register , knowledge teaching method used. potential lurking variables study?professor wishes evaluate effectiveness various teaching methods (traditional vs flipped classroom). professor uses traditional approach section meets Mondays, Wednesdays, Fridays 9 10am uses flipped classroom approach section meets Mondays, Wednesdays, Fridays 2 3pm. Students free choose whichever section wanted register , knowledge teaching method used. potential lurking variables study?","code":""},{"path":"diag.html","id":"diag","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5 Model Diagnostics and Remedial Measures in SLR","text":"","code":""},{"path":"diag.html","id":"introduction-4","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.1 Introduction","text":"regression model based number assumptions. assumptions made can apply commonly used probability distributions quantify variability associated estimated regression model. means assumptions met regression model, quantify variability associated model longer reliable. analysis statistical inference becomes questionable.module, learn assess whether regression assumptions met. explore ways can transform variables diagnosing assumptions met can still proceed build regression model.","code":""},{"path":"diag.html","id":"assumptions-in-linear-regression","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.2 Assumptions in Linear Regression","text":"module 3, stated SLR model \\[\\begin{equation}\ny=\\beta_0+\\beta_{1} x + \\epsilon.\n\\tag{5.1}\n\\end{equation}\\]\\(f(x) = \\beta_0 + \\beta_1 x\\). need make assumptions error term \\(\\epsilon\\). Mathematically, assumptions expressed \\[\\begin{equation}\n\\epsilon_1,\\ldots,\\epsilon_n \\ ..d. \\sim N(0,\\sigma^2)\n\\tag{5.2}\n\\end{equation}\\]Breaking (5.2) assumptions can expressed following:errors mean 0.errors constant variance denoted \\(\\sigma^2\\).errors independent.errors normally distributed.Let’s dig little deeper meaning implications 4 assumptions.","code":""},{"path":"diag.html","id":"assumption-1-errors-have-mean-0.","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.2.1 Assumption 1: Errors have mean 0.","text":"value predictor, errors mean 0. -product statement relationship \\(y\\) \\(x\\), expressed via \\(y \\approx f(x)\\), correct. , \\(f(x) = \\beta_0 + \\beta_1 x\\), relationship approximately linear.plots Figure  based simulated data. scatterplot shown Figure  example assumption met. move left right plot, data points generally evenly scattered sides regression line overlaid.scatterplot shown Figure  example assumption met. move left right plot Figure , data points generally evenly scattered sides regression line overlaid.\\(-2 \\leq x \\leq -1.2\\), data points generally regression line;\\(-1.2 < x < 1\\), data points generally regression line;\\(x \\geq 1\\), data points generally regression line.Please see associated video explanation use Figure  assess assumption 1. ","code":""},{"path":"diag.html","id":"consequences-of-violating-this-assumption","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.2.1.1 Consequences of violating this assumption","text":"Predictions biased. means predicted values systematically - - estimate true values response variable. 4 assumptions listed, crucial assumption.Using Figure  example, implies thatwhen \\(-2 \\leq x \\leq -1.2\\), regression line systematically -predict response variable;\\(-1.2 < x < 1\\), regression line systematically -predict response variable;\\(x \\geq 1\\), regression line systematically -predict response variable.","code":""},{"path":"diag.html","id":"assumption-2-errors-have-constant-variance","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.2.2 Assumption 2: Errors have constant variance","text":"value predictor, error terms constant variance, denoted \\(\\sigma^2\\). implies looking scatterplot, vertical variation data points around regression equation magnitude everywhere.plots Figure  based simulated data. scatterplot shown Figure  example assumption met (figure actually Figure , data produced plots satisfy assumptions). move left right plot, vertical variation data points regression line approximately constant.scatterplot shown Figure  example assumption met. move left right plot Figure , vertical variation data points regression line becomes larger value response variable gets larger, variance constant.Please see associated video explanation use Figure  assess assumption 2. ","code":""},{"path":"diag.html","id":"consequences-of-violating-this-assumption-1","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.2.2.1 Consequences of violating this assumption","text":"Statistical inference longer reliable. means results hypothesis test, confidence interval, prediction interval longer reliable.Interestingly, scatterplot Figure , can say assumption 1 met, since data points generally evenly scattered sides regression line. Predictions still unbiased; predicted response, \\(\\hat{y}\\), systematically - -predict response variable. goal assess relationship approximately linear, scatterplot fine. lose utility hypothesis tests, CIs, PIs.","code":""},{"path":"diag.html","id":"assumption-3-errors-are-independent","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.2.3 Assumption 3: Errors are independent","text":"-product assumption values response variable, \\(y_i\\), independent . \\(y_i\\) depend values response variable.","code":""},{"path":"diag.html","id":"consequences-of-violating-this-assumption-2","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.2.3.1 Consequences of violating this assumption","text":"Statistical inference longer reliable. means results hypothesis test, confidence interval, prediction interval longer reliable.","code":""},{"path":"diag.html","id":"assumption-4-errors-are-normally-distributed","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.2.4 Assumption 4: Errors are normally distributed","text":"create density plot errors, errors follow normal distribution.","code":""},{"path":"diag.html","id":"consequences-of-violating-this-assumption-3","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.2.4.1 Consequences of violating this assumption","text":"regression model fairly robust assumption errors normally distributed. words, violation particular assumption consequential. 4 assumptions, least crucial satisfy.","code":""},{"path":"diag.html","id":"assessing-regression-assumptions","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.3 Assessing Regression Assumptions","text":"visualizations help detecting violations regression assumptions. visualizations :Scatterplot \\(y\\) \\(x\\) (assumptions 1 2).Residual plot (assumptions 1 2).Autocorrelation function (ACF) plot residuals (assumption 3).Normal probability plot residuals (often called QQ plot) (assumption 4).","code":""},{"path":"diag.html","id":"scatterplot","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.3.1 Scatterplot","text":"can examine scatterplot \\(y\\) \\(x\\) check assumptions 1 2. want see following scatterplot:nonlinear pattern (assumption 1).Data points evenly scattered (value x-axis) around fitted line (assumption 1).Vertical variation data points constant (assumption 2).used Figure  example scatterplot meets assumptions. Let us take look another example worked . scatterplot elmhurst dataset openintro package seeing tutorials. regressing amount gift aid student receives based student’s family income. corresponding scatterplot shown Figure 5.1.\nFigure 5.1: Scatterplot Gift Aid Family Income\nFigure 5.1, see data points evenly scattered around fitted line. also see vertical variation data points fairly constant. assumptions errors 0 mean constant variance appear met.","code":""},{"path":"diag.html","id":"practice-question","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.3.1.1 Practice question","text":"data prices used cars. regressing sale price car age car. corresponding scatterplot shown Figure 5.2. Based Figure 5.2, assumptions 1 2 (, neither), met? go tutorial.\nFigure 5.2: Scatterplot Sale Price Age\n","code":""},{"path":"diag.html","id":"residual-plot","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.3.2 Residual plot","text":"using scatterplot intuitive way assessing regression assumptions, limitation. used multiple predictors regression, encounter (happens often just one predictor). Another visualization can use assess assumptions 1 2 residual plot. scatterplot residuals, \\(e\\), fitted values, \\(\\hat{y}\\). want observe following residual plot.Residuals evenly scattered across horizontal axis (assumption 1).residuals similar vertical variation across plot (assumption 2).writers combine two points following statement: residuals fall horizontal band around 0 apparent pattern (assumption 1, 2).residual plots Figure  based simulated data Figures , , .make following observations:\n- Figure , see residuals evenly scattered across horizontal axis, vertical variation fairly constant across plot. assumptions met.\n- Figure , see residuals evenly scattered across horizontal axis, although vertical variation fairly constant across plot. assumption 1 met.\n- Figure , see residuals evenly scattered across horizontal axis, vertical variation constant across plot. fact, vertical variation increasing move left right. assumption 2 met.compare conclusions residuals plots scatterplots, . SLR, takeaways consistent.Please see associated video explanation use Figure  assess assumptions 1 2. ","code":""},{"path":"diag.html","id":"practice-questions-2","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.3.2.1 Practice questions","text":"residual plot Figure 5.3 comes regressing gift aid family income elmhurst dataset. Based residual plot, assumptions met?\nFigure 5.3: Residual Plot Regressing Gift Aid Family Income\nresidual plot Figure 5.4 comes regressing price cars age used cars dataset. Based residual plot, assumptions met?\nFigure 5.4: Residual Plot Regressing Price Age Used Cars\nPlease see associated video go practice questions.","code":""},{"path":"diag.html","id":"acf-plot","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.3.3 ACF plot","text":"Assumption 3 states errors independent. assumption implies values response variable independent . assumption typically assessed via knowing nature data.observations obtained random sample, likely observations independent . nature random sample random samples preferred convenience samples.observations obtained random sample, likely observations independent . nature random sample random samples preferred convenience samples.data inherent sequence, likely observations independent, dependent. example, record value stock end day, value day 2 likely related value day 1. values stock prices end day independent.data inherent sequence, likely observations independent, dependent. example, record value stock end day, value day 2 likely related value day 1. values stock prices end day independent.autocorrelation function (ACF) plot residuals may used help assess assumption errors independent met. However, plot substitute using understanding nature data used confirmation.ACF plot measures correlation vector observations lagged versions observations. observations uncorrelated, correlations vector observations lagged versions observations theoretically 0. may create ACF plot residuals regression.ACF plot Figure  based simulated data independently generated.notes ACF plot:ACF lag 0 always 1. correlation vector always 1.dashed horizontal lines represent critical values. ACF lag beyond critical value indicates ACF significant. evidence correlation (hence dependence) residuals.observed values response variable independent, expect ACFs lags greater 0 insignificant. note conducting multiple hypothesis tests, alarmed ACFs slightly beyond critical values isolated lag 2.Based Figure , see ACFs lags greater 0 insignificant. evidence residuals correlated , evidence assumption 3 met.Sometimes, dataframe can sorted manner (e.g. increasing order response variable), , actually expect see significant correlations ACF plot. ACF plot Figure  example. residuals simulated dataset, data sorted response variable. just looked ACF plot Figure  without understanding data simulated independently sorted, erroneously concluded residuals independent regression assumption met.","code":""},{"path":"diag.html","id":"qq-plot","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.3.4 QQ plot","text":"normal probability plot (also called QQ plot) used assess distribution variable normal. typically plots residuals theoretical residual followed normal distribution. QQ line typically overlaid. plots fall closely QQ line, evidence observations follow normal distribution. Figure 5.5 shows QQ plot comes normally distributed variable.\nFigure 5.5: QQ Plot\n","code":""},{"path":"diag.html","id":"remedial-measures","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.3.5 Remedial measures","text":"now know assess specific regression assumptions met. remedial measures involve transforming either predictor variable / response variable. transformations chosen handle violations assumptions 1 / 2 respectively. general strategy selecting variable transform:Transforming response variable, \\(y\\), affects assumptions 1 2.\nVisually, can think transforming \\(y\\) terms stretching squeezing scatterplot \\(y\\) \\(x\\) vertically. Thus, transforming \\(y\\) affects shape relationship vertical spread data points.\nHowever, choice transform \\(y\\) based handling assumption 2.\nVisually, can think transforming \\(y\\) terms stretching squeezing scatterplot \\(y\\) \\(x\\) vertically. Thus, transforming \\(y\\) affects shape relationship vertical spread data points.However, choice transform \\(y\\) based handling assumption 2.Transforming predictor variable, \\(x\\) affects assumption 1 theoretically affect assumption 2.\nVisually, can think transforming \\(x\\) terms stretching squeezing scatterplot \\(y\\) \\(x\\) horizontally. Thus, transforming \\(x\\) affects shape relationship vertical spread data points.\nTherefore, transforming \\(x\\) based handling assumption 1.\nVisually, can think transforming \\(x\\) terms stretching squeezing scatterplot \\(y\\) \\(x\\) horizontally. Thus, transforming \\(x\\) affects shape relationship vertical spread data points.Therefore, transforming \\(x\\) based handling assumption 1.assumption 2 met, transform \\(y\\) stabilize variance make constant.assumption 1 met, transform \\(x\\) find appropriate shape relate variables.assumptions met, transform \\(y\\) first stabilize variance. assumption 2 solved, check assumption 1 met. met, transform \\(x\\).Assumption 1 deals whether way expressed \\(y\\) \\(x\\) related, \\(f(x)\\), appropriate. Assumption 2 deals vertical variation data points scatterplot.","code":""},{"path":"diag.html","id":"remedial-measures-variance-stabilizing-transformations","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.4 Remedial Measures: Variance Stabilizing Transformations","text":"transform response variable stabilize variance (assumption 2). couple ways decide appropriate transformation:Pattern seen residual plot can guide choice transform response variable.Box-Cox plot.","code":""},{"path":"diag.html","id":"use-pattern-in-residual-plot","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.4.1 Use Pattern in Residual Plot","text":"can stabilize variance errors based residual plot, see either following scenarios:vertical variation residuals increasing fitted response increases, move left right, Figure , orvertical variation residuals decreasing fitted response increases, move left right, Figure .Note increasing variance fitted response increases much common real data. Generally, larger values variable associated larger spread.transform \\(y\\) using \\(y^{*} = y^{\\lambda}\\), \\(\\lambda\\) chosen based whether variance residuals increasing decreasing fitted response:Figure , choose \\(\\lambda < 1\\).\n\\(\\lambda = 0\\), means use logarithmic transformation base e, .e. \\(y^* = \\log(y)\\).\nNote logarithm base means natural log, ln.\n\\(\\lambda = 0\\), means use logarithmic transformation base e, .e. \\(y^* = \\log(y)\\).Note logarithm base means natural log, ln.Figure , choose \\(\\lambda > 1\\).based residual plot, range values \\(\\lambda\\).","code":""},{"path":"diag.html","id":"box-cox-plot","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.4.2 Box-Cox Plot","text":"can use Box-Cox plot help us narrow range \\(\\lambda\\) use. plot log-likelihood function \\(\\lambda\\), choose \\(\\lambda\\) maximizes log-likelihood function. example, Figure  shows Box Cox plot generated regression associated residual plot Figure .Notice approximate 95% CI provided \\(\\lambda\\). comments use Box-Cox plot:Three vertical dashed lines displayed: middle line corresponds optimal value \\(\\lambda\\); two lines lower upper bounds 95% CI \\(\\lambda\\).choose \\(\\lambda\\) within CI (even close ) easy understand. choose optimal value, especially value difficult interpret. example, choose \\(\\lambda = 2\\), square transformation \\(y\\). Transform response \\(y^* = y^2\\). Regress \\(y^*\\) \\(x\\).1 lies CI, transformation \\(y\\) may needed.transformation needed, log transformation preferred, since can still interpret estimated coefficients. difficult interpret type transformation.View Box-Cox procedure guide selecting transformation, rather definitive.Need recheck residuals every transformation assess transformation worked.","code":""},{"path":"diag.html","id":"interpretation-with-log-transformed-response","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.4.3 Interpretation with Log Transformed Response","text":"log transformation response preferred transformation, can still interpret regression coefficients. couple ways interpret estimated slope \\(\\hat{\\beta}_1\\):predicted response variable multiplied factor \\(\\exp(\\hat{\\beta_1})\\) one-unit increase predictor.can also subtract 1 \\(\\exp(\\hat{\\beta_1})\\) express change percentage.\n\\(\\hat{\\beta}_1\\) positive, percent increase. predicted response variable increases \\((\\exp(\\hat{\\beta_1}) - 1) \\times 100\\) percent one-unit increase predictor.\n\\(\\hat{\\beta}_1\\) negative, percent decrease. predicted response variable decreases \\((1 - \\exp(\\hat{\\beta_1})) \\times 100\\) percent one-unit increase predictor.\n\\(\\hat{\\beta}_1\\) positive, percent increase. predicted response variable increases \\((\\exp(\\hat{\\beta_1}) - 1) \\times 100\\) percent one-unit increase predictor.\\(\\hat{\\beta}_1\\) negative, percent decrease. predicted response variable decreases \\((1 - \\exp(\\hat{\\beta_1})) \\times 100\\) percent one-unit increase predictor.Please see associated video go math explaining interpret regression coefficients response variable log transformed.","code":""},{"path":"diag.html","id":"remedial-measures-linearization-transformations","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.5 Remedial Measures: Linearization Transformations","text":"first ensure variance stabilized assumption 2 met. \\(f(x)\\) accurately capture relationship variables, transform predictor variable meet assumption 1. writers call linearization transformation, seek make transformed version predictor variable, \\(x^*\\), approximately linear response variable (transformed \\(y\\)), .e. \\(y = \\beta_0 + \\beta_1 x^* + \\epsilon\\). consider transforming response variable deal assumption 1, transforming response variable likely reintroduce violation assumption 2.general strategy transform predictor via scatterplot \\(y\\) (\\(y^*\\)) \\(x\\). use pattern seen plot decide transform predictor. examples shown Figure  .","code":""},{"path":"diag.html","id":"hierarchical-principle","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.5.1 Hierarchical Principle","text":"One thing aware hierarchical principle: relationship response predictor higher order polynomial (e.g. quadratic, cubic), hierarchical principle states lower order terms remain model. example, relationship order \\(h\\), fit \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_h x^h + \\epsilon\\) via multiple linear regression framework. see next module.","code":""},{"path":"diag.html","id":"interpretation-with-log-transformed-predictor","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.5.2 Interpretation with Log Transformed Predictor","text":"log transformation predictor preferred transformation, can still interpret regression coefficient, \\(\\hat{\\beta}_1\\), couple ways:\\(\\%\\) increase predictor, predicted response increases \\(\\hat{\\beta}_1 \\log(1+ \\frac{}{100})\\).\\(\\log(1 + \\frac{1}{100}) \\approx \\frac{1}{100}\\) (Taylor series). alternative interpretation : 1% increase predictor, predicted response increases approximately \\(\\frac{\\hat{\\beta}_1}{100}\\).Please see associated video go math explaining interpret regression coefficients predictor variable log transformed.","code":""},{"path":"diag.html","id":"interpretation-with-log-transformed-response-and-predictor","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.5.3 Interpretation with Log Transformed Response and Predictor","text":"response predictor variables log transformed, regression coefficient, \\(\\hat{\\beta}_1\\), can interpreted couple ways:\\(\\%\\) increase predictor, predicted response multiplied \\((1 + \\frac{}{100})^{\\hat{\\beta}_1}\\).\\(\\%\\) increase predictor, predicted response multiplied \\((1 + \\frac{}{100})^{\\hat{\\beta}_1}\\).\\((1 + \\frac{1}{100})^{\\hat{\\beta}_1} \\approx 1 + \\frac{\\hat{\\beta}_1}{100}\\) (Taylor series). alternative interpretation : 1% increase predictor, predicted response increases approximately \\(\\hat{\\beta}_1\\) percent. Note approximation works better \\({\\hat{\\beta}_1}\\) small magnitude.\\((1 + \\frac{1}{100})^{\\hat{\\beta}_1} \\approx 1 + \\frac{\\hat{\\beta}_1}{100}\\) (Taylor series). alternative interpretation : 1% increase predictor, predicted response increases approximately \\(\\hat{\\beta}_1\\) percent. Note approximation works better \\({\\hat{\\beta}_1}\\) small magnitude.Please see associated video go math explaining interpret regression coefficients response predictor variables log transformed.","code":""},{"path":"diag.html","id":"some-general-comments-about-assessing-assumptions-and-transformations","chapter":"5 Model Diagnostics and Remedial Measures in SLR","heading":"5.5.4 Some General Comments about Assessing Assumptions and Transformations","text":"assessing assumptions residual plot, assessing assumptions reasonably / approximately met.assessing assumptions residual plot, assessing assumptions reasonably / approximately met.real data, assumptions rarely met 100%.real data, assumptions rarely met 100%.unsure, proceed model building, test model performs new data. poor performance, go back residual plot assess transformation appropriate.unsure, proceed model building, test model performs new data. poor performance, go back residual plot assess transformation appropriate.Assess plots decide variables need transformed, . choice transformation guided see plots, trial error.Assess plots decide variables need transformed, . choice transformation guided see plots, trial error.residual plot always produced transformation. Box Cox plot also produced. plots assessed transformation helped way intended.residual plot always produced transformation. Box Cox plot also produced. plots assessed transformation helped way intended.Solve assumption 2 first, assumption 1.Solve assumption 2 first, assumption 1.","code":""},{"path":"try.html","id":"try","chapter":"6 TABLE","heading":"6 TABLE","text":"","code":""},{"path":"try.html","id":"introduction-5","chapter":"6 TABLE","heading":"6.1 Introduction","text":"","code":""},{"path":"intro.html","id":"intro","chapter":"7 Introduction","heading":"7 Introduction","text":"can label chapter section titles using {#label} , e.g., can reference Chapter 7. manually label , automatic labels anyway, e.g., Chapter 9.Figures tables captions placed figure table environments, respectively.\nFigure 7.1: nice figure!\nReference figure code chunk label fig: prefix, e.g., see Figure 7.1. Similarly, can reference tables generated knitr::kable(), e.g., see Table 7.1.Table 7.1: nice table!can write citations, . example, using bookdown package (Xie 2023) sample book, built top R Markdown knitr (Xie 2015).","code":"\npar(mar = c(4, 4, .1, .1))\nplot(pressure, type = 'b', pch = 19)\nknitr::kable(\n  head(iris, 20), caption = 'Here is a nice table!',\n  booktabs = TRUE\n)"},{"path":"literature.html","id":"literature","chapter":"8 Literature","heading":"8 Literature","text":"review existing methods.","code":""},{"path":"literature.html","id":"new-section","chapter":"8 Literature","heading":"8.1 New section","text":"Add text . BLAH BLAH","code":""},{"path":"methods.html","id":"methods","chapter":"9 Methods","heading":"9 Methods","text":"describe methods chapter.Math can added body using usual syntax like ","code":""},{"path":"methods.html","id":"math-example","chapter":"9 Methods","heading":"9.1 math example","text":"\\(p\\) unknown expected around 1/3. Standard error approximated\\[\nSE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027\n\\]can also use math footnotes like this1.approximate standard error 0.0272","code":""},{"path":"applications.html","id":"applications","chapter":"10 Applications","heading":"10 Applications","text":"significant applications demonstrated chapter.","code":""},{"path":"applications.html","id":"example-one","chapter":"10 Applications","heading":"10.1 Example one","text":"","code":""},{"path":"applications.html","id":"example-two","chapter":"10 Applications","heading":"10.2 Example two","text":"","code":""},{"path":"final-words.html","id":"final-words","chapter":"11 Final Words","heading":"11 Final Words","text":"finished nice book.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
