<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 5 Model Diagnostics and Remedial Measures in SLR | Linear Models for Data Science</title>
<meta name="author" content="Jeffrey Woo">
<meta name="description" content="5.1 Introduction The regression model is based on a number of assumptions. Those assumptions are made so that we can apply commonly used probability distributions to we quantify the variability...">
<meta name="generator" content="bookdown 0.34 with bs4_book()">
<meta property="og:title" content="Chapter 5 Model Diagnostics and Remedial Measures in SLR | Linear Models for Data Science">
<meta property="og:type" content="book">
<meta property="og:description" content="5.1 Introduction The regression model is based on a number of assumptions. Those assumptions are made so that we can apply commonly used probability distributions to we quantify the variability...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 5 Model Diagnostics and Remedial Measures in SLR | Linear Models for Data Science">
<meta name="twitter:description" content="5.1 Introduction The regression model is based on a number of assumptions. Those assumptions are made so that we can apply commonly used probability distributions to we quantify the variability...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Linear Models for Data Science</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="wrangling.html"><span class="header-section-number">1</span> Data Wrangling with R</a></li>
<li><a class="" href="viz.html"><span class="header-section-number">2</span> Data Visualization with R Using ggplot2</a></li>
<li><a class="" href="slr.html"><span class="header-section-number">3</span> Basics with Simple Linear Regression (SLR)</a></li>
<li><a class="" href="inf.html"><span class="header-section-number">4</span> Inference with Simple Linear Regression (SLR)</a></li>
<li><a class="active" href="diag.html"><span class="header-section-number">5</span> Model Diagnostics and Remedial Measures in SLR</a></li>
<li><a class="" href="mlr.html"><span class="header-section-number">6</span> Multiple Linear Regression (MLR)</a></li>
<li><a class="" href="genF.html"><span class="header-section-number">7</span> General Linear \(F\) Test and Multicollinearity</a></li>
<li><a class="" href="cat.html"><span class="header-section-number">8</span> Categorical Predictors in MLR</a></li>
<li><a class="" href="crit.html"><span class="header-section-number">9</span> Model Selection Criteria and Automated Search Procedures</a></li>
<li><a class="" href="out.html"><span class="header-section-number">10</span> Analysis of Residuals in MLR</a></li>
<li><a class="" href="logistic1.html"><span class="header-section-number">11</span> Logistic Regression</a></li>
<li><a class="" href="logistic2.html"><span class="header-section-number">12</span> Logistic Regression 2</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="diag" class="section level1" number="5">
<h1>
<span class="header-section-number">5</span> Model Diagnostics and Remedial Measures in SLR<a class="anchor" aria-label="anchor" href="#diag"><i class="fas fa-link"></i></a>
</h1>
<div id="introduction-4" class="section level2" number="5.1">
<h2>
<span class="header-section-number">5.1</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction-4"><i class="fas fa-link"></i></a>
</h2>
<p>The regression model is based on a number of assumptions. Those assumptions are made so that we can apply commonly used probability distributions to we quantify the variability associated with our estimated regression model. This means that if the assumptions are not met for our regression model, then how we quantify the variability associated with our model is no longer reliable. All our analysis with statistical inference becomes questionable.</p>
<p>In this module, you will learn how to assess whether the regression assumptions are met. We will explore ways in which we can transform our variables after diagnosing which assumptions are not met so that we can still proceed to build our regression model.</p>
</div>
<div id="assumptions-in-linear-regression" class="section level2" number="5.2">
<h2>
<span class="header-section-number">5.2</span> Assumptions in Linear Regression<a class="anchor" aria-label="anchor" href="#assumptions-in-linear-regression"><i class="fas fa-link"></i></a>
</h2>
<p>In module <a href="slr.html#slr">3</a>, we stated the SLR model as</p>
<p><span class="math display" id="eq:5SLRmod">\[\begin{equation}
y=\beta_0+\beta_{1} x + \epsilon.
\tag{5.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(f(x) = \beta_0 + \beta_1 x\)</span>. We need to make some assumptions for the error term <span class="math inline">\(\epsilon\)</span>. Mathematically, the assumptions are expressed as</p>
<p><span class="math display" id="eq:5assumptions">\[\begin{equation}
\epsilon_1,\ldots,\epsilon_n \ i.i.d. \sim N(0,\sigma^2)
\tag{5.2}
\end{equation}\]</span></p>
<p>Breaking down <a href="diag.html#eq:5assumptions">(5.2)</a> the assumptions can be expressed as the following:</p>
<ol style="list-style-type: decimal">
<li>The errors have <strong>mean 0</strong>.</li>
<li>The errors have <strong>constant variance denoted by <span class="math inline">\(\sigma^2\)</span></strong>.</li>
<li>The errors are <strong>independent</strong>.</li>
<li>The errors are <strong>normally distributed</strong>.</li>
</ol>
<p>Let’s dig a little deeper into the meaning and implications of these 4 assumptions.</p>
<div id="assumption-1-errors-have-mean-0." class="section level3" number="5.2.1">
<h3>
<span class="header-section-number">5.2.1</span> Assumption 1: Errors have mean 0.<a class="anchor" aria-label="anchor" href="#assumption-1-errors-have-mean-0."><i class="fas fa-link"></i></a>
</h3>
<p>For each value of the predictor, the errors have <strong>mean 0</strong>. A by-product of this statement is that the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>, as expressed via <span class="math inline">\(y \approx f(x)\)</span>, is correct. So, if <span class="math inline">\(f(x) = \beta_0 + \beta_1 x\)</span>, then the relationship is approximately linear.</p>
<p>The plots in Figure <a href="diag.html#fig:ass1">5.1</a> are based on simulated data. The scatterplot shown in Figure <a href="diag.html#fig:ass1">5.1</a>(a) is an example of when this assumption is met. As we move from left to right on the plot, the data points are generally evenly scattered on both sides of the regression line that is overlaid.</p>
<div class="figure">
<span style="display:block;" id="fig:ass1"></span>
<img src="images/ass1.jpg" alt="Assumption 1"><p class="caption">
Figure 5.1: Assumption 1
</p>
</div>
<p>The scatterplot shown in Figure <a href="diag.html#fig:ass1">5.1</a>(b) is an example of when this assumption is <strong>not</strong> met. As we move from left to right on the plot in Figure <a href="diag.html#fig:ass1">5.1</a>(b), the data points are generally not evenly scattered on both sides of the regression line that is overlaid.</p>
<ul>
<li>When <span class="math inline">\(-2 \leq x \leq -1.2\)</span>, the data points are generally above the regression line;</li>
<li>then when <span class="math inline">\(-1.2 &lt; x &lt; 1\)</span>, the data points are generally below the regression line;</li>
<li>and then when <span class="math inline">\(x \geq 1\)</span>, the data points are generally above the regression line.</li>
</ul>
<p><em>Please see the associated video for more explanation on how to use Figure <a href="diag.html#fig:ass1">5.1</a> to assess assumption 1.</em></p>
<div id="consequences-of-violating-this-assumption" class="section level4" number="5.2.1.1">
<h4>
<span class="header-section-number">5.2.1.1</span> Consequences of violating this assumption<a class="anchor" aria-label="anchor" href="#consequences-of-violating-this-assumption"><i class="fas fa-link"></i></a>
</h4>
<p><strong>Predictions will be biased</strong>. This means that predicted values will systematically over- or under- estimate the true values of the response variable. Of the 4 assumptions listed, this is <strong>most crucial assumption</strong>.</p>
<p>Using Figure <a href="diag.html#fig:ass1">5.1</a>(b) as an example, this implies that</p>
<ul>
<li>when <span class="math inline">\(-2 \leq x \leq -1.2\)</span>, the regression line will systematically under-predict the response variable;</li>
<li>then when <span class="math inline">\(-1.2 &lt; x &lt; 1\)</span>, the regression line will systematically over-predict the response variable;</li>
<li>and then when <span class="math inline">\(x \geq 1\)</span>, the regression line will systematically under-predict the response variable.</li>
</ul>
</div>
</div>
<div id="assumption-2-errors-have-constant-variance" class="section level3" number="5.2.2">
<h3>
<span class="header-section-number">5.2.2</span> Assumption 2: Errors have constant variance<a class="anchor" aria-label="anchor" href="#assumption-2-errors-have-constant-variance"><i class="fas fa-link"></i></a>
</h3>
<p>For each value of the predictor, the error terms have <strong>constant variance</strong>, denoted by <span class="math inline">\(\sigma^2\)</span>. This implies that when looking at a scatterplot, the vertical variation of data points around the regression equation has the same magnitude everywhere.</p>
<p>The plots in Figure <a href="diag.html#fig:ass2">5.2</a> are based on simulated data. The scatterplot shown in Figure <a href="diag.html#fig:ass2">5.2</a>(a) is an example of when this assumption is met (this figure is actually the same as Figure <a href="diag.html#fig:ass1">5.1</a>(a), so the data that produced these plots satisfy both assumptions). As we move from left to right on the plot, the vertical variation of the data points about the regression line is approximately constant.</p>
<div class="figure">
<span style="display:block;" id="fig:ass2"></span>
<img src="images/ass2.jpg" alt="Assumption 2"><p class="caption">
Figure 5.2: Assumption 2
</p>
</div>
<p>The scatterplot shown in Figure <a href="diag.html#fig:ass2">5.2</a>(b) is an example of when this assumption is <strong>not</strong> met. As we move from left to right on the plot in Figure <a href="diag.html#fig:ass2">5.2</a>(b), the vertical variation of the data points about the regression line becomes larger as the value of the response variable gets larger, so the variance is not constant.</p>
<p><em>Please see the associated video for more explanation on how to use Figure <a href="diag.html#fig:ass2">5.2</a> to assess assumption 2.</em></p>
<div id="consequences-of-violating-this-assumption-1" class="section level4" number="5.2.2.1">
<h4>
<span class="header-section-number">5.2.2.1</span> Consequences of violating this assumption<a class="anchor" aria-label="anchor" href="#consequences-of-violating-this-assumption-1"><i class="fas fa-link"></i></a>
</h4>
<p><strong>Statistical inference will no longer be reliable.</strong> This means that the results from any hypothesis test, confidence interval, or prediction interval are no longer reliable.</p>
<p>Interestingly, for the scatterplot in Figure <a href="diag.html#fig:ass2">5.2</a>(b), we can say that assumption 1 is met, since the the data points are generally evenly scattered on both sides of the regression line. Predictions will still be unbiased; the predicted response, <span class="math inline">\(\hat{y}\)</span>, do not systematically over- or under-predict the response variable. So if our goal is to assess if the relationship is approximately linear, this scatterplot is fine. We do lose the utility from hypothesis tests, CIs, and PIs.</p>
</div>
</div>
<div id="assumption-3-errors-are-independent" class="section level3" number="5.2.3">
<h3>
<span class="header-section-number">5.2.3</span> Assumption 3: Errors are independent<a class="anchor" aria-label="anchor" href="#assumption-3-errors-are-independent"><i class="fas fa-link"></i></a>
</h3>
<p>A by-product of this assumption is that the values of the response variable, <span class="math inline">\(y_i\)</span>, are independent from each other. Any <span class="math inline">\(y_i\)</span> does not depend on other values of the response variable.</p>
<div id="consequences-of-violating-this-assumption-2" class="section level4" number="5.2.3.1">
<h4>
<span class="header-section-number">5.2.3.1</span> Consequences of violating this assumption<a class="anchor" aria-label="anchor" href="#consequences-of-violating-this-assumption-2"><i class="fas fa-link"></i></a>
</h4>
<p><strong>Statistical inference will no longer be reliable.</strong> This means that the results from any hypothesis test, confidence interval, or prediction interval are no longer reliable.</p>
</div>
</div>
<div id="assumption-4-errors-are-normally-distributed" class="section level3" number="5.2.4">
<h3>
<span class="header-section-number">5.2.4</span> Assumption 4: Errors are normally distributed<a class="anchor" aria-label="anchor" href="#assumption-4-errors-are-normally-distributed"><i class="fas fa-link"></i></a>
</h3>
<p>If we were to create a density plot of the errors, the errors should follow a normal distribution.</p>
<div id="consequences-of-violating-this-assumption-3" class="section level4" number="5.2.4.1">
<h4>
<span class="header-section-number">5.2.4.1</span> Consequences of violating this assumption<a class="anchor" aria-label="anchor" href="#consequences-of-violating-this-assumption-3"><i class="fas fa-link"></i></a>
</h4>
<p>The regression model is fairly robust to the assumption that the errors are normally distributed. In other words, violation of this particular assumption is not very consequential. <strong>Of the 4 assumptions, this is the least crucial to satisfy.</strong></p>
</div>
</div>
</div>
<div id="assessing-regression-assumptions" class="section level2" number="5.3">
<h2>
<span class="header-section-number">5.3</span> Assessing Regression Assumptions<a class="anchor" aria-label="anchor" href="#assessing-regression-assumptions"><i class="fas fa-link"></i></a>
</h2>
<p>There are a few visualizations that help in detecting violations of the regression assumptions. These visualizations are:</p>
<ul>
<li>Scatterplot of <span class="math inline">\(y\)</span> against <span class="math inline">\(x\)</span> (assumptions 1 and 2).</li>
<li>Residual plot (assumptions 1 and 2).</li>
<li>Autocorrelation function (ACF) plot of residuals (assumption 3).</li>
<li>Normal probability plot of residuals (often called QQ plot) (assumption 4).</li>
</ul>
<div id="scatterplot" class="section level3" number="5.3.1">
<h3>
<span class="header-section-number">5.3.1</span> Scatterplot<a class="anchor" aria-label="anchor" href="#scatterplot"><i class="fas fa-link"></i></a>
</h3>
<p>We can examine the scatterplot of <span class="math inline">\(y\)</span> against <span class="math inline">\(x\)</span> to check for assumptions 1 and 2. We want to see the following in the scatterplot:</p>
<ul>
<li>
<strong>No nonlinear pattern</strong> (assumption 1).</li>
<li>Data points <strong>evenly scattered</strong> (for each value on the x-axis) around fitted line (assumption 1).</li>
<li>Vertical variation of data points constant (assumption 2).</li>
</ul>
<p>We have used Figure <a href="diag.html#fig:ass2">5.2</a>(a) as an example of a scatterplot that meets these assumptions. Let us take a look at another example that we have worked with. This scatterplot is from the <code>elmhurst</code> dataset from the <code>openintro</code> package that we have been seeing in tutorials. We are regressing the amount of gift aid a student receives based on the student’s family income. The corresponding scatterplot is shown in in Figure <a href="diag.html#fig:elmhurst">5.3</a>.</p>
<div class="figure">
<span style="display:block;" id="fig:elmhurst"></span>
<img src="bookdown-demo_files/figure-html/elmhurst-1.png" alt="Scatterplot of Gift Aid Against Family Income" width="672"><p class="caption">
Figure 5.3: Scatterplot of Gift Aid Against Family Income
</p>
</div>
<p>In Figure <a href="diag.html#fig:elmhurst">5.3</a>, we see that the data points are evenly scattered around the fitted line. We also see the vertical variation of the data points is fairly constant. So assumptions that the errors have 0 mean and constant variance appear to be met.</p>
<div id="practice-question" class="section level4" number="5.3.1.1">
<h4>
<span class="header-section-number">5.3.1.1</span> Practice question<a class="anchor" aria-label="anchor" href="#practice-question"><i class="fas fa-link"></i></a>
</h4>
<p>The data are about the prices of used cars. We are regressing the sale price of the car against the age of the car. The corresponding scatterplot is shown in Figure <a href="diag.html#fig:mazda">5.4</a>. Based on Figure <a href="diag.html#fig:mazda">5.4</a>, which of assumptions 1 or 2 (or both, or neither), is met? We will go over this in the tutorial.</p>
<div class="figure">
<span style="display:block;" id="fig:mazda"></span>
<img src="bookdown-demo_files/figure-html/mazda-1.png" alt="Scatterplot of Sale Price Against Age" width="672"><p class="caption">
Figure 5.4: Scatterplot of Sale Price Against Age
</p>
</div>
</div>
</div>
<div id="residual-plot" class="section level3" number="5.3.2">
<h3>
<span class="header-section-number">5.3.2</span> Residual plot<a class="anchor" aria-label="anchor" href="#residual-plot"><i class="fas fa-link"></i></a>
</h3>
<p>While using the scatterplot is an intuitive way of assessing regression assumptions, it has a limitation. It cannot be used if we have multiple predictors in our regression, which we will encounter (and happens more often than just having one predictor). Another visualization that we can use to assess assumptions 1 and 2 is a <strong>residual plot</strong>. This is a scatterplot of residuals, <span class="math inline">\(e\)</span>, against fitted values, <span class="math inline">\(\hat{y}\)</span>. We want to observe the following in a residual plot.</p>
<ul>
<li>Residuals should be <strong>evenly scattered</strong> across the horizontal axis (assumption 1).</li>
<li>The residuals should have <strong>similar vertical variation</strong> across the plot (assumption 2).</li>
<li>Some writers combine these two points into the following statement: the residuals should fall in a <strong>horizontal band around 0</strong> with no apparent pattern (assumption 1, 2).</li>
</ul>
<p>The residual plots in Figure <a href="diag.html#fig:resplots">5.5</a> are based on simulated data from Figures <a href="diag.html#fig:ass1">5.1</a>(a), <a href="diag.html#fig:ass1">5.1</a>(b), and <a href="diag.html#fig:ass2">5.2</a>(b).</p>
<div class="figure">
<span style="display:block;" id="fig:resplots"></span>
<img src="images/resplots.jpg" alt="Residual Plots from Fig 1(a), 1(b), 2(b) Respectively"><p class="caption">
Figure 5.5: Residual Plots from Fig 1(a), 1(b), 2(b) Respectively
</p>
</div>
<p>We make the following observations:</p>
<ul>
<li>From Figure <a href="diag.html#fig:resplots">5.5</a>(a), we see that the residuals are evenly scattered across the horizontal axis, and their vertical variation is fairly constant across the plot. So both assumptions are met.</li>
<li>From Figure <a href="diag.html#fig:resplots">5.5</a>(b), we see that the residuals are <strong>not</strong> evenly scattered across the horizontal axis, although their vertical variation is fairly constant across the plot. So only assumption 1 is not met.</li>
<li>From Figure <a href="diag.html#fig:resplots">5.5</a>(c), we see that the residuals are evenly scattered across the horizontal axis, but their vertical variation is <strong>not constant</strong> across the plot. In fact, the vertical variation is increasing as we move from left to right. So only assumption 2 is not met.</li>
</ul>
<p>If you compare the conclusions from the residuals plots and scatterplots, they are the same. In SLR, the takeaways should be consistent.</p>
<p><em>Please see the associated video for more explanation on how to use Figure <a href="diag.html#fig:resplots">5.5</a> to assess assumptions 1 and 2.</em></p>
<div id="practice-questions-2" class="section level4" number="5.3.2.1">
<h4>
<span class="header-section-number">5.3.2.1</span> Practice questions<a class="anchor" aria-label="anchor" href="#practice-questions-2"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li><p>The residual plot in Figure <a href="diag.html#fig:practice">5.6</a>(a) comes from regressing gift aid against family income for the <code>elmhurst</code> dataset. Based on this residual plot, which assumptions are met?</p></li>
<li><p>The residual plot in Figure <a href="diag.html#fig:practice">5.6</a>(b) comes from regressing price of cars against age for the used cars dataset. Based on this residual plot, which assumptions are met?</p></li>
</ol>
<p><em>Please see the associated video as I go over these practice questions.</em></p>
<div class="figure">
<span style="display:block;" id="fig:practice"></span>
<img src="images/practice.jpg" alt="Residual Plots for Practice Questions"><p class="caption">
Figure 5.6: Residual Plots for Practice Questions
</p>
</div>
</div>
</div>
<div id="acf-plot" class="section level3" number="5.3.3">
<h3>
<span class="header-section-number">5.3.3</span> ACF plot<a class="anchor" aria-label="anchor" href="#acf-plot"><i class="fas fa-link"></i></a>
</h3>
<p>Assumption 3 states that the errors are <strong>independent</strong>. This assumption implies that the values of the response variable are independent from each other. This assumption is typically assessed via knowing the nature of the data.</p>
<ul>
<li><p>If the observations were obtained from a random sample, it is likely that the observations will be independent from each other. This is the very nature of a random sample and why random samples are preferred over convenience samples.</p></li>
<li><p>If the data has some inherent sequence, it is likely the observations will not be independent, and are dependent. For example, if I record the value of a stock at the end of each day, the value at day 2 is likely to be related to its value at day 1. So the values of stock prices at the end of each day are not independent.</p></li>
</ul>
<p>An autocorrelation function (ACF) plot of the residuals may be a used to help assess if the assumption that the errors are independent is met. However, the plot is not a substitute for using your understanding about the nature of the data and should only be used as a confirmation.</p>
<p>The ACF plot measures the correlation between a vector of observations and the lagged versions of the observations. If the observations are uncorrelated, the correlations between the vector of observations and lagged versions of these observations are theoretically 0. We may create an ACF plot for the residuals from our regression.</p>
<p>The ACF plot in Figure <a href="diag.html#fig:ass3">5.7</a>(a) and is based on simulated data that were independently generated.</p>
<div class="figure">
<span style="display:block;" id="fig:ass3"></span>
<img src="images/ass3.jpg" alt="Assumption 3"><p class="caption">
Figure 5.7: Assumption 3
</p>
</div>
<p>A few notes about the ACF plot:</p>
<ul>
<li>The ACF at lag 0 is always 1. The correlation of any vector with itself is always 1.</li>
<li>The dashed horizontal lines represent critical values. An ACF at any lag beyond the critical value indicates an ACF that is significant. We have evidence of correlation (and hence dependence) in our residuals.</li>
<li>If the observed values for the response variable are independent, then we would expect the ACFs at lags greater than 0 to be insignificant. Do note that because we are conducting multiple hypothesis tests, do not be too alarmed if the ACFs are slightly beyond the critical values at an isolated lag or 2.</li>
</ul>
<p>Based on Figure <a href="diag.html#fig:ass3">5.7</a>(a), we see that the ACFs at all lags greater than 0 are insignificant. We do not have evidence the residuals are correlated with each other, so we do not have evidence that assumption 3 is not met.</p>
<p>Sometimes, the dataframe can be sorted in some manner (e.g. increasing order for response variable), and if so, we would actually expect to see significant correlations in the ACF plot. The ACF plot in Figure <a href="diag.html#fig:ass3">5.7</a>(b) is such an example. The residuals are from the same simulated dataset, only with the data sorted by the response variable. If we had just looked at the ACF plot in Figure <a href="diag.html#fig:ass3">5.7</a>(b) without understanding the data were simulated independently and then sorted, we would have erroneously concluded that the residuals are not independent and the regression assumption is not met.</p>
</div>
<div id="qq-plot" class="section level3" number="5.3.4">
<h3>
<span class="header-section-number">5.3.4</span> QQ plot<a class="anchor" aria-label="anchor" href="#qq-plot"><i class="fas fa-link"></i></a>
</h3>
<p>A normal probability plot (also called a QQ plot) is used to assess if the distribution of a variable is normal. It typically plots the residuals against their theoretical residual if they followed a normal distribution. A QQ line is typically overlaid. If the plots fall closely to the QQ line, we have evidence that the observations follow a normal distribution. Figure <a href="diag.html#fig:qq">5.8</a> shows a QQ plot that comes from a normally distributed variable.</p>
<div class="figure">
<span style="display:block;" id="fig:qq"></span>
<img src="images/qqplot.jpeg" alt="QQ Plot"><p class="caption">
Figure 5.8: QQ Plot
</p>
</div>
</div>
<div id="remedial-measures" class="section level3" number="5.3.5">
<h3>
<span class="header-section-number">5.3.5</span> Remedial measures<a class="anchor" aria-label="anchor" href="#remedial-measures"><i class="fas fa-link"></i></a>
</h3>
<p>We now know how to assess if specific regression assumptions are not met. The remedial measures involve transforming either the predictor variable and / or the response variable. These transformations are chosen to handle violations to assumptions 1 and / or 2 respectively. The general strategy on selecting which variable to transform:</p>
<ul>
<li>Transforming the response variable, <span class="math inline">\(y\)</span>, affects both assumptions 1 and 2.
<ul>
<li>Visually, we can think of transforming <span class="math inline">\(y\)</span> in terms of stretching or squeezing the scatterplot of <span class="math inline">\(y\)</span> against <span class="math inline">\(x\)</span> vertically. Thus, transforming <span class="math inline">\(y\)</span> affects the shape of the relationship and the vertical spread of the data points.</li>
<li>However, the <strong>choice on how we transform <span class="math inline">\(y\)</span> is based on handling assumption 2.</strong>
</li>
</ul>
</li>
<li>Transforming the predictor variable, <span class="math inline">\(x\)</span> affects assumption 1 and does not theoretically affect assumption 2.
<ul>
<li>Visually, we can think of transforming <span class="math inline">\(x\)</span> in terms of stretching or squeezing the scatterplot of <span class="math inline">\(y\)</span> against <span class="math inline">\(x\)</span> horizontally. Thus, transforming <span class="math inline">\(x\)</span> affects the shape of the relationship but not the vertical spread of the data points.</li>
<li>Therefore, <strong>transforming <span class="math inline">\(x\)</span> is based on handling assumption 1.</strong>
</li>
</ul>
</li>
<li>If assumption 2 is not met, we transform <span class="math inline">\(y\)</span> to stabilize the variance and make it constant.</li>
<li>If assumption 1 is not met, we transform <span class="math inline">\(x\)</span> to find the appropriate shape to relate the variables.</li>
<li>If both assumptions are not met, we transform <span class="math inline">\(y\)</span> first to stabilize the variance. Once assumption 2 is solved, check if assumption 1 is not met. If not met, transform <span class="math inline">\(x\)</span>.</li>
</ul>
<p>Assumption 1 deals with whether the way we have expressed how <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> are related, through <span class="math inline">\(f(x)\)</span>, is appropriate. Assumption 2 deals with the vertical variation of the data points in the scatterplot.</p>
</div>
</div>
<div id="remedial-measures-variance-stabilizing-transformations" class="section level2" number="5.4">
<h2>
<span class="header-section-number">5.4</span> Remedial Measures: Variance Stabilizing Transformations<a class="anchor" aria-label="anchor" href="#remedial-measures-variance-stabilizing-transformations"><i class="fas fa-link"></i></a>
</h2>
<p>We transform the response variable to stabilize the variance (assumption 2). There are a couple of ways to decide the appropriate transformation:</p>
<ol style="list-style-type: decimal">
<li>Pattern seen in residual plot can guide choice in how to transform the response variable.</li>
<li>Box-Cox plot.</li>
</ol>
<div id="use-pattern-in-residual-plot" class="section level3" number="5.4.1">
<h3>
<span class="header-section-number">5.4.1</span> Use Pattern in Residual Plot<a class="anchor" aria-label="anchor" href="#use-pattern-in-residual-plot"><i class="fas fa-link"></i></a>
</h3>
<p>We can stabilize the variance of the errors based on the residual plot, if we see either of the following scenarios:</p>
<ul>
<li>vertical variation of residuals <strong>increasing</strong> as fitted response increases, or as we move from left to right, as in Figure <a href="diag.html#fig:variance">5.9</a>(a), or</li>
<li>vertical variation of residuals <strong>decreasing</strong> as fitted response increases, or as we move from left to right, as in Figure <a href="diag.html#fig:variance">5.9</a>(b).</li>
</ul>
<div class="figure">
<span style="display:block;" id="fig:variance"></span>
<img src="images/variance.jpg" alt="Non Constant Variance in Residual Plot"><p class="caption">
Figure 5.9: Non Constant Variance in Residual Plot
</p>
</div>
<p>Note that increasing variance as fitted response increases is much more common with real data. Generally, larger values of a variable are associated with larger spread.</p>
<p>We transform <span class="math inline">\(y\)</span> using <span class="math inline">\(y^{*} = y^{\lambda}\)</span>, with <span class="math inline">\(\lambda\)</span> chosen based on whether the variance of the residuals is increasing or decreasing with fitted response:</p>
<ul>
<li>For Figure <a href="diag.html#fig:variance">5.9</a>(a), choose <span class="math inline">\(\lambda &lt; 1\)</span>.
<ul>
<li>If <span class="math inline">\(\lambda = 0\)</span>, it means we use a logarithmic transformation with base e, i.e. <span class="math inline">\(y^* = \log(y)\)</span>.</li>
<li>Note that a logarithm with no base means a natural log, or ln.</li>
</ul>
</li>
<li>For Figure <a href="diag.html#fig:variance">5.9</a>(b), choose <span class="math inline">\(\lambda &gt; 1\)</span>.</li>
</ul>
<p>So based on the residual plot, we have a range of values for <span class="math inline">\(\lambda\)</span>.</p>
</div>
<div id="box-cox-plot" class="section level3" number="5.4.2">
<h3>
<span class="header-section-number">5.4.2</span> Box-Cox Plot<a class="anchor" aria-label="anchor" href="#box-cox-plot"><i class="fas fa-link"></i></a>
</h3>
<p>We can use a Box-Cox plot to help us narrow the range of <span class="math inline">\(\lambda\)</span> to use. It is a plot of the log-likelihood function against <span class="math inline">\(\lambda\)</span>, and we choose <span class="math inline">\(\lambda\)</span> that maximizes this log-likelihood function. For example, Figure <a href="diag.html#fig:bcplot">5.10</a> shows the Box Cox plot generated for the regression associated with the residual plot in Figure <a href="diag.html#fig:variance">5.9</a>(b).</p>
<div class="figure">
<span style="display:block;" id="fig:bcplot"></span>
<img src="images/bcplot.jpg" alt="Box-Cox Plot based on Figure 9(b)"><p class="caption">
Figure 5.10: Box-Cox Plot based on Figure 9(b)
</p>
</div>
<p>Notice an approximate 95% CI is provided for <span class="math inline">\(\lambda\)</span>. A few comments on how to use the Box-Cox plot:</p>
<ul>
<li>Three vertical dashed lines are displayed: the middle line corresponds to the optimal value of <span class="math inline">\(\lambda\)</span>; the other two lines are the lower and upper bounds of a 95% CI for <span class="math inline">\(\lambda\)</span>.</li>
<li>We choose <span class="math inline">\(\lambda\)</span> within the CI (or even close to it) that is easy to understand. We do not have to choose the optimal value, especially if its value is difficult to interpret. In this example, I will choose <span class="math inline">\(\lambda = 2\)</span>, so a square transformation for <span class="math inline">\(y\)</span>. Transform response with <span class="math inline">\(y^* = y^2\)</span>. Regress <span class="math inline">\(y^*\)</span> against <span class="math inline">\(x\)</span>.</li>
<li>If 1 lies in the CI, <strong>no transformation</strong> on <span class="math inline">\(y\)</span> may be needed.</li>
<li>If a transformation is needed, a <strong>log transformation</strong> is preferred, since we can still interpret the estimated coefficients. It is difficult to interpret with any other type of transformation.</li>
<li>View the Box-Cox procedure as a guide for selecting a transformation, rather than being definitive.</li>
<li>Need to recheck the residuals after every transformation to assess if the transformation worked.</li>
</ul>
</div>
<div id="interpretation-with-log-transformed-response" class="section level3" number="5.4.3">
<h3>
<span class="header-section-number">5.4.3</span> Interpretation with Log Transformed Response<a class="anchor" aria-label="anchor" href="#interpretation-with-log-transformed-response"><i class="fas fa-link"></i></a>
</h3>
<p>A log transformation on the response is preferred over any other transformation, as we can still interpret regression coefficients. A couple of ways to interpret the estimated slope <span class="math inline">\(\hat{\beta}_1\)</span>:</p>
<ul>
<li>The predicted response variable is <strong>multiplied by a factor</strong> of <span class="math inline">\(\exp(\hat{\beta_1})\)</span> for a one-unit increase in the predictor.</li>
<li>We can also subtract 1 from <span class="math inline">\(\exp(\hat{\beta_1})\)</span> to express the change as a percentage.
<ul>
<li>If <span class="math inline">\(\hat{\beta}_1\)</span> is positive, we have a percent <strong>increase</strong>. The predicted response variable increases by <span class="math inline">\((\exp(\hat{\beta_1}) - 1) \times 100\)</span> percent for a one-unit increase in the predictor.</li>
<li>If <span class="math inline">\(\hat{\beta}_1\)</span> is negative, we have a percent <strong>decrease</strong>. The predicted response variable decreases by <span class="math inline">\((1 - \exp(\hat{\beta_1})) \times 100\)</span> percent for a one-unit increase in the predictor.</li>
</ul>
</li>
</ul>
<p><em>Please see the associated video as I go over the math explaining how we interpret regression coefficients when the response variable is log transformed.</em></p>
</div>
</div>
<div id="remedial-measures-linearization-transformations" class="section level2" number="5.5">
<h2>
<span class="header-section-number">5.5</span> Remedial Measures: Linearization Transformations<a class="anchor" aria-label="anchor" href="#remedial-measures-linearization-transformations"><i class="fas fa-link"></i></a>
</h2>
<p>We first ensure the variance has been stabilized and assumption 2 is met. If <span class="math inline">\(f(x)\)</span> does not accurately capture the relationship between the variables, we transform the predictor variable to meet assumption 1. Some writers call this a linearization transformation, as we seek to make the transformed version of the predictor variable, <span class="math inline">\(x^*\)</span>, to be approximately linear with the response variable (or transformed <span class="math inline">\(y\)</span>), i.e. <span class="math inline">\(y = \beta_0 + \beta_1 x^* + \epsilon\)</span>. We do not consider transforming the response variable to deal with assumption 1, as transforming the response variable is likely to reintroduce violation of assumption 2.</p>
<p>The general strategy on how to transform the predictor is via a scatterplot of <span class="math inline">\(y\)</span> (or <span class="math inline">\(y^*\)</span>) against <span class="math inline">\(x\)</span>. We use the pattern seen in the plot to decide how to transform the predictor. Some examples are shown in Figure <a href="diag.html#fig:xstar">5.11</a> below.</p>
<div class="figure">
<span style="display:block;" id="fig:xstar"></span>
<img src="images/xstar.jpg" alt="Transformations for x"><p class="caption">
Figure 5.11: Transformations for x
</p>
</div>
<div id="hierarchical-principle" class="section level3" number="5.5.1">
<h3>
<span class="header-section-number">5.5.1</span> Hierarchical Principle<a class="anchor" aria-label="anchor" href="#hierarchical-principle"><i class="fas fa-link"></i></a>
</h3>
<p>One thing to be aware of is the <strong>hierarchical principle</strong>: if the relationship between the response and predictor is of a higher order polynomial (e.g. quadratic, cubic), the hierarchical principle states that the lower order terms should remain in the model. For example, if the relationship is of order <span class="math inline">\(h\)</span>, fit <span class="math inline">\(y = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_h x^h + \epsilon\)</span> via a multiple linear regression framework. We will see how to do this in the next module.</p>
</div>
<div id="interpretation-with-log-transformed-predictor" class="section level3" number="5.5.2">
<h3>
<span class="header-section-number">5.5.2</span> Interpretation with Log Transformed Predictor<a class="anchor" aria-label="anchor" href="#interpretation-with-log-transformed-predictor"><i class="fas fa-link"></i></a>
</h3>
<p>A log transformation on the predictor is preferred over any other transformation, as we can still interpret the regression coefficient, <span class="math inline">\(\hat{\beta}_1\)</span>, in a couple of ways:</p>
<ul>
<li>For an <span class="math inline">\(a\%\)</span> increase in the predictor, the predicted response <strong>increases by <span class="math inline">\(\hat{\beta}_1 \log(1+ \frac{a}{100})\)</span>.</strong>
</li>
<li>
<span class="math inline">\(\log(1 + \frac{1}{100}) \approx \frac{1}{100}\)</span> (Taylor series). So an alternative interpretation is: for a 1% increase in the predictor, the predicted response increases by approximately <span class="math inline">\(\frac{\hat{\beta}_1}{100}\)</span>.</li>
</ul>
<p><em>Please see the associated video as I go over the math explaining how we interpret regression coefficients when the predictor variable is log transformed.</em></p>
</div>
<div id="interpretation-with-log-transformed-response-and-predictor" class="section level3" number="5.5.3">
<h3>
<span class="header-section-number">5.5.3</span> Interpretation with Log Transformed Response and Predictor<a class="anchor" aria-label="anchor" href="#interpretation-with-log-transformed-response-and-predictor"><i class="fas fa-link"></i></a>
</h3>
<p>If both response and predictor variables are log transformed, the regression coefficient, <span class="math inline">\(\hat{\beta}_1\)</span>, can be interpreted in a couple of ways:</p>
<ul>
<li><p>For an <span class="math inline">\(a\%\)</span> increase in the predictor, the predicted response is <strong>multiplied by <span class="math inline">\((1 + \frac{a}{100})^{\hat{\beta}_1}\)</span>.</strong></p></li>
<li><p><span class="math inline">\((1 + \frac{1}{100})^{\hat{\beta}_1} \approx 1 + \frac{\hat{\beta}_1}{100}\)</span> (Taylor series). So an alternative interpretation is: for a 1% increase in the predictor, the predicted response <strong>increases by approximately <span class="math inline">\(\hat{\beta}_1\)</span> percent.</strong> Note that this approximation works better when <span class="math inline">\({\hat{\beta}_1}\)</span> is small in magnitude.</p></li>
</ul>
<p><em>Please see the associated video as I go over the math explaining how we interpret regression coefficients when the both response and predictor variables are log transformed.</em></p>
</div>
<div id="some-general-comments-about-assessing-assumptions-and-transformations" class="section level3" number="5.5.4">
<h3>
<span class="header-section-number">5.5.4</span> Some General Comments about Assessing Assumptions and Transformations<a class="anchor" aria-label="anchor" href="#some-general-comments-about-assessing-assumptions-and-transformations"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>When assessing the assumptions with a residual plot, we are assessing if the assumptions are reasonably / approximately met.</p></li>
<li><p>With real data, assumptions are rarely met 100%.</p></li>
<li><p>If unsure, proceed with model building, and test how model performs on new data. If poor performance, go back to residual plot to assess what transformation will be appropriate.</p></li>
<li><p>Assess the plots to decide which variables need to be transformed, and how. The choice of transformation should be guided by what you see in the plots, and not by trial and error.</p></li>
<li><p>A residual plot should always be produced after each transformation. A Box Cox plot could also be produced. The plots should be assessed if the transformation helped in the way you intended.</p></li>
<li><p>Solve assumption 2 first, then assumption 1.</p></li>
</ul>
</div>
</div>
<div id="r-tutorial-2" class="section level2" number="5.6">
<h2>
<span class="header-section-number">5.6</span> R Tutorial<a class="anchor" aria-label="anchor" href="#r-tutorial-2"><i class="fas fa-link"></i></a>
</h2>
<div id="example-1" class="section level3" number="5.6.1">
<h3>
<span class="header-section-number">5.6.1</span> Example 1<a class="anchor" aria-label="anchor" href="#example-1"><i class="fas fa-link"></i></a>
</h3>
<p>The linear regression model involves several assumptions. Among them are:</p>
<ol style="list-style-type: decimal">
<li>The errors, for each fixed value of <span class="math inline">\(x\)</span>, have mean 0. This implies that the relationship as specified in the regression equation is appropriate.</li>
<li>The errors, for each fixed value of <span class="math inline">\(x\)</span>, have constant variance. That is, the variation in the errors is theoretically the same regardless of the value of <span class="math inline">\(x\)</span> (or <span class="math inline">\(\hat{y}\)</span>).</li>
<li>The errors are independent.</li>
<li>The errors, for each fixed value of <span class="math inline">\(x\)</span>, follow a normal distribution.</li>
</ol>
<p>To assess assumptions 1 and 2, we can examine scatterplots of:</p>
<ul>
<li>
<span class="math inline">\(y\)</span> versus <span class="math inline">\(x\)</span>.</li>
<li>residuals versus fitted values, <span class="math inline">\(\hat{y}\)</span>.</li>
</ul>
<p>Assumption 3 is assessed based on knowledge of the data. An autocorrelation (ACF) plot of the residuals may also be used.</p>
<p>Assumption 4 is assessed with a normal probability plot, and is considered the least crucial of the assumptions.</p>
<p>We will see how to generate the relevant graphical displays to help us assess whether the assumptions are met, and if needed, carry out transformations on the variable(s) so the assumptions are met.</p>
<p>For this tutorial, we will go over a dataset involving prices of used cars (Mazdas). The two variables are the sales price of the used car, and the age of the car in years. Download the data file, <code>mazda.txt</code> and read the data:</p>
<div class="sourceCode" id="cb222"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Data</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">"mazda.txt"</span>, header<span class="op">=</span><span class="cn">TRUE</span>, sep<span class="op">=</span><span class="st">""</span><span class="op">)</span></span></code></pre></div>
<div id="model-diagnostics-with-scatterplots" class="section level4 unnumbered">
<h4>Model Diagnostics with Scatterplots<a class="anchor" aria-label="anchor" href="#model-diagnostics-with-scatterplots"><i class="fas fa-link"></i></a>
</h4>
<p>We can use a scatterplot of the response variable against the predictor to assess assumptions 1 and 2:</p>
<div class="sourceCode" id="cb223"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co">##scatterplot, and overlay regression line</span></span>
<span><span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">Data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">Age</span>,y<span class="op">=</span><span class="va">Price</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x<span class="op">=</span><span class="st">"Age"</span>, y<span class="op">=</span><span class="st">"Sales Price"</span>, title<span class="op">=</span><span class="st">"Scatterplot of Sales Price against Age"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-154-1.png" width="672"></div>
<p>To assess assumption 1, the data points should be evenly scattered on both sides of the regression line, as we move from left to right. We do not see this in the scatterplot, so assumption 1 is not met. When age is between 0 and 2, the data points are mostly above the line. When age is between 5 and 11, the data points are mostly below the line, and when age is greater than 13, the data points are above the line.</p>
<p>To assess assumption 2, the vertical spread of the data points should be constant as we move from left to right. The spread seems to be decreasing as we move from left to right (or in other words, the spread is increasing as the response increases), so assumption 2 is not met.</p>
</div>
<div id="model-diagnostics-with-residual-plots" class="section level4 unnumbered">
<h4>Model Diagnostics with Residual Plots<a class="anchor" aria-label="anchor" href="#model-diagnostics-with-residual-plots"><i class="fas fa-link"></i></a>
</h4>
<p>Sometimes, a residual plot is easier to visualize than a scatterplot. We fit our SLR model using <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> as usual. Applying the <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code> function to an object created with <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> actually produces a four diagnostic plots. To display the four diagnostic plots in a 2 by 2 array, we specify <code>par(mfrow = c(2, 2))</code> so the plotting window is split into a 2 by 2 array:</p>
<div class="sourceCode" id="cb224"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">result</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Price</span><span class="op">~</span><span class="va">Age</span>, data<span class="op">=</span><span class="va">Data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">result</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-155-1.png" width="672"></div>
<ul>
<li><p>The first plot (top left) is the residual plot, with residuals on the y-axis and fitted values on the x-axis. The residual plot can be used to address assumptions 1 and 2. A red line is overlayed to represent the average value of the residuals for differing values along the x-axis. This line should be along the x-axis without any apparent curvature to indicate the form of our model is reasonable. This is not what we see, as we see a clear curved pattern. So assumption 1 is not met. For assumption 2, we want to see the vertical spread of the residuals to be fairly constant as we move from left to right. We do not see this in the residual plot; the vertical spread increases as we move from left to right, so assumption 2 is not met.</p></li>
<li><p>The second plot (top right) is the normal probability plot (also called a QQ plot), and addresses assumption 4. If the residuals are normal, the residuals should fall along the 45 degree line. The regression model is fairly robust to this assumption though; the normality assumption is the least crucial of the four.</p></li>
<li><p>The third plot (bottom left) is a plot of the square root of the absolute value of the standardized residuals against the fitted values (scale-location). This plot should be used to assess assumption 2, the constant variance assumption. A red line is overlayed to represent the average value on the vertical axis for differing values along the x-axis. If the variance is constant, the red line should be horizontal and the vertical spread of the plot should be constant. This plot should be used to assess assumption 2, if we have a small sample size. Otherwise, this plot should tell a similar story to the first plot (top left) when assessing assumption 2.</p></li>
<li><p>The last plot (bottom right) is a plot to identify influential outliers. Data points that lie in the contour lines with large Cook’s distance are influential. None of our data points have Cook’s distance greater than 0.5. As a general rule of thumb, observations with Cook’s distance greater than 1 are flagged as influential. We will talk more about influential observations in a future module.</p></li>
</ul>
<p>Now that we know that both assumptions 1 and 2 are not met. We need to transform the response variable first, to stabilize the variance.</p>
<p>Based on the residual plot, we see that the variance of the residuals increases as we move from left to right. So we know we need to transform the response variable using <span class="math inline">\(y^* = y^\lambda\)</span> with <span class="math inline">\(\lambda &lt; 1\)</span>. A log transform should be considered since we can still interpret regression coefficients.</p>
</div>
<div id="box-cox-transformation-on-y" class="section level4 unnumbered">
<h4>Box Cox Transformation on y<a class="anchor" aria-label="anchor" href="#box-cox-transformation-on-y"><i class="fas fa-link"></i></a>
</h4>
<p>The Box Cox plot can be used to decide how to transform the response variable. The transformation takes the form <span class="math inline">\(y^* = y^{\lambda}\)</span>, with the value of <span class="math inline">\(\lambda\)</span> to be chosen. If <span class="math inline">\(\lambda = 0\)</span>, we perform a <span class="math inline">\(\log\)</span> transformation.</p>
<p>We will use the <code><a href="https://rdrr.io/pkg/MASS/man/boxcox.html">boxcox()</a></code> function from the <code>MASS</code> package:</p>
<div class="sourceCode" id="cb225"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span> <span class="co">##to use boxcox function</span></span>
<span><span class="fu">MASS</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/boxcox.html">boxcox</a></span><span class="op">(</span><span class="va">result</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-156-1.png" width="672"></div>
<p>We can “zoom in” on the plot to have a better idea about the value of <span class="math inline">\(\lambda\)</span> we can use, by specifying the range of <code>lambda</code> inside the function:</p>
<div class="sourceCode" id="cb226"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##adjust lambda for better visualization. Choose lambda between -0.5 and 0.5</span></span>
<span><span class="fu">MASS</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/boxcox.html">boxcox</a></span><span class="op">(</span><span class="va">result</span>, lambda <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">1</span><span class="op">/</span><span class="fl">10</span><span class="op">)</span><span class="op">)</span> </span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-157-1.png" width="672"></div>
<p>We can choose any value of <span class="math inline">\(\lambda\)</span> within the CI. A log transformation is preferred if possible, since we can still interpret coefficients. Since 0 lies in the CI, we choose <span class="math inline">\(\lambda = 0\)</span>, to log transform the response variable to get <span class="math inline">\(y^* = \log(y)\)</span>. We regress <span class="math inline">\(y^*\)</span> against <span class="math inline">\(x\)</span>, and check the resulting residual plot:</p>
<div class="sourceCode" id="cb227"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##transform y and then regress ystar on x</span></span>
<span><span class="va">ystar</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">Data</span><span class="op">$</span><span class="va">Price</span><span class="op">)</span></span>
<span><span class="va">Data</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">Data</span>,<span class="va">ystar</span><span class="op">)</span></span>
<span><span class="va">result.ystar</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">ystar</span><span class="op">~</span><span class="va">Age</span>, data<span class="op">=</span><span class="va">Data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">result.ystar</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-158-1.png" width="672"></div>
<p>We need to reassess assumptions 1 and 2 after the transformation.</p>
<ul>
<li><p>For assumption 2, we see that the vertical spread of the residuals in the residual plot (top left) is fairly constant, as we move from left to right. So assumption 2 is met. The log transformation worked.</p></li>
<li><p>We also notice that the residuals are now evenly scattered across the horizontal axis in the residual plot (top left). So assumption 1 is now met.</p></li>
</ul>
<p>We do not need to perform any other transformations.</p>
</div>
<div id="interpreting-coefficients-with-log-transformed-response" class="section level4 unnumbered">
<h4>Interpreting Coefficients with Log Transformed Response<a class="anchor" aria-label="anchor" href="#interpreting-coefficients-with-log-transformed-response"><i class="fas fa-link"></i></a>
</h4>
<p>So our regression equation is</p>
<div class="sourceCode" id="cb228"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">result.ystar</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = ystar ~ Age, data = Data)
## 
## Coefficients:
## (Intercept)          Age  
##     10.1878      -0.1647</code></pre>
<p><span class="math inline">\(\hat{y^*} = 10.1878 - 0.1647x\)</span>, where <span class="math inline">\(y^* = \log(y)\)</span>. To interpret the slope:</p>
<ul>
<li>The price of used Mazdas is multiplied by <span class="math inline">\(\exp(-0.1647) = 0.8481481\)</span> for each year older the car is.</li>
<li>The price of used Mazdas decreases by <span class="math inline">\((1 - 0.8481481) \times 100\)</span> percent, or 15.18519 percent, for each year older the car is.</li>
</ul>
</div>
<div id="acf-plot-of-residuals" class="section level4 unnumbered">
<h4>ACF Plot of Residuals<a class="anchor" aria-label="anchor" href="#acf-plot-of-residuals"><i class="fas fa-link"></i></a>
</h4>
<p>We have yet to assess the assumption that the observed prices are independent from each other. Assuming that these prices are from different cars and not the same car measured repeatedly over time, there is no reason to think the prices are dependent on each other.</p>
<p>We an also produce an ACF plot to confirm our thought:</p>
<div class="sourceCode" id="cb230"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/acf.html">acf</a></span><span class="op">(</span><span class="va">result.ystar</span><span class="op">$</span><span class="va">residuals</span>, main<span class="op">=</span><span class="st">"ACF Plot of Residuals with ystar"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-160-1.png" width="672"></div>
<p>None of the ACFs beyond lag 0 are significant, so we don’t have evidence that the observations are dependent on each other.</p>
</div>
</div>
<div id="example-2" class="section level3" number="5.6.2">
<h3>
<span class="header-section-number">5.6.2</span> Example 2<a class="anchor" aria-label="anchor" href="#example-2"><i class="fas fa-link"></i></a>
</h3>
<p>For this second example, we will go over an example for the <code>faraway</code> package. The dataframe is called <code>gala</code>. The data are about species diversity on the Galapagos Islands. There are 30 islands, and for each island, we have data on 7 variables. We will focus on the variable <code>Species</code>, which denotes the number of plant species found on the island, and <code>Area</code>, the area of the island in squared kilometers. We wish to see how the number of plant species of an island is related to the area of the island.</p>
<div class="sourceCode" id="cb231"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/julianfaraway/faraway">faraway</a></span><span class="op">)</span></span>
<span><span class="va">Data</span><span class="op">&lt;-</span><span class="fu">faraway</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/faraway/man/gala.html">gala</a></span></span></code></pre></div>
<div id="model-diagnostics-with-scatterplots-1" class="section level4 unnumbered">
<h4>Model Diagnostics with Scatterplots<a class="anchor" aria-label="anchor" href="#model-diagnostics-with-scatterplots-1"><i class="fas fa-link"></i></a>
</h4>
<p>We can use a scatterplot of the response variable against the predictor to assess assumptions 1 and 2.</p>
<div class="sourceCode" id="cb232"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co">##scatterplot, and overlay regression line</span></span>
<span><span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">Data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">Area</span>,y<span class="op">=</span><span class="va">Species</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x<span class="op">=</span><span class="st">"Area of Island (sq km)"</span>, y<span class="op">=</span><span class="st">"# of Plant Species"</span>, </span>
<span>       title<span class="op">=</span><span class="st">"Scatterplot of Number of Plant Species against Area of Galapagos Island"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-163-1.png" width="672"></div>
<p>To assess assumption 1, the data points should be evenly scattered on both sides of the regression line, as we move from left to right. This plot looks nonlinear.</p>
<p>To assess assumption 2, the vertical spread of the data points should be constant as we move from left to right. This can be a bit difficult to assess with this scatterplot, although observations with small areas are closer to the line, which suggests the assumption is not met.</p>
</div>
<div id="model-diagnostics-with-residual-plots-1" class="section level4 unnumbered">
<h4>Model Diagnostics with Residual Plots<a class="anchor" aria-label="anchor" href="#model-diagnostics-with-residual-plots-1"><i class="fas fa-link"></i></a>
</h4>
<p>Fairly often, when assessing regression assumptions, a residual plot is easier to visualize than a scatterplot.</p>
<div class="sourceCode" id="cb233"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">result</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">Area</span>, data<span class="op">=</span><span class="va">Data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">result</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-164-1.png" width="672"></div>
<ul>
<li><p>From the residual plot (top left), we see a curved pattern, so we have a nonlinear relationship. Assumption 1 is not met.</p></li>
<li><p>From the scale-location plot (bottom left), the vertical variance of the plots appear to be higher for islands with larger fitted valies, so assumption 2 is not met.</p></li>
</ul>
<p>Now that we know that both assumptions 1 and 2 are not met. We need to transform the response variable first, to stabilize the variance.</p>
</div>
<div id="box-cox-transformation-on-y-1" class="section level4 unnumbered">
<h4>Box Cox Transformation on y<a class="anchor" aria-label="anchor" href="#box-cox-transformation-on-y-1"><i class="fas fa-link"></i></a>
</h4>
<p>From the scale-location plot, we see the variance of the residuals is increasing, so we expect to transform the response variable with <span class="math inline">\(y^* = y^{\lambda}\)</span> with <span class="math inline">\(\lambda &lt; 1\)</span>. So see which specific value of <span class="math inline">\(\lambda\)</span> to use, we can use the Box Cox plot:</p>
<div class="sourceCode" id="cb234"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span><span class="fu">MASS</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/boxcox.html">boxcox</a></span><span class="op">(</span><span class="va">result</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-165-1.png" width="672"></div>
<p>A log transformation is preferred if possible, since we can still interpret coefficients. Since 0 lies in the CI, we choose <span class="math inline">\(\lambda = 0\)</span>, to log transform the response variable to get <span class="math inline">\(y^* = \log(y)\)</span>. We regress <span class="math inline">\(y^*\)</span> against <span class="math inline">\(x\)</span>, and check the resulting residual plot:</p>
<div class="sourceCode" id="cb235"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##log transform response and add to dataframe</span></span>
<span><span class="va">Data</span><span class="op">$</span><span class="va">y.star</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">Data</span><span class="op">$</span><span class="va">Species</span><span class="op">)</span></span>
<span><span class="co">##perform new regression</span></span>
<span><span class="va">result.ystar</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y.star</span><span class="op">~</span><span class="va">Area</span>, data<span class="op">=</span><span class="va">Data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">result.ystar</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-166-1.png" width="672"></div>
<p>We need to reassess assumptions 1 and 2 after the transformation.</p>
<ul>
<li><p>For assumption 2, we see that the vertical spread of the residuals in the residual plot (top left) is fairly constant, as we move from left to right. So assumption 2 is met. The log transformation worked in stabilizing the variance.</p></li>
<li><p>However, the residual plot still appears to be nonlinear. So assumption 1 is still not met.</p></li>
</ul>
</div>
<div id="transformation-on-x" class="section level4 unnumbered">
<h4>Transformation on x<a class="anchor" aria-label="anchor" href="#transformation-on-x"><i class="fas fa-link"></i></a>
</h4>
<p>To see which specific transformation on the predictor to use, we create a scatterplot of the transformed response and the predictor.</p>
<div class="sourceCode" id="cb236"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">Data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">Area</span>,y<span class="op">=</span><span class="va">y.star</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x<span class="op">=</span><span class="st">"Area of Island (sq km)"</span>, y<span class="op">=</span><span class="st">"Log # of Plant Species"</span>, </span>
<span>       title<span class="op">=</span><span class="st">"Scatterplot of Log Number of Plant Species against Area of Galapagos Island"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-167-1.png" width="672"></div>
<p>This plot resembles a logarithmic curve, so we use a log transformation on the predictor, so let <span class="math inline">\(x^{*} = \log(x)\)</span>. As usual, a log transformation is preferred since we can still interpret the regression coefficients. We then perform a regression using both the log transformed response variable and predictor, and assess the diagnostic plots.</p>
<div class="sourceCode" id="cb237"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##log transform predictor and add to dataframe</span></span>
<span><span class="va">Data</span><span class="op">$</span><span class="va">x.star</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">Data</span><span class="op">$</span><span class="va">Area</span><span class="op">)</span></span>
<span></span>
<span><span class="co">##perform new regression</span></span>
<span><span class="va">result.xystar</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y.star</span><span class="op">~</span><span class="va">x.star</span>, data<span class="op">=</span><span class="va">Data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">result.xystar</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-168-1.png" width="672"></div>
<p>Based on the residual plot (top left), both assumptions are met. The residuals are evenly scattered across the horizontal axis with no pattern. The vertical spread of the residuals is also constant. So the transformations worked.</p>
</div>
<div id="interpreting-coefficients-with-log-transformed-response-and-predictor" class="section level4 unnumbered">
<h4>Interpreting Coefficients with Log Transformed Response and Predictor<a class="anchor" aria-label="anchor" href="#interpreting-coefficients-with-log-transformed-response-and-predictor"><i class="fas fa-link"></i></a>
</h4>
<p>So our regression equation is</p>
<div class="sourceCode" id="cb238"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">result.xystar</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y.star ~ x.star, data = Data)
## 
## Coefficients:
## (Intercept)       x.star  
##      2.9037       0.3886</code></pre>
<p><span class="math inline">\(\hat{y^*} = 2.9037 + 0.3886x^{*}\)</span>, where <span class="math inline">\(y^* = \log(y)\)</span> and <span class="math inline">\(x^* = \log(x)\)</span>. There are a couple of ways to interpret the slope:</p>
<ul>
<li><p>For a 1% increase in the area of a Galapagos island, the number of plant species found on the island is multiplied by <span class="math inline">\((1.01)^{0.3886} = 1.003874\)</span>, OR</p></li>
<li><p>For a 1% increase in the area of a Galapagos island, the number of plant species increases by about 0.3886%.</p></li>
</ul>
</div>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="inf.html"><span class="header-section-number">4</span> Inference with Simple Linear Regression (SLR)</a></div>
<div class="next"><a href="mlr.html"><span class="header-section-number">6</span> Multiple Linear Regression (MLR)</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#diag"><span class="header-section-number">5</span> Model Diagnostics and Remedial Measures in SLR</a></li>
<li><a class="nav-link" href="#introduction-4"><span class="header-section-number">5.1</span> Introduction</a></li>
<li>
<a class="nav-link" href="#assumptions-in-linear-regression"><span class="header-section-number">5.2</span> Assumptions in Linear Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#assumption-1-errors-have-mean-0."><span class="header-section-number">5.2.1</span> Assumption 1: Errors have mean 0.</a></li>
<li><a class="nav-link" href="#assumption-2-errors-have-constant-variance"><span class="header-section-number">5.2.2</span> Assumption 2: Errors have constant variance</a></li>
<li><a class="nav-link" href="#assumption-3-errors-are-independent"><span class="header-section-number">5.2.3</span> Assumption 3: Errors are independent</a></li>
<li><a class="nav-link" href="#assumption-4-errors-are-normally-distributed"><span class="header-section-number">5.2.4</span> Assumption 4: Errors are normally distributed</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#assessing-regression-assumptions"><span class="header-section-number">5.3</span> Assessing Regression Assumptions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#scatterplot"><span class="header-section-number">5.3.1</span> Scatterplot</a></li>
<li><a class="nav-link" href="#residual-plot"><span class="header-section-number">5.3.2</span> Residual plot</a></li>
<li><a class="nav-link" href="#acf-plot"><span class="header-section-number">5.3.3</span> ACF plot</a></li>
<li><a class="nav-link" href="#qq-plot"><span class="header-section-number">5.3.4</span> QQ plot</a></li>
<li><a class="nav-link" href="#remedial-measures"><span class="header-section-number">5.3.5</span> Remedial measures</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#remedial-measures-variance-stabilizing-transformations"><span class="header-section-number">5.4</span> Remedial Measures: Variance Stabilizing Transformations</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#use-pattern-in-residual-plot"><span class="header-section-number">5.4.1</span> Use Pattern in Residual Plot</a></li>
<li><a class="nav-link" href="#box-cox-plot"><span class="header-section-number">5.4.2</span> Box-Cox Plot</a></li>
<li><a class="nav-link" href="#interpretation-with-log-transformed-response"><span class="header-section-number">5.4.3</span> Interpretation with Log Transformed Response</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#remedial-measures-linearization-transformations"><span class="header-section-number">5.5</span> Remedial Measures: Linearization Transformations</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#hierarchical-principle"><span class="header-section-number">5.5.1</span> Hierarchical Principle</a></li>
<li><a class="nav-link" href="#interpretation-with-log-transformed-predictor"><span class="header-section-number">5.5.2</span> Interpretation with Log Transformed Predictor</a></li>
<li><a class="nav-link" href="#interpretation-with-log-transformed-response-and-predictor"><span class="header-section-number">5.5.3</span> Interpretation with Log Transformed Response and Predictor</a></li>
<li><a class="nav-link" href="#some-general-comments-about-assessing-assumptions-and-transformations"><span class="header-section-number">5.5.4</span> Some General Comments about Assessing Assumptions and Transformations</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#r-tutorial-2"><span class="header-section-number">5.6</span> R Tutorial</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#example-1"><span class="header-section-number">5.6.1</span> Example 1</a></li>
<li><a class="nav-link" href="#example-2"><span class="header-section-number">5.6.2</span> Example 2</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Linear Models for Data Science</strong>" was written by Jeffrey Woo. It was last built on 2024-08-02.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
