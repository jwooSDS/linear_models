<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 11 Logistic Regression | Linear Models for Data Science</title>
<meta name="author" content="Jeffrey Woo">
<meta name="description" content="11.1 Introduction Up to this point, you have been learning about linear regression, which is used when we have one quantitative response variable and at least one predictor. When we have a binary...">
<meta name="generator" content="bookdown 0.34 with bs4_book()">
<meta property="og:title" content="Chapter 11 Logistic Regression | Linear Models for Data Science">
<meta property="og:type" content="book">
<meta property="og:description" content="11.1 Introduction Up to this point, you have been learning about linear regression, which is used when we have one quantitative response variable and at least one predictor. When we have a binary...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 11 Logistic Regression | Linear Models for Data Science">
<meta name="twitter:description" content="11.1 Introduction Up to this point, you have been learning about linear regression, which is used when we have one quantitative response variable and at least one predictor. When we have a binary...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Linear Models for Data Science</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="wrangling.html"><span class="header-section-number">1</span> Data Wrangling with R</a></li>
<li><a class="" href="viz.html"><span class="header-section-number">2</span> Data Visualization with R Using ggplot2</a></li>
<li><a class="" href="slr.html"><span class="header-section-number">3</span> Basics with Simple Linear Regression (SLR)</a></li>
<li><a class="" href="inf.html"><span class="header-section-number">4</span> Inference with Simple Linear Regression (SLR)</a></li>
<li><a class="" href="diag.html"><span class="header-section-number">5</span> Model Diagnostics and Remedial Measures in SLR</a></li>
<li><a class="" href="mlr.html"><span class="header-section-number">6</span> Multiple Linear Regression (MLR)</a></li>
<li><a class="" href="genF.html"><span class="header-section-number">7</span> General Linear \(F\) Test and Multicollinearity</a></li>
<li><a class="" href="cat.html"><span class="header-section-number">8</span> Categorical Predictors in MLR</a></li>
<li><a class="" href="crit.html"><span class="header-section-number">9</span> Model Selection Criteria and Automated Search Procedures</a></li>
<li><a class="" href="out.html"><span class="header-section-number">10</span> Analysis of Residuals in MLR</a></li>
<li><a class="active" href="logistic1.html"><span class="header-section-number">11</span> Logistic Regression</a></li>
<li><a class="" href="logistic2.html"><span class="header-section-number">12</span> Logistic Regression 2</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="logistic1" class="section level1" number="11">
<h1>
<span class="header-section-number">11</span> Logistic Regression<a class="anchor" aria-label="anchor" href="#logistic1"><i class="fas fa-link"></i></a>
</h1>
<div id="introduction-10" class="section level2" number="11.1">
<h2>
<span class="header-section-number">11.1</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction-10"><i class="fas fa-link"></i></a>
</h2>
<p>Up to this point, you have been learning about linear regression, which is used when we have one quantitative response variable and at least one predictor. When we have a binary response variable and at least one predictor, we use logistic regression.</p>
<p>Common ways of summarizing binary variables include using probabilities and odds. For example, you may want to estimate the probability of college students who haven driven while drunk based on characteristics such as how often they party and how often they drink alcohol. You may consider using a linear regression model, with the probability of driving while drunk as the response variable. However, a linear regression model may end up having estimated probabilities that are less than 0 or greater than 1. A logistic regression model is set up to guarantee the estimated probability is always between 0 and 1.</p>
<p>Typical questions that a logistic regression model can answer include how does partying more often increase the odds of driving while drunk? Do characteristics such as gender help us better predict the odds of driving while drunk when we already know how often the student parties? How confident are we of our estimated odds?</p>
<p>In this module, you will learn about the logistic regression model. Similar to what you learned when studying the linear regression model, you will learn how to interpret the coefficients of a logistic regression model, perform various inferential procedures to answer various questions of interests, and learn how to assess the accuracy of the model. As you study the logistic regression model, it will be helpful for you to compare and contrast what you are learning with what you learned about the linear regression model.</p>
</div>
<div id="logistic-regression" class="section level2" number="11.2">
<h2>
<span class="header-section-number">11.2</span> Logistic Regression<a class="anchor" aria-label="anchor" href="#logistic-regression"><i class="fas fa-link"></i></a>
</h2>
<p>We will introduce the notation of terms commonly used in logistic regression:</p>
<ul>
<li>
<span class="math inline">\(\pi\)</span>: <strong>probability</strong> of “success” (belonging in class coded 1 when using indicator variable to denote the binary response variable).</li>
<li>
<span class="math inline">\(1-\pi\)</span>: probability of “failure” (belonging in class coded 0 when using indicator variable to denote the binary response variable).</li>
<li>
<span class="math inline">\(\frac{\pi}{1-\pi}\)</span>: <strong>odds</strong> of “success”.</li>
<li>
<span class="math inline">\(\log \left( \frac{\pi}{1-\pi} \right)\)</span>: <strong>log-odds</strong> of “success”.</li>
</ul>
<p>Using the driving while drunk example from the introduction section, the response variable is whether the student has driven drunk, which is binary with levels “yes” or “no”. We could use indicator variable to denote this binary response, with 1 denoting “yes”, and 0 denoting “no”. In this example:</p>
<ul>
<li>
<span class="math inline">\(\pi\)</span> denotes the probability that a student has driven while drunk,</li>
<li>
<span class="math inline">\(1-\pi\)</span> denotes the probability that a student has not driven while drunk,</li>
<li>
<span class="math inline">\(\frac{\pi}{1-\pi}\)</span> denotes the odds that a student has driven while drunk,</li>
<li>
<span class="math inline">\(\log \left( \frac{\pi}{1-\pi} \right)\)</span> denotes the log odds that a student has driven while drunk.</li>
</ul>
<p>Notice in these definitions, probability and odds are not exactly the same. If the probability is small, then probability and odds are approximately the same.</p>
<p>Because the response variable is binary, it can no longer be modeled using a normal distribution (which is assumed in linear regression). We will now assume the response variable follows a <strong>Bernoulli</strong> distribution. Another assumption we make is that observations are independent of each other.</p>
<ul>
<li>A Bernoulli variable has a probability distribution <span class="math inline">\(P(y_i = 1) = \pi_i\)</span> and <span class="math inline">\(P(y_i = 0) = 1-\pi_i\)</span>.</li>
<li>The expectation is <span class="math inline">\(E(y_i) = \pi_i\)</span>.</li>
<li>The variance is <span class="math inline">\(Var(y_i) = \pi_i (1-\pi_i)\)</span>.</li>
</ul>
<p>It may be tempting to try to fit a linear regression model in this framework, i.e.</p>
<p><span class="math display">\[
E(y_i) = \pi_i = \beta_0 + \beta_1 x_1 + \cdots + \beta_k x_k.
\]</span>
However, this formulation does not work. The estimated value for <span class="math inline">\(\hat{\pi_i}\)</span> has to be between 0 and 1. Nothing in this formulation ensures this.</p>
<div id="the-logistic-regression-equation" class="section level3" number="11.2.1">
<h3>
<span class="header-section-number">11.2.1</span> The logistic regression equation<a class="anchor" aria-label="anchor" href="#the-logistic-regression-equation"><i class="fas fa-link"></i></a>
</h3>
<p>Instead, the logistic regression equation is written as</p>
<p><span class="math display" id="eq:11logistic">\[\begin{equation}
\log(\frac{\pi}{1-\pi})=\beta_0+\beta_1x_1+...+\beta_{k} x_{k} = \boldsymbol{X \beta},
\tag{11.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{X}\)</span> and <span class="math inline">\(\boldsymbol{\beta}\)</span> denote the design matrix and vector of parameters respectively. The formulation in <a href="logistic1.html#eq:11logistic">(11.1)</a> has the following characteristics:</p>
<ul>
<li>The estimated log odds, <span class="math inline">\(\log \left( \frac{\hat{\pi}}{1-\hat{\pi}} \right)\)</span>, will be any real number.</li>
<li>The estimated probability, <span class="math inline">\(\hat{\pi}\)</span>, will be between 0 and 1.</li>
<li>The log odds is expressed as a linear combination of the predictors.</li>
<li>The log odds is a transformed version of the mean of the response.<br>
</li>
<li>This transformation <span class="math inline">\(\log \left( \frac{\pi}{1-\pi} \right)\)</span> is called a <strong>logit link</strong>, and is denoted as <span class="math inline">\(logit(\pi)\)</span>.</li>
</ul>
<p>Two algebraically equivalent expressions of the logistic regression equation <a href="logistic1.html#eq:11logistic">(11.1)</a> are</p>
<p><span class="math display" id="eq:11odds">\[\begin{equation}
\text{Odds: }\frac{\pi}{1-\pi}=e^{\beta_0+\beta_1x_1+...+\beta_{k} x_{k}} = \exp(\boldsymbol{X \beta})
\tag{11.2}
\end{equation}\]</span></p>
<p>and</p>
<p><span class="math display" id="eq:11prob">\[\begin{equation}
\text{Probability: }\pi=\frac{e^{\beta_0+\beta_1x_1+...+\beta_{k} x_{k}}}{1+e^{\beta_0+\beta_1x_1+...+\beta_{k} x_{k}}} = \frac{\exp(\boldsymbol{X \beta})}{1+\exp(\boldsymbol{X \beta})}.
\tag{11.3}
\end{equation}\]</span></p>
<p>Suppose we have just one predictor, <span class="math inline">\(x\)</span>, and we wish to plot probability against <span class="math inline">\(x\)</span>, we will have (assuming <span class="math inline">\(\beta_1\)</span> is positive)</p>
<div class="inline-figure">
<img src="images/graph.jpg"><!-- -->
</div>
<p>So we see that with a positive coefficient for the predictor, the probability increases as the predictor increases. However, the increase is not linear. It is the log odds that increases linearly with the predictor, not the probability.</p>
<div id="thought-question-1" class="section level4" number="11.2.1.1">
<h4>
<span class="header-section-number">11.2.1.1</span> Thought question<a class="anchor" aria-label="anchor" href="#thought-question-1"><i class="fas fa-link"></i></a>
</h4>
<p>How are probability and odds related? In other words, given the odds, how can we quickly calculate the probability?</p>
</div>
</div>
</div>
<div id="coefficient-estimation-in-logistic-regression" class="section level2" number="11.3">
<h2>
<span class="header-section-number">11.3</span> Coefficient Estimation in Logistic Regression<a class="anchor" aria-label="anchor" href="#coefficient-estimation-in-logistic-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Recall that we used the method of least squares to estimate the coefficients in a linear regression model. So a reasonable thought would be to apply a similar idea in a logistic regression framework, i.e. minimize</p>
<p><span class="math display">\[
\sum_{i=1}^n (y_i - \boldsymbol{x_i^{\prime} \hat{\beta}})^2
\]</span></p>
<p>with respect to <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span>, and using a rule that the fitted response <span class="math inline">\(\boldsymbol{x_i^{\prime} \hat{\beta}}\)</span> is rounded to either 0 or 1.</p>
<p>From a purely predictive standpoint, this idea could work. However, statistical inference (intervals, hypothesis tests) is not reliable as the distribution of the response variable is now Bernoulli and not normal. Although the linear regression model is robust to the normality assumption, we still need the distribution to be continuous, not discrete.</p>
<p>For logistic regression, the coefficients are estimated using a different method, called the method of <strong>maximum likelihood</strong>.</p>
<div id="maximum-likelihood-estimation-for-logistic-regression" class="section level3" number="11.3.1">
<h3>
<span class="header-section-number">11.3.1</span> Maximum likelihood estimation for logistic regression<a class="anchor" aria-label="anchor" href="#maximum-likelihood-estimation-for-logistic-regression"><i class="fas fa-link"></i></a>
</h3>
<p>This subsection will give a very brief overview of maximum likelihood estimation in logistic regression.</p>
<p>The motivation behind <strong>maximum likelihood estimation</strong> is that model parameters are estimated by maximizing the likelihood that the process described by the model produced the observed data.</p>
<p>Since we assume the response variable follows a Bernoulli distribution, the probability distribution of each observation is</p>
<p><span class="math display" id="eq:11dist">\[\begin{equation}
f_i(y_i) = \pi_i^{y_i} (1 - \pi_i)^{1-y_i}
\tag{11.4}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(y_i\)</span> is either 0 or 1. Since we assume the observations are independent, the likelihood function is</p>
<p><span class="math display" id="eq:11likelihood">\[\begin{equation}
L(y_1, \cdots, y_n, \boldsymbol{\beta}) = \prod_{i=1}^n f_i(y_i) = \prod_{i=1}^n \pi_i^{y_i} (1 - \pi_i)^{1-y_i}
\tag{11.5}
\end{equation}\]</span></p>
<p>We want to maximize the likelihood function <a href="logistic1.html#eq:11likelihood">(11.5)</a> with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>. Since maximizing a function is the same as maximizing the log of a function, we can instead maximize the log-likelihood function</p>
<p><span class="math display" id="eq:11log">\[\begin{equation}
\log L(y_1, \cdots, y_n, \boldsymbol{\beta}).
\tag{11.6}
\end{equation}\]</span></p>
<p>It turns out that for logistic regression, it is computationally more efficient to maximize the log-likelihood function <a href="logistic1.html#eq:11log">(11.6)</a> instead of the likelihood function <a href="logistic1.html#eq:11likelihood">(11.5)</a>.</p>
<p>Using <span class="math inline">\(\pi_i=\frac{\exp(\boldsymbol{x_i^{\prime}\beta})}{1 + \exp(\boldsymbol{x_i^{\prime}\beta})}\)</span> from <a href="logistic1.html#eq:11prob">(11.3)</a> and after some algebra on the log-likelihood function <a href="logistic1.html#eq:11log">(11.6)</a>, we maximize the following with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span></p>
<p><span class="math display" id="eq:11max">\[\begin{equation}
\log L(\boldsymbol{y}, \boldsymbol{\beta}) = \sum_{i=1}^n y_i \boldsymbol{x_i^{\prime}\beta} - \sum_{i=1}^n \log [ 1 + \exp(\boldsymbol{x_i^{\prime}\beta}) ]
\tag{11.7}
\end{equation}\]</span></p>
<p>There is no closed-form solution to maximizing <a href="logistic1.html#eq:11max">(11.7)</a>. A numerical method called iteratively reweighted least squares is used. The details of which are beyond the scope of this class.</p>
<p><em>Please view the video for a more thorough explanation on maximum likelihood estimation.</em></p>
</div>
</div>
<div id="interpreting-coefficients-in-logistic-regression" class="section level2" number="11.4">
<h2>
<span class="header-section-number">11.4</span> Interpreting Coefficients in Logistic Regression<a class="anchor" aria-label="anchor" href="#interpreting-coefficients-in-logistic-regression"><i class="fas fa-link"></i></a>
</h2>
<p>There are a few equivalent interpretations of the coefficient of a predictor. For <span class="math inline">\(\beta_1\)</span>, they are:</p>
<ul>
<li>For a one-unit increase in the predictor <span class="math inline">\(x_1\)</span>, the <strong>log odds changes by <span class="math inline">\(\beta_1\)</span></strong>, while holding other predictors constant.</li>
<li>For a one-unit increase in the predictor <span class="math inline">\(x_1\)</span>, the <strong>odds are multiplied by a factor of <span class="math inline">\(\exp(\beta_1)\)</span></strong>, while holding other predictors constant.</li>
</ul>
<p>Let us go back to the driving while drunk example. The response variable is whether the student has driven while drunk, with 1 denoting “yes” and 0 denoting “no”. Let us consider two predictors: <span class="math inline">\(x_1\)</span>: <code>PartyNum</code> which denotes the number of days the student parties in a month, on average, and a categorical predictor <code>Gender</code>. The estimated coefficients are shown below:</p>
<div class="sourceCode" id="cb455"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">result2</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">DrivDrnk</span><span class="op">~</span><span class="va">PartyNum</span><span class="op">+</span><span class="va">Gender</span>, family<span class="op">=</span><span class="va">binomial</span>, data<span class="op">=</span><span class="va">train</span><span class="op">)</span></span>
<span><span class="va">result2</span></span></code></pre></div>
<pre><code>## 
## Call:  glm(formula = DrivDrnk ~ PartyNum + Gender, family = binomial, 
##     data = train)
## 
## Coefficients:
## (Intercept)     PartyNum   Gendermale  
##     -1.2433       0.1501       0.4136  
## 
## Degrees of Freedom: 121 Total (i.e. Null);  119 Residual
##   (2 observations deleted due to missingness)
## Null Deviance:       169.1 
## Residual Deviance: 151.9     AIC: 157.9</code></pre>
<p>So the estimated logistic regression equation is</p>
<p><span class="math display">\[
\log \left( \frac{\hat{\pi}}{1-\hat{\pi}} \right) = -1.2433 + 0.1501x_1 + 0.4136I_1,
\]</span></p>
<p>where <span class="math inline">\(I_1\)</span> is 1 for male students and 0 for female students.</p>
<p>The estimated coefficient for <span class="math inline">\(x_1\)</span> is <span class="math inline">\(\hat{\beta_1} = 0.1501\)</span>, and can be interpreted as</p>
<ul>
<li>The estimated log odds of having driven while drunk for college students increases by 0.1501 for each additional day of partying, when controlling for gender.</li>
<li>The estimated odds of having driven while drunk for college students is multiplied by <span class="math inline">\(\exp(0.1501) = 1.1620\)</span> for each additional day of partying, when controlling for gender.</li>
</ul>
<p>The estimated coefficient for <span class="math inline">\(I_1\)</span> is <span class="math inline">\(\hat{\beta_2} = 0.4136\)</span>, and can be interpreted as</p>
<ul>
<li>The estimated log odds of having driven while drunk for college students is 0.4136 higher for males than females, when controlling for the number of days they party.</li>
<li>The estimated odds of having driven while drunk for male college students is <span class="math inline">\(\exp(0.4136) = 1.5123\)</span> times the odds for female college students, when controlling for the number of days they party.</li>
</ul>
<p><em>Please view the video for a more thorough explanation on interpreting the coefficients in logistic regression.</em></p>
</div>
<div id="inference-in-logistic-regression" class="section level2" number="11.5">
<h2>
<span class="header-section-number">11.5</span> Inference in Logistic Regression<a class="anchor" aria-label="anchor" href="#inference-in-logistic-regression"><i class="fas fa-link"></i></a>
</h2>
<p>There are a number of hypothesis tests that we can conduct in logistic regression. These tests have analogous versions with linear regression.</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="28%">
<col width="40%">
<col width="32%">
</colgroup>
<thead><tr class="header">
<th align="center"></th>
<th align="center">Logistic</th>
<th align="center">Linear</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center">Drop single term</td>
<td align="center">Wald (Z) test</td>
<td align="center">
<span class="math inline">\(t\)</span> test</td>
</tr>
<tr class="even">
<td align="center">Is model useful?</td>
<td align="center">Likelihood ratio test</td>
<td align="center">ANOVA <span class="math inline">\(F\)</span> test</td>
</tr>
<tr class="odd">
<td align="center">Full vs reduced models</td>
<td align="center">Likelihood ratio test</td>
<td align="center">General linear <span class="math inline">\(F\)</span> test</td>
</tr>
</tbody>
</table></div>
<div id="wald-test" class="section level3" number="11.5.1">
<h3>
<span class="header-section-number">11.5.1</span> Wald test<a class="anchor" aria-label="anchor" href="#wald-test"><i class="fas fa-link"></i></a>
</h3>
<p>We can assess whether to drop a single term, with a <strong>Wald test</strong>, which is basically a <span class="math inline">\(Z\)</span> test. We can test <span class="math inline">\(H_0: \beta_j = 0\)</span> with the Z statistic</p>
<p><span class="math display" id="eq:11Z">\[\begin{equation}
Z = \frac{\hat{\beta}_j - 0}{se(\hat{\beta}_j)},
\tag{11.8}
\end{equation}\]</span></p>
<p>which is compared with a standard normal distribution. A standard normal distribution is denoted by <span class="math inline">\(N(0,1)\)</span>, which is read as a normal distribution with mean 0 and variance 1. Let us look at the Wald tests in our earlier drink driving example:</p>
<div class="sourceCode" id="cb457"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">result2</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = DrivDrnk ~ PartyNum + Gender, family = binomial, 
##     data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1357  -1.0211  -0.1941   1.0701   1.7302  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.24333    0.38795  -3.205  0.00135 ** 
## PartyNum     0.15012    0.04216   3.560  0.00037 ***
## Gendermale   0.41363    0.40466   1.022  0.30670    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 169.13  on 121  degrees of freedom
## Residual deviance: 151.93  on 119  degrees of freedom
##   (2 observations deleted due to missingness)
## AIC: 157.93
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<p>To assess the coefficient for <code>PartyNum</code>, we have</p>
<ul>
<li>
<span class="math inline">\(H_0: \beta_1 = 0, H_a: \beta_1 \neq 0\)</span>.</li>
<li>Test statistic <span class="math inline">\(Z = \frac{\hat{\beta}_1 - 0}{se(\hat{\beta}_1)} = \frac{0.15012}{0.04216} = 3.560\)</span>.</li>
<li>P-value of 0.00037 (or using <code>2*(1-pnorm(abs(3.560)))</code> in R).</li>
<li>Reject the null. Do not drop <code>PartyNum</code> from the logistic regression model.</li>
</ul>
<p>To assess the coefficient for <code>Gender</code>, we have a large p-value, which means we can drop <code>Gender</code> from the model and leave <code>PartyNum</code> in.</p>
<div id="pratice" class="section level4" number="11.5.1.1">
<h4>
<span class="header-section-number">11.5.1.1</span> Pratice<a class="anchor" aria-label="anchor" href="#pratice"><i class="fas fa-link"></i></a>
</h4>
<p>Verify the Z statistic and p-value when testing <span class="math inline">\(H_0: \beta_2 = 0, H_a: \beta_2 \neq 0\)</span> in this example.</p>
</div>
</div>
<div id="confidence-intervals-for-coefficients" class="section level3" number="11.5.2">
<h3>
<span class="header-section-number">11.5.2</span> Confidence intervals for coefficients<a class="anchor" aria-label="anchor" href="#confidence-intervals-for-coefficients"><i class="fas fa-link"></i></a>
</h3>
<p>A <span class="math inline">\((1-\alpha) \times 100\%\)</span> confidence interval for <span class="math inline">\(\beta_j\)</span> is</p>
<p><span class="math display" id="eq:11CI">\[\begin{equation}
\hat{\beta}_j \pm Z_{1- \alpha/2} se(\hat{\beta}_j).
\tag{11.9}
\end{equation}\]</span></p>
<p>Going back to our drink driving example, the <span class="math inline">\(95\%\)</span> CI for <span class="math inline">\(\beta_1\)</span> is:</p>
<p><span class="math display">\[\begin{eqnarray*}
\hat{\beta_1} &amp;\pm&amp; Z_{0.975} se(\hat{\beta}_1) \nonumber \\
= 0.15012 &amp;\pm&amp; 1.96 \times 0.04216 \nonumber \\
= (0.0675&amp;,&amp; 0.2328). \nonumber
\end{eqnarray*}\]</span></p>
<p>Note that <span class="math inline">\(Z_{0.975}\)</span> is found using <code>qnorm(0.975)</code>. Based on this confidence interval, we can say that:</p>
<ul>
<li>We are 95% confident the true <span class="math inline">\(\beta_1\)</span> is between 0.0675 and 0.2328.</li>
<li>CI excludes 0, so coefficient is significant. Do not drop term from model.</li>
<li>Consistent conclusion with 2-sided hypothesis test at <span class="math inline">\(\alpha = 0.05\)</span>.</li>
<li>The log odds of having driven while drunk for college students increases between 0.0675 and 0.2328 for each additional day of partying, when controlling for gender.</li>
<li>The odds of having driven while drunk for college students is multiplied by a factor between <span class="math inline">\(\exp(0.0675) = 1.0698\)</span> and <span class="math inline">\(\exp(0.2328) = 1.2621\)</span> for each additional day of partying, when controlling for gender.</li>
</ul>
</div>
<div id="likelihood-ratio-tests-in-logistic-regression" class="section level3" number="11.5.3">
<h3>
<span class="header-section-number">11.5.3</span> Likelihood ratio tests in logistic regression<a class="anchor" aria-label="anchor" href="#likelihood-ratio-tests-in-logistic-regression"><i class="fas fa-link"></i></a>
</h3>
<p>Likelihood ratio tests (LRTs) allow us to compare between a full and reduced model, denoted by <span class="math inline">\(F\)</span> and <span class="math inline">\(R\)</span> respectively. The test statistic measures the difference in deviances of the models, i.e.</p>
<p><span class="math display" id="eq:11LRT">\[\begin{equation}
\Delta G^2 = D(R) - D(F),
\tag{11.10}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(D(R)\)</span> and <span class="math inline">\(D(F)\)</span> denote the deviance of the reduced model and deviance of the full model respectively. The deviance of a model is analogous to the <span class="math inline">\(SS_{res}\)</span> of a linear regression model.</p>
<p>The test statistic <span class="math inline">\(\Delta G^2\)</span> is then compared with a chi-squared distribution, denoted by <span class="math inline">\(\chi^2_{df}\)</span>, where df denotes the number of parameters you are dropping to get the reduced model.</p>
<p>The deviance of a model is labeled as <strong>residual deviance</strong> in R. The <strong>null deviance</strong> in R is the deviance of an intercept-only model. Going back to our drink driving example, with predictors <code>PartyNum</code> and <code>Gender</code>, the residual deviance is 151.93, and the null deviance is 169.13. So we can compare our model with an intercept-only model.</p>
<ul>
<li><span class="math inline">\(H_0: \beta_1 = \beta_2 = 0, H_a: \text{ at least one coefficient in null is nonzero.}\)</span></li>
<li>
<span class="math inline">\(\Delta G^2 = D(R) - D(F) = 169.13 - 151.93 = 17.2\)</span>.</li>
<li>P-value is <code>1-pchisq(17.2,2)</code> which is close to 0.</li>
<li>Critical value is <code>qchisq(0.95,2)</code> which is 5.9915.</li>
<li>We reject the null hypothesis and support our two predictor over the intercept-only model.</li>
</ul>
</div>
</div>
<div id="r-tutorial-8" class="section level2" number="11.6">
<h2>
<span class="header-section-number">11.6</span> R Tutorial<a class="anchor" aria-label="anchor" href="#r-tutorial-8"><i class="fas fa-link"></i></a>
</h2>
<p>In this tutorial, we will learn how to fit a (binary) logistic regression model in R. Logistic regression is used when the response variable is binary. We model the log odds of “success” as a linear combination of coefficients and predictors:</p>
<p><span class="math display">\[
\log(\frac{\pi}{1-\pi}) = \beta_0 + \beta_1 x_1 + \cdots \beta_{k} x_{k}.
\]</span>
We have a dataset, <code>students.txt</code>, that contains information on about 250 college students at a large public university and their study and party habits. The variables are:</p>
<ul>
<li>
<code>Gender</code>: gender of student</li>
<li>
<code>Smoke</code>: whether the student smokes</li>
<li>
<code>Marijuan</code>: whether the student uses marijuana</li>
<li>
<code>DrivDrnk</code>: whether the student has driven while drunk</li>
<li>
<code>GPA</code>: student’s GPA</li>
<li>
<code>PartyNum</code>: number of times the student parties in a month</li>
<li>
<code>DaysBeer</code>: number of days the student drinks at least 2 beers in a month</li>
<li>
<code>StudyHrs</code>: number of hours the students studies in a week</li>
</ul>
<p>Suppose we want to relate the likelihood of a student driving while drunk with the other variables. Notice that the response variable, <code>DrivDrnk</code> is a binary variable with yes or no as levels. When the response variable is binary and not quantitative, we have to use logistic regression instead of linear regression.</p>
<p>Let us read the data in:</p>
<div class="sourceCode" id="cb459"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span><span class="va">Data</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">"students.txt"</span>, header<span class="op">=</span><span class="cn">T</span>, sep<span class="op">=</span><span class="st">""</span><span class="op">)</span></span></code></pre></div>
<p>We are going to perform some basic data wrangling for our dataframe:</p>
<ul>
<li>Remove the first column, as it is just an index.</li>
<li>Apply <code><a href="https://rdrr.io/r/base/factor.html">factor()</a></code> to categorical variables. As a reminder, this should be done to categorical variables if you want to change the reference class.</li>
</ul>
<div class="sourceCode" id="cb460"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##first column is index, remove it</span></span>
<span><span class="va">Data</span><span class="op">&lt;-</span><span class="va">Data</span><span class="op">[</span>,<span class="op">-</span><span class="fl">1</span><span class="op">]</span></span>
<span></span>
<span><span class="co">##convert categorical to factors. needed for contrasts</span></span>
<span><span class="va">Data</span><span class="op">$</span><span class="va">Gender</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">Data</span><span class="op">$</span><span class="va">Gender</span><span class="op">)</span></span>
<span><span class="va">Data</span><span class="op">$</span><span class="va">Smoke</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">Data</span><span class="op">$</span><span class="va">Smoke</span><span class="op">)</span></span>
<span><span class="va">Data</span><span class="op">$</span><span class="va">Marijuan</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">Data</span><span class="op">$</span><span class="va">Marijuan</span><span class="op">)</span></span>
<span><span class="va">Data</span><span class="op">$</span><span class="va">DrivDrnk</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">Data</span><span class="op">$</span><span class="va">DrivDrnk</span><span class="op">)</span></span></code></pre></div>
<p>We are going to split the dataset into equal sets: one a training set, and another a test set. Recall that the training set is used to build the model, and the test set is used to assess how the model performs on new observations. We use the <code><a href="https://rdrr.io/r/base/Random.html">set.seed()</a></code> function so that we can replicate the same split each time this block of code is run. An integer needs to be supplied to the function.</p>
<div class="sourceCode" id="cb461"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##set seed so results (split) are reproducible</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">6021</span><span class="op">)</span></span>
<span></span>
<span><span class="co">##evenly split data into train and test sets</span></span>
<span><span class="va">sample.data</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample.int</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">Data</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">floor</a></span><span class="op">(</span><span class="fl">.50</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">Data</span><span class="op">)</span><span class="op">)</span>, replace <span class="op">=</span> <span class="cn">F</span><span class="op">)</span></span>
<span><span class="va">train</span><span class="op">&lt;-</span><span class="va">Data</span><span class="op">[</span><span class="va">sample.data</span>, <span class="op">]</span></span>
<span><span class="va">test</span><span class="op">&lt;-</span><span class="va">Data</span><span class="op">[</span><span class="op">-</span><span class="va">sample.data</span>, <span class="op">]</span></span></code></pre></div>
<div id="visualizations-with-logistic-regression" class="section level3 unnumbered">
<h3>Visualizations with Logistic Regression<a class="anchor" aria-label="anchor" href="#visualizations-with-logistic-regression"><i class="fas fa-link"></i></a>
</h3>
<p>Given that the response variable, <code>DrivDrnk</code>, is categorical, we use slightly different visualizations than with linear regression.</p>
<div id="barcharts" class="section level4 unnumbered">
<h4>Barcharts<a class="anchor" aria-label="anchor" href="#barcharts"><i class="fas fa-link"></i></a>
</h4>
<p>Barcharts are useful to visualize how categorical predictors may be related to the categorical response variable. Since we have three categorical predictors, <code>Gender</code>, <code>Smoke</code>, and <code>Marijuan</code>, we will be creating three barcharts:</p>
<div class="sourceCode" id="cb462"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">chart1</span><span class="op">&lt;-</span><span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">train</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">Gender</span>, fill<span class="op">=</span><span class="va">DrivDrnk</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_bar.html">geom_bar</a></span><span class="op">(</span>position <span class="op">=</span> <span class="st">"fill"</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x<span class="op">=</span><span class="st">"Gender"</span>, y<span class="op">=</span><span class="st">"Proportion"</span>,</span>
<span>       title<span class="op">=</span><span class="st">"Proportion of Driven Drunk by Gender"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">chart2</span><span class="op">&lt;-</span><span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">train</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">Smoke</span>, fill<span class="op">=</span><span class="va">DrivDrnk</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_bar.html">geom_bar</a></span><span class="op">(</span>position <span class="op">=</span> <span class="st">"fill"</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x<span class="op">=</span><span class="st">"Smokes?"</span>, y<span class="op">=</span><span class="st">"Proportion"</span>,</span>
<span>       title<span class="op">=</span><span class="st">"Proportion of Driven Drunk by Smoking Status"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">chart3</span><span class="op">&lt;-</span><span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">train</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">Marijuan</span>, fill<span class="op">=</span><span class="va">DrivDrnk</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_bar.html">geom_bar</a></span><span class="op">(</span>position <span class="op">=</span> <span class="st">"fill"</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x<span class="op">=</span><span class="st">"Use Marijuana?"</span>, y<span class="op">=</span><span class="st">"Proportion"</span>,</span>
<span>       title<span class="op">=</span><span class="st">"Proportion of Driven Drunk by Marijuana Status"</span><span class="op">)</span></span></code></pre></div>
<p>Instead of displaying these barcharts individually, we can display them simultaneously, using the <code><a href="https://rdrr.io/pkg/gridExtra/man/arrangeGrob.html">grid.arrange()</a></code> function from the <code>gridExtra</code> package. We will display these 3 barcharts in a 2 by 2 matrix:</p>
<div class="sourceCode" id="cb463"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##put barcharts in a matrix</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">gridExtra</span><span class="op">)</span></span>
<span><span class="fu">gridExtra</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/gridExtra/man/arrangeGrob.html">grid.arrange</a></span><span class="op">(</span><span class="va">chart1</span>, <span class="va">chart2</span>, <span class="va">chart3</span>, ncol <span class="op">=</span> <span class="fl">2</span>, nrow <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-298-1.png" width="672"></div>
<ul>
<li>We can see that a slightly higher proportion of male students have driven drunk, compared to female students.</li>
<li>There is a much higher proportion of smokers who have driven drunk, compared to non smokers.</li>
<li>Similarly, a higher proportion of students who use marijuana have driven drunk, compared to non users.</li>
<li>Each of these categorical predictors have some relationship with whether the student has driven drunk.</li>
</ul>
</div>
<div id="two-way-tables-1" class="section level4 unnumbered">
<h4>Two way tables<a class="anchor" aria-label="anchor" href="#two-way-tables-1"><i class="fas fa-link"></i></a>
</h4>
<p>We can create two way tables to summarize the relationship between each categorical predictor and whether students have driven drunk:</p>
<div class="sourceCode" id="cb464"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##two way tables of counts</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">train</span><span class="op">$</span><span class="va">Gender</span>, <span class="va">train</span><span class="op">$</span><span class="va">DrivDrnk</span><span class="op">)</span></span></code></pre></div>
<pre><code>##         
##          No Yes
##   female 47  32
##   male   21  24</code></pre>
<div class="sourceCode" id="cb466"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">train</span><span class="op">$</span><span class="va">Smoke</span>, <span class="va">train</span><span class="op">$</span><span class="va">DrivDrnk</span><span class="op">)</span></span></code></pre></div>
<pre><code>##      
##       No Yes
##   No  63  39
##   Yes  5  17</code></pre>
<div class="sourceCode" id="cb468"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">train</span><span class="op">$</span><span class="va">Marijuan</span>, <span class="va">train</span><span class="op">$</span><span class="va">DrivDrnk</span><span class="op">)</span></span></code></pre></div>
<pre><code>##      
##       No Yes
##   No  50  19
##   Yes 18  37</code></pre>
<p>Or we can display the tables via proportions instead of counts:</p>
<div class="sourceCode" id="cb470"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##two way tables using proportions</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">train</span><span class="op">$</span><span class="va">Gender</span>, <span class="va">train</span><span class="op">$</span><span class="va">DrivDrnk</span><span class="op">)</span>,<span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<pre><code>##         
##                 No       Yes
##   female 0.5949367 0.4050633
##   male   0.4666667 0.5333333</code></pre>
<div class="sourceCode" id="cb472"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">train</span><span class="op">$</span><span class="va">Smoke</span>, <span class="va">train</span><span class="op">$</span><span class="va">DrivDrnk</span><span class="op">)</span>,<span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<pre><code>##      
##              No       Yes
##   No  0.6176471 0.3823529
##   Yes 0.2272727 0.7727273</code></pre>
<div class="sourceCode" id="cb474"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">train</span><span class="op">$</span><span class="va">Marijuan</span>, <span class="va">train</span><span class="op">$</span><span class="va">DrivDrnk</span><span class="op">)</span>,<span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<pre><code>##      
##              No       Yes
##   No  0.7246377 0.2753623
##   Yes 0.3272727 0.6727273</code></pre>
<ul>
<li>About 53% of male students have driven drunk, compared to about 41% of female students.</li>
<li>About 77% of smokers have driven drunk, compared to 38% of non smokers.</li>
<li>About 67% of marijuana users have driven drunk, compared to 28% of non users.</li>
</ul>
<p>The earlier barcharts are the visualizations of these tables.</p>
</div>
<div id="quantitative-predictors" class="section level4 unnumbered">
<h4>Quantitative predictors<a class="anchor" aria-label="anchor" href="#quantitative-predictors"><i class="fas fa-link"></i></a>
</h4>
<p>To see how the quantitative predictors, <code>GPA</code>, <code>PartyNum</code>, <code>DaysBeer</code>, and <code>StudyHrs</code> may differ between those who have driven drunk and those who have not, we can compare the distributions of these quantitative variables between the two groups:</p>
<div class="sourceCode" id="cb476"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dp1</span><span class="op">&lt;-</span><span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">train</span>,<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">GPA</span>, color<span class="op">=</span><span class="va">DrivDrnk</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_density.html">geom_density</a></span><span class="op">(</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title<span class="op">=</span><span class="st">"Density Plot of GPA by Driven Drunk"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">dp2</span><span class="op">&lt;-</span><span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">train</span>,<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">PartyNum</span>, color<span class="op">=</span><span class="va">DrivDrnk</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_density.html">geom_density</a></span><span class="op">(</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title<span class="op">=</span><span class="st">"Density Plot of Number of Party Days by Driven Drunk"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">dp3</span><span class="op">&lt;-</span><span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">train</span>,<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">DaysBeer</span>, color<span class="op">=</span><span class="va">DrivDrnk</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_density.html">geom_density</a></span><span class="op">(</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title<span class="op">=</span><span class="st">"Density Plot of Number of Days drank Beer by Driven Drunk"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">dp4</span><span class="op">&lt;-</span><span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">train</span>,<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">StudyHrs</span>, color<span class="op">=</span><span class="va">DrivDrnk</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_density.html">geom_density</a></span><span class="op">(</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title<span class="op">=</span><span class="st">"Density Plot of Study Hours by Driven Drunk"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">gridExtra</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/gridExtra/man/arrangeGrob.html">grid.arrange</a></span><span class="op">(</span><span class="va">dp1</span>, <span class="va">dp2</span>, <span class="va">dp3</span>, <span class="va">dp4</span>, ncol <span class="op">=</span> <span class="fl">2</span>, nrow <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<pre><code>## Warning: Removed 2 rows containing non-finite values (`stat_density()`).</code></pre>
<pre><code>## Warning: Removed 3 rows containing non-finite values (`stat_density()`).</code></pre>
<div class="inline-figure"><img src="bookdown-demo_files/figure-html/unnamed-chunk-301-1.png" width="672"></div>
<ul>
<li>Among those who have driven drunk, we see a fairly uniform proportion of students with GPAs between 2.75 and 3.5. For those who have not driven drunk, a higher proportion of students have higher GPAs (above 3.0).</li>
<li>A higher proportion of students who have not driven drunk party less than 10 days a month, compared to students who have driven drunk.</li>
<li>Similarly, a higher proportion of students who have not driven drunk spend less than 10 days drinking beer, compared to students who have driven drunk.</li>
<li>The density plots of study hours are almost identical for those who have driven drunk and those who have not driven drunk. This suggests study hours is not associated with the likelihood of having driven drunk.</li>
<li>It looks like lower GPAs, partying more, and drinking more are associated with increased likelihood of having driven drunk.</li>
</ul>
</div>
<div id="correlations-between-quantitative-predictors" class="section level4 unnumbered">
<h4>Correlations between quantitative predictors<a class="anchor" aria-label="anchor" href="#correlations-between-quantitative-predictors"><i class="fas fa-link"></i></a>
</h4>
<p>We can quickly check the correlations between the quantitative predictors:</p>
<div class="sourceCode" id="cb479"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##correlations between quantitative predictors</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">train</span><span class="op">[</span>,<span class="fl">5</span><span class="op">:</span><span class="fl">8</span><span class="op">]</span>, use<span class="op">=</span> <span class="st">"complete.obs"</span><span class="op">)</span>,<span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<pre><code>##             GPA PartyNum DaysBeer StudyHrs
## GPA       1.000   -0.198   -0.246    0.094
## PartyNum -0.198    1.000    0.760   -0.067
## DaysBeer -0.246    0.760    1.000   -0.162
## StudyHrs  0.094   -0.067   -0.162    1.000</code></pre>
<p>Notice that <code>DaysBeer</code> and <code>PartyNum</code> are highly correlated. Probably not surprising since drinking is probably done at parties, so a student who parties more is likely to drink more.</p>
</div>
</div>
<div id="fit-logistic-regression" class="section level3 unnumbered">
<h3>Fit Logistic Regression<a class="anchor" aria-label="anchor" href="#fit-logistic-regression"><i class="fas fa-link"></i></a>
</h3>
<p>Based on the visualizations, we suspect that all the predictors, other than <code>StudyHrs</code>, may influence the response variable. We use the <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> function to fit logistic regression using of these predictors as a starting point:</p>
<div class="sourceCode" id="cb481"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##fit logistic regression</span></span>
<span><span class="va">result</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">DrivDrnk</span><span class="op">~</span><span class="va">.</span>, family<span class="op">=</span><span class="va">binomial</span>, data<span class="op">=</span><span class="va">train</span><span class="op">)</span></span></code></pre></div>
<p>Notice we specify the argument <code>family = "binomial"</code>. This has to be specified for a logistic regression. If this is not specified, a linear regression is fitted instead. The function uses maximum likelihood estimation whereas uses ordinary least squares. We can look at the Wald tests and deviance of our model using <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code>:</p>
<div class="sourceCode" id="cb482"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">result</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = DrivDrnk ~ ., family = binomial, data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9055  -0.8746  -0.4923   0.8913   2.0261  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -3.96023    2.04822  -1.933   0.0532 .
## Gendermale   0.54708    0.45355   1.206   0.2277  
## SmokeYes     1.43666    0.63046   2.279   0.0227 *
## MarijuanYes  0.73292    0.49107   1.492   0.1356  
## GPA          0.51899    0.56048   0.926   0.3545  
## PartyNum     0.04523    0.07112   0.636   0.5248  
## DaysBeer     0.09104    0.06011   1.515   0.1299  
## StudyHrs     0.01341    0.02405   0.558   0.5771  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 163.55  on 118  degrees of freedom
## Residual deviance: 130.73  on 111  degrees of freedom
##   (5 observations deleted due to missingness)
## AIC: 146.73
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>We make the following observations:</p>
<ul>
<li>Notice only <code>Smoke</code> is significant.</li>
<li>We did note earlier that <code>PartyNum</code> and <code>DaysBeer</code> are highly correlated, so both being insignificant may be due to multicollinearity, and not that they are not related to the response variable. The visualizations earlier showed they may be significant predictors.</li>
<li>
<code>Marijuan</code> is also barely insignificant, which may be a bit surprising since we did see that it could be a significant predictor from the visualizations.</li>
</ul>
<p>We can also assess variance inflation factors (VIFs) in logistic regression like before:</p>
<div class="sourceCode" id="cb484"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/julianfaraway/faraway">faraway</a></span><span class="op">)</span></span>
<span><span class="fu">faraway</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/faraway/man/vif.html">vif</a></span><span class="op">(</span><span class="va">result</span><span class="op">)</span></span></code></pre></div>
<pre><code>##  Gendermale    SmokeYes MarijuanYes         GPA    PartyNum    DaysBeer 
##    5.704470    6.874033    7.060265    6.447132   15.288866   16.032542 
##    StudyHrs 
##    5.558041</code></pre>
<p>Not surprisingly, we have evidence of multicollinearity.</p>
<div id="likelihood-ratio-tests" class="section level4 unnumbered">
<h4>Likelihood ratio tests<a class="anchor" aria-label="anchor" href="#likelihood-ratio-tests"><i class="fas fa-link"></i></a>
</h4>
<p>Given what we have seen, we try to fit a reduced model with <code>Smoke</code> (significant), <code>Marijuan</code> (barely insignificant but visualization suggested it may be significant), and <code>DaysBeer</code> (barely insignificant, which could be due to its correlation with <code>PartyNum</code> and visualzation suggested it may be significant).</p>
<p>Let us see if we can drop <code>Gender</code>, <code>GPA</code>, <code>PartyNum</code>, and <code>StudyHrs</code> from the model, via a likelihood ratio test (LRT). The <span class="math inline">\(\Delta G^2\)</span> test statistic is found by finding the difference in the deviances of the two models. The deviance can be found by extracting the component <code>deviance</code> from an object created by <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code>:</p>
<div class="sourceCode" id="cb486"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reduced</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">DrivDrnk</span><span class="op">~</span><span class="va">Smoke</span><span class="op">+</span><span class="va">Marijuan</span><span class="op">+</span><span class="va">DaysBeer</span>, family<span class="op">=</span><span class="va">binomial</span>, data<span class="op">=</span><span class="va">train</span><span class="op">)</span></span>
<span></span>
<span><span class="co">##test to compare reduced and full model</span></span>
<span><span class="co">##test stat</span></span>
<span><span class="va">TS</span><span class="op">&lt;-</span><span class="va">reduced</span><span class="op">$</span><span class="va">deviance</span><span class="op">-</span><span class="va">result</span><span class="op">$</span><span class="va">deviance</span></span>
<span><span class="va">TS</span></span></code></pre></div>
<pre><code>## [1] 6.996033</code></pre>
<div class="sourceCode" id="cb488"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##pvalue</span></span>
<span><span class="fl">1</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span><span class="va">TS</span>,<span class="fl">4</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.136098</code></pre>
<div class="sourceCode" id="cb490"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##critical value</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">qchisq</a></span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="fl">0.05</span>,<span class="fl">4</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 9.487729</code></pre>
<ul>
<li>The null hypothesis supports dropping the 4 predictors, and the alternative hypothesis supports not dropping the 4 predictors.</li>
<li>The <span class="math inline">\(\Delta G^2\)</span> test statistic is 6.996, and is compared with a <span class="math inline">\(\chi^2_4\)</span> distribution, i.e. a chi-squared distribution with 4 degrees of freedom. The degrees of freedom is equal to the number of terms we are dropping.</li>
<li>The p-value is 0.1361, and the critical value us 9.4877.</li>
<li>So we fail to reject the null hypothesis. Data do not support using the full model with all the predictors, so we drop <code>Gender</code>, <code>GPA</code>, <code>PartyNum</code>, and <code>StudyHrs</code> to use the reduced model.</li>
</ul>
<p>Let us take a look at the estimated coefficients for this model:</p>
<div class="sourceCode" id="cb492"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">reduced</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = DrivDrnk ~ Smoke + Marijuan + DaysBeer, family = binomial, 
##     data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.3539  -0.8492  -0.5685   0.9106   1.9506  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.74079    0.39459  -4.412 1.03e-05 ***
## SmokeYes     1.20960    0.58958   2.052  0.04021 *  
## MarijuanYes  0.93362    0.45504   2.052  0.04019 *  
## DaysBeer     0.11332    0.04305   2.632  0.00848 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 170.74  on 123  degrees of freedom
## Residual deviance: 137.73  on 120  degrees of freedom
## AIC: 145.73
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>So our logistic regression equation is</p>
<p><span class="math display">\[
\log \left( \frac{\hat{\pi}}{1-\hat{\pi}}   \right) = -1.7408 + 1.2096I_1 + 0.9336I_2 + 0.1133 DaysBeer,
\]</span>
where <span class="math inline">\(I_1 = 1\)</span> if the student smokes, and <span class="math inline">\(I_2 = 1\)</span> if the student uses marijuana.</p>
<ul>
<li>Given that these coefficients are positive, smoking, using marijuana, and drinking on more days are associated with higher likelihood of having driven drunk.</li>
<li>The odds of driving drunk for smokers is <span class="math inline">\(\exp(1.2096) = 3.3521\)</span> times the odds for non smokers, when controlling for marijuana use and days drinking.</li>
<li>The odds of driving drunk for marijuana users is <span class="math inline">\(\exp(0.9336) = 2.5437\)</span> times the odds for non users, when controlling for smoking and days drinking.</li>
<li>The odds of driving drunk is multiplied by a factor of <span class="math inline">\(\exp(0.1133) = 1.1200\)</span> for each additional day of drinking, when controlling for smoking and marijuana use.</li>
</ul>
</div>
<div id="predicted-log-odds-and-probabilities" class="section level4 unnumbered">
<h4>Predicted log odds and probabilities<a class="anchor" aria-label="anchor" href="#predicted-log-odds-and-probabilities"><i class="fas fa-link"></i></a>
</h4>
<p>We can use the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function to calculate predicted log odds for our test data, using the reduced model:</p>
<div class="sourceCode" id="cb494"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##predicted log odds for test data</span></span>
<span><span class="va">logodds</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">reduced</span>,newdata<span class="op">=</span><span class="va">test</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">logodds</span><span class="op">)</span></span></code></pre></div>
<pre><code>##          2          3          5          6          7          9 
## -1.7407941 -1.2875077  2.1022521  0.9690362 -1.7407941  1.4592572</code></pre>
<p>To find probabilities instead, supply <code>type="response"</code> within the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function:</p>
<div class="sourceCode" id="cb496"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##predicted probabilities for test data</span></span>
<span><span class="va">preds</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">reduced</span>,newdata<span class="op">=</span><span class="va">test</span>, type<span class="op">=</span><span class="st">"response"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">preds</span><span class="op">)</span></span></code></pre></div>
<pre><code>##         2         3         5         6         7         9 
## 0.1492121 0.2162750 0.8911219 0.7249273 0.1492121 0.8114190</code></pre>
<p>So for observation index 2, the student’s predicted log odds of driving drunk is -1.7407941, with corresponding predicted probability of 0.1492121.</p>

</div>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="out.html"><span class="header-section-number">10</span> Analysis of Residuals in MLR</a></div>
<div class="next"><a href="logistic2.html"><span class="header-section-number">12</span> Logistic Regression 2</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#logistic1"><span class="header-section-number">11</span> Logistic Regression</a></li>
<li><a class="nav-link" href="#introduction-10"><span class="header-section-number">11.1</span> Introduction</a></li>
<li>
<a class="nav-link" href="#logistic-regression"><span class="header-section-number">11.2</span> Logistic Regression</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#the-logistic-regression-equation"><span class="header-section-number">11.2.1</span> The logistic regression equation</a></li></ul>
</li>
<li>
<a class="nav-link" href="#coefficient-estimation-in-logistic-regression"><span class="header-section-number">11.3</span> Coefficient Estimation in Logistic Regression</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#maximum-likelihood-estimation-for-logistic-regression"><span class="header-section-number">11.3.1</span> Maximum likelihood estimation for logistic regression</a></li></ul>
</li>
<li><a class="nav-link" href="#interpreting-coefficients-in-logistic-regression"><span class="header-section-number">11.4</span> Interpreting Coefficients in Logistic Regression</a></li>
<li>
<a class="nav-link" href="#inference-in-logistic-regression"><span class="header-section-number">11.5</span> Inference in Logistic Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#wald-test"><span class="header-section-number">11.5.1</span> Wald test</a></li>
<li><a class="nav-link" href="#confidence-intervals-for-coefficients"><span class="header-section-number">11.5.2</span> Confidence intervals for coefficients</a></li>
<li><a class="nav-link" href="#likelihood-ratio-tests-in-logistic-regression"><span class="header-section-number">11.5.3</span> Likelihood ratio tests in logistic regression</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#r-tutorial-8"><span class="header-section-number">11.6</span> R Tutorial</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#visualizations-with-logistic-regression">Visualizations with Logistic Regression</a></li>
<li><a class="nav-link" href="#fit-logistic-regression">Fit Logistic Regression</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Linear Models for Data Science</strong>" was written by Jeffrey Woo. It was last built on 2024-07-17.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
